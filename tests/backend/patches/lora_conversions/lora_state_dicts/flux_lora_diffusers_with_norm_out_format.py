# Sample state dict in the Diffusers FLUX LoRA format.
# This from Hyper-SD, having extra `norm_out` layer
# From https://huggingface.co/ByteDance/Hyper-SD/tree/main?show_file_info=Hyper-FLUX.1-dev-16steps-lora.safetensors
state_dict_keys = {
    "transformer.context_embedder.lora_A.weight": [64, 4096],
    "transformer.context_embedder.lora_B.weight": [3072, 64],
    "transformer.norm_out.linear.lora_A.weight": [64, 3072],
    "transformer.norm_out.linear.lora_B.weight": [6144, 64],
    "transformer.proj_out.lora_A.weight": [64, 3072],
    "transformer.proj_out.lora_B.weight": [64, 64],
    "transformer.single_transformer_blocks.0.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.0.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.0.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.0.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.0.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.0.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.0.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.0.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.0.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.0.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.0.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.0.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.1.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.1.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.1.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.1.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.1.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.1.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.1.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.1.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.1.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.1.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.1.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.1.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.10.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.10.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.10.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.10.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.10.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.10.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.10.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.10.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.10.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.10.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.10.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.10.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.11.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.11.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.11.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.11.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.11.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.11.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.11.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.11.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.11.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.11.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.11.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.11.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.12.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.12.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.12.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.12.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.12.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.12.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.12.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.12.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.12.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.12.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.12.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.12.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.13.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.13.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.13.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.13.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.13.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.13.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.13.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.13.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.13.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.13.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.13.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.13.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.14.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.14.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.14.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.14.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.14.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.14.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.14.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.14.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.14.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.14.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.14.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.14.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.15.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.15.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.15.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.15.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.15.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.15.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.15.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.15.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.15.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.15.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.15.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.15.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.16.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.16.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.16.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.16.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.16.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.16.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.16.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.16.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.16.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.16.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.16.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.16.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.17.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.17.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.17.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.17.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.17.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.17.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.17.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.17.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.17.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.17.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.17.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.17.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.18.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.18.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.18.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.18.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.18.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.18.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.18.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.18.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.18.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.18.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.18.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.18.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.19.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.19.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.19.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.19.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.19.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.19.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.19.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.19.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.19.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.19.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.19.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.19.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.2.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.2.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.2.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.2.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.2.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.2.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.2.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.2.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.2.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.2.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.2.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.2.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.20.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.20.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.20.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.20.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.20.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.20.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.20.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.20.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.20.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.20.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.20.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.20.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.21.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.21.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.21.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.21.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.21.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.21.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.21.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.21.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.21.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.21.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.21.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.21.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.22.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.22.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.22.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.22.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.22.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.22.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.22.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.22.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.22.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.22.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.22.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.22.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.23.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.23.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.23.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.23.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.23.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.23.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.23.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.23.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.23.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.23.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.23.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.23.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.24.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.24.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.24.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.24.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.24.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.24.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.24.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.24.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.24.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.24.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.24.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.24.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.25.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.25.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.25.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.25.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.25.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.25.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.25.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.25.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.25.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.25.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.25.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.25.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.26.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.26.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.26.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.26.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.26.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.26.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.26.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.26.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.26.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.26.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.26.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.26.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.27.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.27.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.27.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.27.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.27.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.27.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.27.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.27.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.27.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.27.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.27.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.27.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.28.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.28.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.28.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.28.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.28.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.28.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.28.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.28.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.28.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.28.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.28.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.28.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.29.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.29.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.29.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.29.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.29.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.29.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.29.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.29.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.29.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.29.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.29.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.29.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.3.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.3.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.3.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.3.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.3.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.3.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.3.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.3.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.3.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.3.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.3.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.3.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.30.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.30.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.30.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.30.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.30.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.30.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.30.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.30.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.30.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.30.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.30.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.30.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.31.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.31.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.31.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.31.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.31.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.31.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.31.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.31.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.31.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.31.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.31.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.31.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.32.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.32.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.32.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.32.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.32.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.32.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.32.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.32.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.32.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.32.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.32.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.32.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.33.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.33.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.33.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.33.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.33.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.33.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.33.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.33.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.33.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.33.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.33.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.33.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.34.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.34.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.34.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.34.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.34.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.34.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.34.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.34.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.34.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.34.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.34.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.34.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.35.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.35.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.35.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.35.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.35.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.35.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.35.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.35.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.35.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.35.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.35.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.35.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.36.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.36.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.36.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.36.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.36.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.36.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.36.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.36.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.36.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.36.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.36.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.36.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.37.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.37.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.37.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.37.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.37.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.37.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.37.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.37.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.37.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.37.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.37.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.37.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.4.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.4.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.4.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.4.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.4.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.4.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.4.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.4.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.4.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.4.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.4.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.4.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.5.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.5.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.5.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.5.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.5.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.5.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.5.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.5.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.5.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.5.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.5.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.5.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.6.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.6.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.6.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.6.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.6.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.6.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.6.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.6.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.6.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.6.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.6.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.6.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.7.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.7.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.7.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.7.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.7.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.7.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.7.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.7.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.7.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.7.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.7.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.7.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.8.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.8.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.8.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.8.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.8.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.8.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.8.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.8.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.8.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.8.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.8.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.8.proj_out.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.9.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.9.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.9.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.9.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.9.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.9.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.single_transformer_blocks.9.norm.linear.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.9.norm.linear.lora_B.weight": [9216, 64],
    "transformer.single_transformer_blocks.9.proj_mlp.lora_A.weight": [64, 3072],
    "transformer.single_transformer_blocks.9.proj_mlp.lora_B.weight": [12288, 64],
    "transformer.single_transformer_blocks.9.proj_out.lora_A.weight": [64, 15360],
    "transformer.single_transformer_blocks.9.proj_out.lora_B.weight": [3072, 64],
    "transformer.time_text_embed.guidance_embedder.linear_1.lora_A.weight": [64, 256],
    "transformer.time_text_embed.guidance_embedder.linear_1.lora_B.weight": [3072, 64],
    "transformer.time_text_embed.guidance_embedder.linear_2.lora_A.weight": [64, 3072],
    "transformer.time_text_embed.guidance_embedder.linear_2.lora_B.weight": [3072, 64],
    "transformer.time_text_embed.text_embedder.linear_1.lora_A.weight": [64, 768],
    "transformer.time_text_embed.text_embedder.linear_1.lora_B.weight": [3072, 64],
    "transformer.time_text_embed.text_embedder.linear_2.lora_A.weight": [64, 3072],
    "transformer.time_text_embed.text_embedder.linear_2.lora_B.weight": [3072, 64],
    "transformer.time_text_embed.timestep_embedder.linear_1.lora_A.weight": [64, 256],
    "transformer.time_text_embed.timestep_embedder.linear_1.lora_B.weight": [3072, 64],
    "transformer.time_text_embed.timestep_embedder.linear_2.lora_A.weight": [64, 3072],
    "transformer.time_text_embed.timestep_embedder.linear_2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.0.attn.add_k_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.0.attn.add_k_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.0.attn.add_q_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.0.attn.add_q_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.0.attn.add_v_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.0.attn.add_v_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.0.attn.to_add_out.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.0.attn.to_add_out.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.0.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.0.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.0.attn.to_out.0.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.0.attn.to_out.0.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.0.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.0.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.0.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.0.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.0.ff.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.0.ff.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.0.ff.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.0.ff.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.0.ff_context.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.0.ff_context.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.0.ff_context.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.0.ff_context.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.0.norm1.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.0.norm1.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.0.norm1_context.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.0.norm1_context.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.1.attn.add_k_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.1.attn.add_k_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.1.attn.add_q_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.1.attn.add_q_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.1.attn.add_v_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.1.attn.add_v_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.1.attn.to_add_out.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.1.attn.to_add_out.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.1.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.1.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.1.attn.to_out.0.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.1.attn.to_out.0.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.1.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.1.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.1.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.1.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.1.ff.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.1.ff.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.1.ff.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.1.ff.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.1.ff_context.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.1.ff_context.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.1.ff_context.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.1.ff_context.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.1.norm1.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.1.norm1.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.1.norm1_context.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.1.norm1_context.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.10.attn.add_k_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.10.attn.add_k_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.10.attn.add_q_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.10.attn.add_q_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.10.attn.add_v_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.10.attn.add_v_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.10.attn.to_add_out.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.10.attn.to_add_out.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.10.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.10.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.10.attn.to_out.0.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.10.attn.to_out.0.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.10.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.10.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.10.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.10.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.10.ff.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.10.ff.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.10.ff.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.10.ff.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.10.ff_context.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.10.ff_context.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.10.ff_context.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.10.ff_context.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.10.norm1.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.10.norm1.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.10.norm1_context.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.10.norm1_context.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.11.attn.add_k_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.11.attn.add_k_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.11.attn.add_q_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.11.attn.add_q_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.11.attn.add_v_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.11.attn.add_v_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.11.attn.to_add_out.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.11.attn.to_add_out.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.11.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.11.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.11.attn.to_out.0.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.11.attn.to_out.0.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.11.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.11.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.11.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.11.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.11.ff.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.11.ff.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.11.ff.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.11.ff.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.11.ff_context.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.11.ff_context.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.11.ff_context.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.11.ff_context.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.11.norm1.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.11.norm1.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.11.norm1_context.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.11.norm1_context.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.12.attn.add_k_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.12.attn.add_k_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.12.attn.add_q_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.12.attn.add_q_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.12.attn.add_v_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.12.attn.add_v_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.12.attn.to_add_out.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.12.attn.to_add_out.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.12.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.12.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.12.attn.to_out.0.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.12.attn.to_out.0.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.12.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.12.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.12.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.12.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.12.ff.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.12.ff.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.12.ff.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.12.ff.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.12.ff_context.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.12.ff_context.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.12.ff_context.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.12.ff_context.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.12.norm1.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.12.norm1.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.12.norm1_context.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.12.norm1_context.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.13.attn.add_k_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.13.attn.add_k_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.13.attn.add_q_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.13.attn.add_q_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.13.attn.add_v_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.13.attn.add_v_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.13.attn.to_add_out.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.13.attn.to_add_out.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.13.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.13.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.13.attn.to_out.0.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.13.attn.to_out.0.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.13.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.13.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.13.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.13.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.13.ff.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.13.ff.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.13.ff.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.13.ff.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.13.ff_context.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.13.ff_context.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.13.ff_context.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.13.ff_context.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.13.norm1.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.13.norm1.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.13.norm1_context.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.13.norm1_context.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.14.attn.add_k_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.14.attn.add_k_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.14.attn.add_q_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.14.attn.add_q_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.14.attn.add_v_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.14.attn.add_v_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.14.attn.to_add_out.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.14.attn.to_add_out.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.14.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.14.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.14.attn.to_out.0.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.14.attn.to_out.0.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.14.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.14.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.14.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.14.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.14.ff.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.14.ff.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.14.ff.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.14.ff.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.14.ff_context.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.14.ff_context.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.14.ff_context.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.14.ff_context.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.14.norm1.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.14.norm1.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.14.norm1_context.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.14.norm1_context.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.15.attn.add_k_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.15.attn.add_k_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.15.attn.add_q_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.15.attn.add_q_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.15.attn.add_v_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.15.attn.add_v_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.15.attn.to_add_out.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.15.attn.to_add_out.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.15.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.15.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.15.attn.to_out.0.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.15.attn.to_out.0.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.15.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.15.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.15.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.15.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.15.ff.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.15.ff.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.15.ff.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.15.ff.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.15.ff_context.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.15.ff_context.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.15.ff_context.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.15.ff_context.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.15.norm1.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.15.norm1.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.15.norm1_context.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.15.norm1_context.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.16.attn.add_k_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.16.attn.add_k_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.16.attn.add_q_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.16.attn.add_q_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.16.attn.add_v_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.16.attn.add_v_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.16.attn.to_add_out.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.16.attn.to_add_out.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.16.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.16.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.16.attn.to_out.0.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.16.attn.to_out.0.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.16.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.16.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.16.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.16.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.16.ff.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.16.ff.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.16.ff.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.16.ff.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.16.ff_context.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.16.ff_context.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.16.ff_context.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.16.ff_context.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.16.norm1.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.16.norm1.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.16.norm1_context.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.16.norm1_context.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.17.attn.add_k_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.17.attn.add_k_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.17.attn.add_q_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.17.attn.add_q_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.17.attn.add_v_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.17.attn.add_v_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.17.attn.to_add_out.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.17.attn.to_add_out.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.17.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.17.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.17.attn.to_out.0.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.17.attn.to_out.0.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.17.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.17.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.17.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.17.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.17.ff.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.17.ff.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.17.ff.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.17.ff.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.17.ff_context.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.17.ff_context.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.17.ff_context.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.17.ff_context.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.17.norm1.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.17.norm1.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.17.norm1_context.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.17.norm1_context.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.18.attn.add_k_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.18.attn.add_k_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.18.attn.add_q_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.18.attn.add_q_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.18.attn.add_v_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.18.attn.add_v_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.18.attn.to_add_out.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.18.attn.to_add_out.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.18.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.18.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.18.attn.to_out.0.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.18.attn.to_out.0.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.18.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.18.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.18.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.18.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.18.ff.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.18.ff.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.18.ff.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.18.ff.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.18.ff_context.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.18.ff_context.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.18.ff_context.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.18.ff_context.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.18.norm1.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.18.norm1.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.18.norm1_context.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.18.norm1_context.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.2.attn.add_k_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.2.attn.add_k_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.2.attn.add_q_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.2.attn.add_q_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.2.attn.add_v_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.2.attn.add_v_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.2.attn.to_add_out.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.2.attn.to_add_out.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.2.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.2.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.2.attn.to_out.0.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.2.attn.to_out.0.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.2.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.2.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.2.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.2.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.2.ff.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.2.ff.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.2.ff.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.2.ff.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.2.ff_context.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.2.ff_context.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.2.ff_context.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.2.ff_context.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.2.norm1.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.2.norm1.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.2.norm1_context.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.2.norm1_context.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.3.attn.add_k_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.3.attn.add_k_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.3.attn.add_q_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.3.attn.add_q_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.3.attn.add_v_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.3.attn.add_v_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.3.attn.to_add_out.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.3.attn.to_add_out.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.3.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.3.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.3.attn.to_out.0.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.3.attn.to_out.0.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.3.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.3.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.3.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.3.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.3.ff.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.3.ff.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.3.ff.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.3.ff.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.3.ff_context.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.3.ff_context.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.3.ff_context.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.3.ff_context.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.3.norm1.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.3.norm1.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.3.norm1_context.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.3.norm1_context.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.4.attn.add_k_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.4.attn.add_k_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.4.attn.add_q_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.4.attn.add_q_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.4.attn.add_v_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.4.attn.add_v_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.4.attn.to_add_out.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.4.attn.to_add_out.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.4.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.4.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.4.attn.to_out.0.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.4.attn.to_out.0.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.4.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.4.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.4.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.4.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.4.ff.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.4.ff.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.4.ff.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.4.ff.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.4.ff_context.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.4.ff_context.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.4.ff_context.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.4.ff_context.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.4.norm1.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.4.norm1.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.4.norm1_context.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.4.norm1_context.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.5.attn.add_k_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.5.attn.add_k_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.5.attn.add_q_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.5.attn.add_q_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.5.attn.add_v_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.5.attn.add_v_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.5.attn.to_add_out.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.5.attn.to_add_out.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.5.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.5.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.5.attn.to_out.0.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.5.attn.to_out.0.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.5.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.5.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.5.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.5.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.5.ff.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.5.ff.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.5.ff.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.5.ff.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.5.ff_context.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.5.ff_context.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.5.ff_context.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.5.ff_context.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.5.norm1.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.5.norm1.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.5.norm1_context.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.5.norm1_context.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.6.attn.add_k_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.6.attn.add_k_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.6.attn.add_q_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.6.attn.add_q_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.6.attn.add_v_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.6.attn.add_v_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.6.attn.to_add_out.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.6.attn.to_add_out.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.6.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.6.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.6.attn.to_out.0.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.6.attn.to_out.0.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.6.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.6.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.6.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.6.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.6.ff.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.6.ff.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.6.ff.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.6.ff.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.6.ff_context.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.6.ff_context.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.6.ff_context.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.6.ff_context.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.6.norm1.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.6.norm1.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.6.norm1_context.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.6.norm1_context.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.7.attn.add_k_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.7.attn.add_k_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.7.attn.add_q_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.7.attn.add_q_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.7.attn.add_v_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.7.attn.add_v_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.7.attn.to_add_out.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.7.attn.to_add_out.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.7.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.7.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.7.attn.to_out.0.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.7.attn.to_out.0.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.7.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.7.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.7.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.7.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.7.ff.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.7.ff.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.7.ff.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.7.ff.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.7.ff_context.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.7.ff_context.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.7.ff_context.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.7.ff_context.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.7.norm1.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.7.norm1.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.7.norm1_context.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.7.norm1_context.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.8.attn.add_k_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.8.attn.add_k_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.8.attn.add_q_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.8.attn.add_q_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.8.attn.add_v_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.8.attn.add_v_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.8.attn.to_add_out.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.8.attn.to_add_out.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.8.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.8.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.8.attn.to_out.0.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.8.attn.to_out.0.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.8.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.8.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.8.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.8.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.8.ff.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.8.ff.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.8.ff.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.8.ff.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.8.ff_context.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.8.ff_context.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.8.ff_context.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.8.ff_context.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.8.norm1.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.8.norm1.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.8.norm1_context.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.8.norm1_context.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.9.attn.add_k_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.9.attn.add_k_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.9.attn.add_q_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.9.attn.add_q_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.9.attn.add_v_proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.9.attn.add_v_proj.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.9.attn.to_add_out.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.9.attn.to_add_out.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.9.attn.to_k.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.9.attn.to_k.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.9.attn.to_out.0.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.9.attn.to_out.0.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.9.attn.to_q.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.9.attn.to_q.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.9.attn.to_v.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.9.attn.to_v.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.9.ff.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.9.ff.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.9.ff.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.9.ff.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.9.ff_context.net.0.proj.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.9.ff_context.net.0.proj.lora_B.weight": [12288, 64],
    "transformer.transformer_blocks.9.ff_context.net.2.lora_A.weight": [64, 12288],
    "transformer.transformer_blocks.9.ff_context.net.2.lora_B.weight": [3072, 64],
    "transformer.transformer_blocks.9.norm1.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.9.norm1.linear.lora_B.weight": [18432, 64],
    "transformer.transformer_blocks.9.norm1_context.linear.lora_A.weight": [64, 3072],
    "transformer.transformer_blocks.9.norm1_context.linear.lora_B.weight": [18432, 64],
    "transformer.x_embedder.lora_A.weight": [64, 64],
    "transformer.x_embedder.lora_B.weight": [3072, 64],
}
