# A sample state dict in the Diffusers FLUX LoRA format with base_model.model prefix.
# These keys are based on the LoRA model in peft_adapter_model.safetensors
state_dict_keys = {
    "base_model.model.proj_out.lora_A.weight": [4, 3072],
    "base_model.model.proj_out.lora_B.weight": [64, 4],
    "base_model.model.single_transformer_blocks.0.attn.to_k.lora_A.weight": [4, 3072],
    "base_model.model.single_transformer_blocks.0.attn.to_k.lora_B.weight": [3072, 4],
    "base_model.model.single_transformer_blocks.0.attn.to_q.lora_A.weight": [4, 3072],
    "base_model.model.single_transformer_blocks.0.attn.to_q.lora_B.weight": [3072, 4],
    "base_model.model.single_transformer_blocks.0.attn.to_v.lora_A.weight": [4, 3072],
    "base_model.model.single_transformer_blocks.0.attn.to_v.lora_B.weight": [3072, 4],
    "base_model.model.single_transformer_blocks.0.proj_mlp.lora_A.weight": [4, 3072],
    "base_model.model.single_transformer_blocks.0.proj_mlp.lora_B.weight": [12288, 4],
    "base_model.model.single_transformer_blocks.0.proj_out.lora_A.weight": [4, 15360],
    "base_model.model.single_transformer_blocks.0.proj_out.lora_B.weight": [3072, 4],
    "base_model.model.single_transformer_blocks.1.attn.to_k.lora_A.weight": [4, 3072],
    "base_model.model.single_transformer_blocks.1.attn.to_k.lora_B.weight": [3072, 4],
    "base_model.model.single_transformer_blocks.1.attn.to_q.lora_A.weight": [4, 3072],
    "base_model.model.single_transformer_blocks.1.attn.to_q.lora_B.weight": [3072, 4],
    "base_model.model.single_transformer_blocks.1.attn.to_v.lora_A.weight": [4, 3072],
    "base_model.model.single_transformer_blocks.1.attn.to_v.lora_B.weight": [3072, 4],
    "base_model.model.single_transformer_blocks.1.proj_mlp.lora_A.weight": [4, 3072],
    "base_model.model.single_transformer_blocks.1.proj_mlp.lora_B.weight": [12288, 4],
    "base_model.model.single_transformer_blocks.1.proj_out.lora_A.weight": [4, 15360],
    "base_model.model.single_transformer_blocks.1.proj_out.lora_B.weight": [3072, 4],
    "base_model.model.transformer_blocks.0.attn.add_k_proj.lora_A.weight": [4, 3072],
    "base_model.model.transformer_blocks.0.attn.add_k_proj.lora_B.weight": [3072, 4],
    "base_model.model.transformer_blocks.0.attn.add_q_proj.lora_A.weight": [4, 3072],
    "base_model.model.transformer_blocks.0.attn.add_q_proj.lora_B.weight": [3072, 4],
    "base_model.model.transformer_blocks.0.attn.add_v_proj.lora_A.weight": [4, 3072],
    "base_model.model.transformer_blocks.0.attn.add_v_proj.lora_B.weight": [3072, 4],
    "base_model.model.transformer_blocks.0.attn.to_add_out.lora_A.weight": [4, 3072],
    "base_model.model.transformer_blocks.0.attn.to_add_out.lora_B.weight": [3072, 4],
    "base_model.model.transformer_blocks.0.attn.to_k.lora_A.weight": [4, 3072],
    "base_model.model.transformer_blocks.0.attn.to_k.lora_B.weight": [3072, 4],
    "base_model.model.transformer_blocks.0.attn.to_out.0.lora_A.weight": [4, 3072],
    "base_model.model.transformer_blocks.0.attn.to_out.0.lora_B.weight": [3072, 4],
    "base_model.model.transformer_blocks.0.attn.to_q.lora_A.weight": [4, 3072],
    "base_model.model.transformer_blocks.0.attn.to_q.lora_B.weight": [3072, 4],
    "base_model.model.transformer_blocks.0.attn.to_v.lora_A.weight": [4, 3072],
    "base_model.model.transformer_blocks.0.attn.to_v.lora_B.weight": [3072, 4],
    "base_model.model.transformer_blocks.0.ff.net.0.proj.lora_A.weight": [4, 3072],
    "base_model.model.transformer_blocks.0.ff.net.0.proj.lora_B.weight": [12288, 4],
    "base_model.model.transformer_blocks.0.ff_context.net.0.proj.lora_A.weight": [4, 3072],
    "base_model.model.transformer_blocks.0.ff_context.net.0.proj.lora_B.weight": [12288, 4],
    "base_model.model.transformer_blocks.1.attn.add_k_proj.lora_A.weight": [4, 3072],
    "base_model.model.transformer_blocks.1.attn.add_k_proj.lora_B.weight": [3072, 4],
    "base_model.model.transformer_blocks.1.attn.add_q_proj.lora_A.weight": [4, 3072],
    "base_model.model.transformer_blocks.1.attn.add_q_proj.lora_B.weight": [3072, 4],
    "base_model.model.transformer_blocks.1.attn.add_v_proj.lora_A.weight": [4, 3072],
    "base_model.model.transformer_blocks.1.attn.add_v_proj.lora_B.weight": [3072, 4],
    "base_model.model.transformer_blocks.1.attn.to_add_out.lora_A.weight": [4, 3072],
    "base_model.model.transformer_blocks.1.attn.to_add_out.lora_B.weight": [3072, 4],
    "base_model.model.transformer_blocks.1.attn.to_k.lora_A.weight": [4, 3072],
    "base_model.model.transformer_blocks.1.attn.to_k.lora_B.weight": [3072, 4],
    "base_model.model.transformer_blocks.1.attn.to_out.0.lora_A.weight": [4, 3072],
    "base_model.model.transformer_blocks.1.attn.to_out.0.lora_B.weight": [3072, 4],
    "base_model.model.transformer_blocks.1.attn.to_q.lora_A.weight": [4, 3072],
    "base_model.model.transformer_blocks.1.attn.to_q.lora_B.weight": [3072, 4],
    "base_model.model.transformer_blocks.1.attn.to_v.lora_A.weight": [4, 3072],
    "base_model.model.transformer_blocks.1.attn.to_v.lora_B.weight": [3072, 4],
    "base_model.model.transformer_blocks.1.ff.net.0.proj.lora_A.weight": [4, 3072],
    "base_model.model.transformer_blocks.1.ff.net.0.proj.lora_B.weight": [12288, 4],
    "base_model.model.transformer_blocks.1.ff_context.net.0.proj.lora_A.weight": [4, 3072],
    "base_model.model.transformer_blocks.1.ff_context.net.0.proj.lora_B.weight": [12288, 4],
}
