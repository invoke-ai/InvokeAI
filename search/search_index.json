{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Invoke","text":"<p>Invoke is a leading creative engine built to empower professionals and enthusiasts alike. Generate and create stunning visual media using the latest AI-driven technologies. Invoke offers an industry leading web-based UI, and serves as the foundation for multiple commercial products.</p>"},{"location":"#installation","title":"Installation","text":"<p>The Invoke Launcher is the easiest way to install, update and run Invoke on Windows, macOS and Linux.</p> <p>You can also install Invoke as python package or with docker.</p>"},{"location":"#help","title":"Help","text":"<p>Please first check the FAQ to see if there is a fix for your issue or answer to your question.</p> <p>If you still have a problem, create an issue or ask for help on Discord.</p>"},{"location":"#training","title":"Training","text":"<p>Invoke Training has moved to its own repository, with a dedicated UI for accessing common scripts like Textual Inversion and LoRA training.</p> <p>You can find more by visiting the repo at https://github.com/invoke-ai/invoke-training.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions, big and small. Please review our contributing guide if you'd like make a contribution.</p> <p>This software is a combined effort of people across the world. We thank them for their time, hard work and effort!</p>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at https://github.com/invoke-ai/InvokeAI/issues.  All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"RELEASE/","title":"Release Process","text":"<p>The Invoke application is published as a python package on PyPI. This includes both a source distribution and built distribution (a wheel).</p> <p>Most users install it with the Launcher, others with <code>pip</code>.</p> <p>The launcher uses GitHub as the source of truth for available releases.</p>"},{"location":"RELEASE/#broad-strokes","title":"Broad Strokes","text":"<ul> <li>Merge all changes and bump the version in the codebase.</li> <li>Tag the release commit.</li> <li>Wait for the release workflow to complete.</li> <li>Approve the PyPI publish jobs.</li> <li>Write GH release notes.</li> </ul>"},{"location":"RELEASE/#general-prep","title":"General Prep","text":"<p>Make a developer call-out for PRs to merge. Merge and test things out. Create a branch with a name like user/chore/vX.X.X-prep and bump the version by editing <code>invokeai/version/invokeai_version.py</code> and commit locally.</p>"},{"location":"RELEASE/#release-workflow","title":"Release Workflow","text":"<p>The <code>release.yml</code> workflow runs a number of jobs to handle code checks, tests, build and publish on PyPI.</p> <p>It is triggered on tag push, when the tag matches <code>v*</code>.</p>"},{"location":"RELEASE/#triggering-the-workflow","title":"Triggering the Workflow","text":"<p>Ensure all commits that should be in the release are merged into this branch, and that you have pulled them locally.</p> <p>Run <code>make tag-release</code> to tag the current commit and kick off the workflow. You will be prompted to provide a message - use the version specifier.</p> <p>If this version's tag already exists for some reason (maybe you had to make a last minute change), the script will overwrite it.</p> <p>Push the commit to trigger the workflow.</p> <p>In case you cannot use the Make target, the release may also be dispatched manually via GH.</p>"},{"location":"RELEASE/#workflow-jobs-and-process","title":"Workflow Jobs and Process","text":"<p>The workflow consists of a number of concurrently-run checks and tests, then two final publish jobs.</p> <p>The publish jobs require manual approval and are only run if the other jobs succeed.</p>"},{"location":"RELEASE/#check-version-job","title":"<code>check-version</code> Job","text":"<p>This job ensures that the <code>invokeai</code> python package version specifier matches the tag for the release. The version specifier is pulled from the <code>__version__</code> variable in <code>invokeai/version/invokeai_version.py</code>.</p> <p>This job uses samuelcolvin/check-python-version.</p> <p>Any valid version specifier works, so long as the tag matches the version. The release workflow works exactly the same for <code>RC</code>, <code>post</code>, <code>dev</code>, etc.</p>"},{"location":"RELEASE/#check-and-test-jobs","title":"Check and Test Jobs","text":"<p>Next, these jobs run and must pass. They are the same jobs that are run for every PR.</p> <ul> <li><code>python-tests</code>: runs <code>pytest</code> on matrix of platforms</li> <li><code>python-checks</code>: runs <code>ruff</code> (format and lint)</li> <li><code>frontend-tests</code>: runs <code>vitest</code></li> <li><code>frontend-checks</code>: runs <code>prettier</code> (format), <code>eslint</code> (lint), <code>dpdm</code> (circular refs), <code>tsc</code> (static type check) and <code>knip</code> (unused imports)</li> <li><code>typegen-checks</code>: ensures the frontend and backend types are synced</li> </ul>"},{"location":"RELEASE/#build-wheel-job","title":"<code>build-wheel</code> Job","text":"<p>This sets up both python and frontend dependencies and builds the python package. Internally, this runs <code>./scripts/build_wheel.sh</code> and uploads <code>dist.zip</code>, which contains the wheel and unarchived build.</p> <p>You don't need to download or test these artifacts.</p>"},{"location":"RELEASE/#sanity-check-smoke-test","title":"Sanity Check &amp; Smoke Test","text":"<p>At this point, the release workflow pauses as the remaining publish jobs require approval.</p> <p>It's possible to test the python package before it gets published to PyPI. We've never had problems with it, so it's not necessary to do this.</p> <p>But, if you want to be extra-super careful, here's how to test it:</p> <ul> <li>Download the <code>dist.zip</code> build artifact from the <code>build-wheel</code> job</li> <li>Unzip it and find the wheel file</li> <li>Create a fresh Invoke install by following the manual install guide - but instead of installing from PyPI, install from the wheel</li> <li>Test the app</li> </ul>"},{"location":"RELEASE/#something-isnt-right","title":"Something isn't right","text":"<p>If testing reveals any issues, no worries. Cancel the workflow, which will cancel the pending publish jobs (you didn't approve them prematurely, right?) and start over.</p>"},{"location":"RELEASE/#pypi-publish-jobs","title":"PyPI Publish Jobs","text":"<p>The publish jobs will not run if any of the previous jobs fail.</p> <p>They use GitHub environments, which are configured as trusted publishers on PyPI.</p> <p>Both jobs require a @lstein or @blessedcoolant to approve them from the workflow's Summary tab.</p> <ul> <li>Click the Review deployments button</li> <li>Select the environment (either <code>testpypi</code> or <code>pypi</code> - typically you select both)</li> <li>Click Approve and deploy</li> </ul> <p>If the version already exists on PyPI, the publish jobs will fail. PyPI only allows a given version to be published once - you cannot change it. If version published on PyPI has a problem, you'll need to \"fail forward\" by bumping the app version and publishing a followup release.</p>"},{"location":"RELEASE/#failing-pypi-publish","title":"Failing PyPI Publish","text":"<p>Check the python infrastructure status page for incidents.</p> <p>If there are no incidents, contact @lstein or @blessedcoolant, who have owner access to GH and PyPI, to see if access has expired or something like that.</p>"},{"location":"RELEASE/#publish-testpypi-job","title":"<code>publish-testpypi</code> Job","text":"<p>Publishes the distribution on the Test PyPI index, using the <code>testpypi</code> GitHub environment.</p> <p>This job is not required for the production PyPI publish, but included just in case you want to test the PyPI release for some reason:</p> <ul> <li>Approve this publish job without approving the prod publish</li> <li>Let it finish</li> <li>Create a fresh Invoke install by following the manual install guide, making sure to use the Test PyPI index URL: <code>https://test.pypi.org/simple/</code></li> <li>Test the app</li> </ul>"},{"location":"RELEASE/#publish-pypi-job","title":"<code>publish-pypi</code> Job","text":"<p>Publishes the distribution on the production PyPI index, using the <code>pypi</code> GitHub environment.</p> <p>It's a good idea to wait to approve and run this job until you have the release notes ready!</p>"},{"location":"RELEASE/#prep-and-publish-the-github-release","title":"Prep and publish the GitHub Release","text":"<ol> <li>Draft a new release on GitHub, choosing the tag that triggered the release.</li> <li>The Generate release notes button automatically inserts the changelog and new contributors. Make sure to select the correct tags for this release and the last stable release. GH often selects the wrong tags - do this manually.</li> <li>Write the release notes, describing important changes. Contributions from community members should be shouted out. Use the GH-generated changelog to see all contributors. If there are Weblate translation updates, open that PR and shout out every person who contributed a translation.</li> <li>Check Set as a pre-release if it's a pre-release.</li> <li>Approve and wait for the <code>publish-pypi</code> job to finish if you haven't already.</li> <li>Publish the GH release.</li> <li>Post the release in Discord in the releases channel with abbreviated notes. For example: <p>Invoke v5.7.0 (stable): https://github.com/invoke-ai/InvokeAI/releases/tag/v5.7.0</p> <p>It's a pretty big one - Form Builder, Metadata Nodes (thanks @SkunkWorxDark!), and much more.</p> </li> <li>Right click the message in releases and copy the link to it. Then, post that link in the new-release-discussion channel. For example: <p>Invoke v5.7.0 (stable): https://discord.com/channels/1020123559063990373/1149260708098359327/1344521744916021248</p> </li> </ol>"},{"location":"RELEASE/#manual-release","title":"Manual Release","text":"<p>The <code>release</code> workflow can be dispatched manually. You must dispatch the workflow from the right tag, else it will fail the version check.</p> <p>This functionality is available as a fallback in case something goes wonky. Typically, releases should be triggered via tag push as described above.</p>"},{"location":"configuration/","title":"InvokeAI Configuration","text":""},{"location":"configuration/#intro","title":"Intro","text":"<p>Runtime settings, including the location of files and directories, memory usage, and performance, are managed via the <code>invokeai.yaml</code> config file or environment variables. A subset of settings may be set via commandline arguments.</p> <p>Settings sources are used in this order:</p> <ul> <li>CLI args</li> <li>Environment variables</li> <li><code>invokeai.yaml</code> settings</li> <li>Fallback: defaults</li> </ul>"},{"location":"configuration/#invokeai-root-directory","title":"InvokeAI Root Directory","text":"<p>On startup, InvokeAI searches for its \"root\" directory. This is the directory that contains models, images, the database, and so on. It also contains a configuration file called <code>invokeai.yaml</code>.</p> <p>InvokeAI searches for the root directory in this order:</p> <ol> <li>The <code>--root &lt;path&gt;</code> CLI arg.</li> <li>The environment variable INVOKEAI_ROOT.</li> <li>The directory containing the currently active virtual environment.</li> <li>Fallback: a directory in the current user's home directory named <code>invokeai</code>.</li> </ol>"},{"location":"configuration/#invokeai-configuration-file","title":"InvokeAI Configuration File","text":"<p>Inside the root directory, we read settings from the <code>invokeai.yaml</code> file.</p> <p>It has two sections - one for internal use and one for user settings:</p> <pre><code># Internal metadata - do not edit:\nschema_version: 4.0.2\n\n# Put user settings here - see https://invoke-ai.github.io/InvokeAI/features/CONFIGURATION/:\nhost: 0.0.0.0 # serve the app on your local network\nmodels_dir: D:\\invokeai\\models # store models on an external drive\nprecision: float16 # always use fp16 precision\n</code></pre> <p>The settings in this file will override the defaults. You only need to change this file if the default for a particular setting doesn't work for you.</p> <p>You'll find an example file next to <code>invokeai.yaml</code> that shows the default values.</p> <p>Some settings, like Model Marketplace API Keys, require the YAML to be formatted correctly. Here is a basic guide to YAML files.</p>"},{"location":"configuration/#custom-config-file-location","title":"Custom Config File Location","text":"<p>You can use any config file with the <code>--config</code> CLI arg. Pass in the path to the <code>invokeai.yaml</code> file you want to use.</p> <p>Note that environment variables will trump any settings in the config file.</p>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<p>All settings may be set via environment variables by prefixing <code>INVOKEAI_</code> to the variable name. For example, <code>INVOKEAI_HOST</code> would set the <code>host</code> setting.</p> <p>For non-primitive values, pass a JSON-encoded string:</p> <pre><code>export INVOKEAI_REMOTE_API_TOKENS='[{\"url_regex\":\"modelmarketplace\", \"token\": \"12345\"}]'\n</code></pre> <p>We suggest using <code>invokeai.yaml</code>, as it is more user-friendly.</p>"},{"location":"configuration/#cli-args","title":"CLI Args","text":"<p>A subset of settings may be specified using CLI args:</p> <ul> <li><code>--root</code>: specify the root directory</li> <li><code>--config</code>: override the default <code>invokeai.yaml</code> file location</li> </ul>"},{"location":"configuration/#low-vram-mode","title":"Low-VRAM Mode","text":"<p>See the Low-VRAM mode docs for details on enabling this feature.</p>"},{"location":"configuration/#all-settings","title":"All Settings","text":"<p>Following the table are additional explanations for certain settings.</p> <p>Attributes:</p> Name Type Description <code>host</code> <code>str</code> <p>IP address to bind to. Use <code>0.0.0.0</code> to serve to your local network.</p> <code>port</code> <code>int</code> <p>Port to bind to.</p> <code>allow_origins</code> <code>list[str]</code> <p>Allowed CORS origins.</p> <code>allow_credentials</code> <code>bool</code> <p>Allow CORS credentials.</p> <code>allow_methods</code> <code>list[str]</code> <p>Methods allowed for CORS.</p> <code>allow_headers</code> <code>list[str]</code> <p>Headers allowed for CORS.</p> <code>ssl_certfile</code> <code>Optional[Path]</code> <p>SSL certificate file for HTTPS. See https://www.uvicorn.org/settings/#https.</p> <code>ssl_keyfile</code> <code>Optional[Path]</code> <p>SSL key file for HTTPS. See https://www.uvicorn.org/settings/#https.</p> <code>log_tokenization</code> <code>bool</code> <p>Enable logging of parsed prompt tokens.</p> <code>patchmatch</code> <code>bool</code> <p>Enable patchmatch inpaint code.</p> <code>models_dir</code> <code>Path</code> <p>Path to the models directory.</p> <code>convert_cache_dir</code> <code>Path</code> <p>Path to the converted models cache directory (DEPRECATED, but do not delete because it is needed for migration from previous versions).</p> <code>download_cache_dir</code> <code>Path</code> <p>Path to the directory that contains dynamically downloaded models.</p> <code>legacy_conf_dir</code> <code>Path</code> <p>Path to directory of legacy checkpoint config files.</p> <code>db_dir</code> <code>Path</code> <p>Path to InvokeAI databases directory.</p> <code>outputs_dir</code> <code>Path</code> <p>Path to directory for outputs.</p> <code>custom_nodes_dir</code> <code>Path</code> <p>Path to directory for custom nodes.</p> <code>style_presets_dir</code> <code>Path</code> <p>Path to directory for style presets.</p> <code>workflow_thumbnails_dir</code> <code>Path</code> <p>Path to directory for workflow thumbnails.</p> <code>log_handlers</code> <code>list[str]</code> <p>Log handler. Valid options are \"console\", \"file=\", \"syslog=path|address:host:port\", \"http=\". <code>log_format</code> <code>LOG_FORMAT</code> <p>Log format. Use \"plain\" for text-only, \"color\" for colorized output, \"legacy\" for 2.3-style logging and \"syslog\" for syslog-style.Valid values: <code>plain</code>, <code>color</code>, <code>syslog</code>, <code>legacy</code></p> <code>log_level</code> <code>LOG_LEVEL</code> <p>Emit logging messages at this level or higher.Valid values: <code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code>, <code>critical</code></p> <code>log_sql</code> <code>bool</code> <p>Log SQL queries. <code>log_level</code> must be <code>debug</code> for this to do anything. Extremely verbose.</p> <code>log_level_network</code> <code>LOG_LEVEL</code> <p>Log level for network-related messages. 'info' and 'debug' are very verbose.Valid values: <code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code>, <code>critical</code></p> <code>use_memory_db</code> <code>bool</code> <p>Use in-memory database. Useful for development.</p> <code>dev_reload</code> <code>bool</code> <p>Automatically reload when Python sources are changed. Does not reload node definitions.</p> <code>profile_graphs</code> <code>bool</code> <p>Enable graph profiling using <code>cProfile</code>.</p> <code>profile_prefix</code> <code>Optional[str]</code> <p>An optional prefix for profile output files.</p> <code>profiles_dir</code> <code>Path</code> <p>Path to profiles output directory.</p> <code>max_cache_ram_gb</code> <code>Optional[float]</code> <p>The maximum amount of CPU RAM to use for model caching in GB. If unset, the limit will be configured based on the available RAM. In most cases, it is recommended to leave this unset.</p> <code>max_cache_vram_gb</code> <code>Optional[float]</code> <p>The amount of VRAM to use for model caching in GB. If unset, the limit will be configured based on the available VRAM and the device_working_mem_gb. In most cases, it is recommended to leave this unset.</p> <code>log_memory_usage</code> <code>bool</code> <p>If True, a memory snapshot will be captured before and after every model cache operation, and the result will be logged (at debug level). There is a time cost to capturing the memory snapshots, so it is recommended to only enable this feature if you are actively inspecting the model cache's behaviour.</p> <code>model_cache_keep_alive_min</code> <code>float</code> <p>How long to keep models in cache after last use, in minutes. A value of 0 (the default) means models are kept in cache indefinitely. If no model generations occur within the timeout period, the model cache is cleared using the same logic as the 'Clear Model Cache' button.</p> <code>device_working_mem_gb</code> <code>float</code> <p>The amount of working memory to keep available on the compute device (in GB). Has no effect if running on CPU. If you are experiencing OOM errors, try increasing this value.</p> <code>enable_partial_loading</code> <code>bool</code> <p>Enable partial loading of models. This enables models to run with reduced VRAM requirements (at the cost of slower speed) by streaming the model from RAM to VRAM as its used. In some edge cases, partial loading can cause models to run more slowly if they were previously being fully loaded into VRAM.</p> <code>keep_ram_copy_of_weights</code> <code>bool</code> <p>Whether to keep a full RAM copy of a model's weights when the model is loaded in VRAM. Keeping a RAM copy increases average RAM usage, but speeds up model switching and LoRA patching (assuming there is sufficient RAM). Set this to False if RAM pressure is consistently high.</p> <code>ram</code> <code>Optional[float]</code> <p>DEPRECATED: This setting is no longer used. It has been replaced by <code>max_cache_ram_gb</code>, but most users will not need to use this config since automatic cache size limits should work well in most cases. This config setting will be removed once the new model cache behavior is stable.</p> <code>vram</code> <code>Optional[float]</code> <p>DEPRECATED: This setting is no longer used. It has been replaced by <code>max_cache_vram_gb</code>, but most users will not need to use this config since automatic cache size limits should work well in most cases. This config setting will be removed once the new model cache behavior is stable.</p> <code>lazy_offload</code> <code>bool</code> <p>DEPRECATED: This setting is no longer used. Lazy-offloading is enabled by default. This config setting will be removed once the new model cache behavior is stable.</p> <code>pytorch_cuda_alloc_conf</code> <code>Optional[str]</code> <p>Configure the Torch CUDA memory allocator. This will impact peak reserved VRAM usage and performance. Setting to \"backend:cudaMallocAsync\" works well on many systems. The optimal configuration is highly dependent on the system configuration (device type, VRAM, CUDA driver version, etc.), so must be tuned experimentally.</p> <code>device</code> <code>str</code> <p>Preferred execution device. <code>auto</code> will choose the device depending on the hardware platform and the installed torch capabilities.Valid values: <code>auto</code>, <code>cpu</code>, <code>cuda</code>, <code>mps</code>, <code>cuda:N</code> (where N is a device number)</p> <code>precision</code> <code>PRECISION</code> <p>Floating point precision. <code>float16</code> will consume half the memory of <code>float32</code> but produce slightly lower-quality images. The <code>auto</code> setting will guess the proper precision based on your video card and operating system.Valid values: <code>auto</code>, <code>float16</code>, <code>bfloat16</code>, <code>float32</code></p> <code>sequential_guidance</code> <code>bool</code> <p>Whether to calculate guidance in serial instead of in parallel, lowering memory requirements.</p> <code>attention_type</code> <code>ATTENTION_TYPE</code> <p>Attention type.Valid values: <code>auto</code>, <code>normal</code>, <code>xformers</code>, <code>sliced</code>, <code>torch-sdp</code></p> <code>attention_slice_size</code> <code>ATTENTION_SLICE_SIZE</code> <p>Slice size, valid when attention_type==\"sliced\".Valid values: <code>auto</code>, <code>balanced</code>, <code>max</code>, <code>1</code>, <code>2</code>, <code>3</code>, <code>4</code>, <code>5</code>, <code>6</code>, <code>7</code>, <code>8</code></p> <code>force_tiled_decode</code> <code>bool</code> <p>Whether to enable tiled VAE decode (reduces memory consumption with some performance penalty).</p> <code>pil_compress_level</code> <code>int</code> <p>The compress_level setting of PIL.Image.save(), used for PNG encoding. All settings are lossless. 0 = no compression, 1 = fastest with slightly larger filesize, 9 = slowest with smallest filesize. 1 is typically the best setting.</p> <code>max_queue_size</code> <code>int</code> <p>Maximum number of items in the session queue.</p> <code>clear_queue_on_startup</code> <code>bool</code> <p>Empties session queue on startup.</p> <code>allow_nodes</code> <code>Optional[list[str]]</code> <p>List of nodes to allow. Omit to allow all.</p> <code>deny_nodes</code> <code>Optional[list[str]]</code> <p>List of nodes to deny. Omit to deny none.</p> <code>node_cache_size</code> <code>int</code> <p>How many cached nodes to keep in memory.</p> <code>hashing_algorithm</code> <code>HASHING_ALGORITHMS</code> <p>Model hashing algorthim for model installs. 'blake3_multi' is best for SSDs. 'blake3_single' is best for spinning disk HDDs. 'random' disables hashing, instead assigning a UUID to models. Useful when using a memory db to reduce model installation time, or if you don't care about storing stable hashes for models. Alternatively, any other hashlib algorithm is accepted, though these are not nearly as performant as blake3.Valid values: <code>blake3_multi</code>, <code>blake3_single</code>, <code>random</code>, <code>md5</code>, <code>sha1</code>, <code>sha224</code>, <code>sha256</code>, <code>sha384</code>, <code>sha512</code>, <code>blake2b</code>, <code>blake2s</code>, <code>sha3_224</code>, <code>sha3_256</code>, <code>sha3_384</code>, <code>sha3_512</code>, <code>shake_128</code>, <code>shake_256</code></p> <code>remote_api_tokens</code> <code>Optional[list[URLRegexTokenPair]]</code> <p>List of regular expression and token pairs used when downloading models from URLs. The download URL is tested against the regex, and if it matches, the token is provided in as a Bearer token.</p> <code>scan_models_on_startup</code> <code>bool</code> <p>Scan the models directory on startup, registering orphaned models. This is typically only used in conjunction with <code>use_memory_db</code> for testing purposes.</p> <code>unsafe_disable_picklescan</code> <code>bool</code> <p>UNSAFE. Disable the picklescan security check during model installation. Recommended only for development and testing purposes. This will allow arbitrary code execution during model installation, so should never be used in production.</p> <code>allow_unknown_models</code> <code>bool</code> <p>Allow installation of models that we are unable to identify. If enabled, models will be marked as <code>unknown</code> in the database, and will not have any metadata associated with them. If disabled, unknown models will be rejected during installation.</p>"},{"location":"configuration/#model-marketplace-api-keys","title":"Model Marketplace API Keys","text":"<p>Some model marketplaces require an API key to download models. You can provide a URL pattern and appropriate token in your <code>invokeai.yaml</code> file to provide that API key.</p> <p>The pattern can be any valid regex (you may need to surround the pattern with quotes):</p> <pre><code>remote_api_tokens:\n  # Any URL containing `models.com` will automatically use `your_models_com_token`\n  - url_regex: models.com\n    token: your_models_com_token\n  # Any URL matching this contrived regex will use `some_other_token`\n  - url_regex: '^[a-z]{3}whatever.*\\.com$'\n    token: some_other_token\n</code></pre> <p>The provided token will be added as a <code>Bearer</code> token to the network requests to download the model files. As far as we know, this works for all model marketplaces that require authorization.</p> <p>HuggingFace Models</p> <p>If you get an error when installing a HF model using a URL instead of repo id, you may need to set up a HF API token and add an entry for it under <code>remote_api_tokens</code>. Use <code>huggingface.co</code> for <code>url_regex</code>.</p>"},{"location":"configuration/#model-hashing","title":"Model Hashing","text":"<p>Models are hashed during installation, providing a stable identifier for models across all platforms. Hashing is a one-time operation.</p> <pre><code>hashing_algorithm: blake3_single # default value\n</code></pre> <p>You might want to change this setting, depending on your system:</p> <ul> <li><code>blake3_single</code> (default): Single-threaded - best for spinning HDDs, still OK for SSDs</li> <li><code>blake3_multi</code>: Parallelized, memory-mapped implementation - best for SSDs, terrible for spinning disks</li> <li><code>random</code>: Skip hashing entirely - fastest but of course no hash</li> </ul> <p>During the first startup after upgrading to v4, all of your models will be hashed. This can take a few minutes.</p> <p>Most common algorithms are supported, like <code>md5</code>, <code>sha256</code>, and <code>sha512</code>. These are typically much, much slower than either of the BLAKE3 variants.</p>"},{"location":"configuration/#path-settings","title":"Path Settings","text":"<p>These options set the paths of various directories and files used by InvokeAI. Any user-defined paths should be absolute paths.</p>"},{"location":"configuration/#logging","title":"Logging","text":"<p>Several different log handler destinations are available, and multiple destinations are supported by providing a list:</p> <pre><code>log_handlers:\n  - console\n  - syslog=localhost\n  - file=/var/log/invokeai.log\n</code></pre> <ul> <li> <p><code>console</code> is the default. It prints log messages to the command-line window from which InvokeAI was launched.</p> </li> <li> <p><code>syslog</code> is only available on Linux and Macintosh systems. It uses   the operating system's \"syslog\" facility to write log file entries   locally or to a remote logging machine. <code>syslog</code> offers a variety   of configuration options:</p> </li> </ul> <pre><code>syslog=/dev/log`      - log to the /dev/log device\nsyslog=localhost`     - log to the network logger running on the local machine\nsyslog=localhost:512` - same as above, but using a non-standard port\nsyslog=fredserver,facility=LOG_USER,socktype=SOCK_DRAM`\n- Log to LAN-connected server \"fredserver\" using the facility LOG_USER and datagram packets.\n</code></pre> <ul> <li><code>http</code> can be used to log to a remote web server. The server must be   properly configured to receive and act on log messages. The option   accepts the URL to the web server, and a <code>method</code> argument   indicating whether the message should be submitted using the GET or   POST method.</li> </ul> <pre><code>http=http://my.server/path/to/logger,method=POST\n</code></pre> <p>The <code>log_format</code> option provides several alternative formats:</p> <ul> <li><code>color</code> - default format providing time, date and a message, using text colors to distinguish different log severities</li> <li><code>plain</code> - same as above, but monochrome text only</li> <li><code>syslog</code> - the log level and error message only, allowing the syslog system to attach the time and date</li> <li><code>legacy</code> - a format similar to the one used by the legacy 2.3 InvokeAI releases.</li> </ul>"},{"location":"faq/","title":"FAQ","text":"<p>If the troubleshooting steps on this page don't get you up and running, please either create an issue or hop on discord for help.</p>"},{"location":"faq/#how-to-install","title":"How to Install","text":"<p>Follow the Quick Start guide to install Invoke.</p>"},{"location":"faq/#downloading-models-and-using-existing-models","title":"Downloading models and using existing models","text":"<p>The Model Manager tab in the UI provides a few ways to install models, including using your already-downloaded models. You'll see a popup directing you there on first startup. For more information, see the model install docs.</p>"},{"location":"faq/#missing-models-after-updating-from-v3","title":"Missing models after updating from v3","text":"<p>If you find some models are missing after updating from v3, it's likely they weren't correctly registered before the update and didn't get picked up in the migration.</p> <p>You can use the <code>Scan Folder</code> tab in the Model Manager UI to fix this. The models will either be in the old, now-unused <code>autoimport</code> folder, or your <code>models</code> folder.</p> <ul> <li>Find and copy your install's old <code>autoimport</code> folder path, install the main install folder.</li> <li>Go to the Model Manager and click <code>Scan Folder</code>.</li> <li>Paste the path and scan.</li> <li>IMPORTANT: Uncheck <code>Inplace install</code>.</li> <li>Click <code>Install All</code> to install all found models, or just install the models you want.</li> </ul> <p>Next, find and copy your install's <code>models</code> folder path (this could be your custom models folder path, or the <code>models</code> folder inside the main install folder).</p> <p>Follow the same steps to scan and import the missing models.</p>"},{"location":"faq/#slow-generation","title":"Slow generation","text":"<ul> <li>Check the system requirements to ensure that your system is capable of generating images.</li> <li>Follow the Low-VRAM mode guide to optimize performance.</li> <li>Check that your generations are happening on your GPU (if you have one). Invoke will log what is being used for generation upon startup. If your GPU isn't used, re-install to and ensure you select the appropriate GPU option.</li> <li>If you are on Windows with an Nvidia GPU, you may have exceeded your GPU's VRAM capacity and are triggering Nvidia's \"sysmem fallback\". There's a guide to opt out of this behaviour in the Low-VRAM mode guide.</li> </ul>"},{"location":"faq/#triton-error-on-startup","title":"Triton error on startup","text":"<p>This can be safely ignored. Invoke doesn't use Triton, but if you are on Linux and wish to dismiss the error, you can install Triton.</p>"},{"location":"faq/#unable-to-copy-on-firefox","title":"Unable to Copy on Firefox","text":"<p>Firefox does not allow Invoke to directly access the clipboard by default. As a result, you may be unable to use certain copy functions. You can fix this by configuring Firefox to allow access to write to the clipboard:</p> <ul> <li>Go to <code>about:config</code> and click the Accept button</li> <li>Search for <code>dom.events.asyncClipboard.clipboardItem</code></li> <li>Set it to <code>true</code> by clicking the toggle button</li> <li>Restart Firefox</li> </ul>"},{"location":"faq/#replicate-image-found-online","title":"Replicate image found online","text":"<p>Most example images with prompts that you'll find on the internet have been generated using different software, so you can't expect to get identical results. In order to reproduce an image, you need to replicate the exact settings and processing steps, including (but not limited to) the model, the positive and negative prompts, the seed, the sampler, the exact image size, any upscaling steps, etc.</p>"},{"location":"faq/#invalid-configuration-file","title":"Invalid configuration file","text":"<p>Everything seems to install ok, you get a <code>ValidationError</code> when starting up the app.</p> <p>This is caused by an invalid setting in the <code>invokeai.yaml</code> configuration file. The error message should tell you what is wrong.</p> <p>Check the configuration docs for more detail about the settings and how to specify them.</p>"},{"location":"faq/#out-of-memory-errors","title":"Out of Memory Errors","text":"<p>The models are large, VRAM is expensive, and you may find yourself faced with Out of Memory errors when generating images. Follow our Low-VRAM mode guide to configure Invoke to prevent these.</p>"},{"location":"faq/#memory-leak-linux","title":"Memory Leak (Linux)","text":"<p>If you notice a memory leak, it could be caused to memory fragmentation as models are loaded and/or moved from CPU to GPU.</p> <p>A workaround is to tune memory allocation with an environment variable:</p> <pre><code># Force blocks &gt;1MB to be allocated with `mmap` so that they are released to the system immediately when they are freed.\nMALLOC_MMAP_THRESHOLD_=1048576\n</code></pre> <p>Speed vs Memory Tradeoff</p> <p>Your generations may be slower overall when setting this environment variable.</p> <p>Possibly dependent on <code>libc</code> implementation</p> <p>It's not known if this issue occurs with other <code>libc</code> implementations such as <code>musl</code>.</p> <p>If you encounter this issue and your system uses a different implementation, please try this environment variable and let us know if it fixes the issue.</p> Detailed Discussion <p>Python (and PyTorch) relies on the memory allocator from the C Standard Library (<code>libc</code>). On linux, with the GNU C Standard Library implementation (<code>glibc</code>), our memory access patterns have been observed to cause severe memory fragmentation.</p> <p>This fragmentation results in large amounts of memory that has been freed but can't be released back to the OS. Loading models from disk and moving them between CPU/CUDA seem to be the operations that contribute most to the fragmentation.</p> <p>This memory fragmentation issue can result in OOM crashes during frequent model switching, even if <code>ram</code> (the max RAM cache size) is set to a reasonable value (e.g. a OOM crash with <code>ram=16</code> on a system with 32GB of RAM).</p> <p>This problem may also exist on other OSes, and other <code>libc</code> implementations. But, at the time of writing, it has only been investigated on linux with <code>glibc</code>.</p> <p>To better understand how the <code>glibc</code> memory allocator works, see these references:</p> <ul> <li>Basics: https://www.gnu.org/software/libc/manual/html_node/The-GNU-Allocator.html</li> <li>Details: https://sourceware.org/glibc/wiki/MallocInternals</li> </ul> <p>Note the differences between memory allocated as chunks in an arena vs. memory allocated with <code>mmap</code>. Under <code>glibc</code>'s default configuration, most model tensors get allocated as chunks in an arena making them vulnerable to the problem of fragmentation.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Invoke originated as a project built by the community, and that vision carries forward today as we aim to build the best pro-grade tools available. We work together to incorporate the latest in AI/ML research, making these tools available in over 20 languages to artists and creatives around the world as part of our fully permissive OSS project designed for individual users to self-host and use.</p> <p>We welcome contributions, whether features, bug fixes, code cleanup, testing, code reviews, documentation or translation. Please check in with us before diving in to code to ensure your work aligns with our vision.</p>"},{"location":"contributing/#development","title":"Development","text":"<p>If you\u2019d like to help with development, please see our development guide.</p> <p>New Contributors: If you\u2019re unfamiliar with contributing to open source projects, take a look at our new contributor guide.</p>"},{"location":"contributing/#nodes","title":"Nodes","text":"<p>If you\u2019d like to add a Node, please see our nodes contribution guide.</p>"},{"location":"contributing/#support-and-triaging","title":"Support and Triaging","text":"<p>Helping support other users in Discord and on Github are valuable forms of contribution that we greatly appreciate.</p> <p>We receive many issues and requests for help from users. We're limited in bandwidth relative to our the user base, so providing answers to questions or helping identify causes of issues is very helpful. By doing this, you enable us to spend time on the highest priority work.</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>If you\u2019d like to help with documentation, please see our documentation guide.</p>"},{"location":"contributing/#translation","title":"Translation","text":"<p>If you'd like to help with translation, please see our\u00a0translation guide.</p>"},{"location":"contributing/#tutorials","title":"Tutorials","text":"<p>Please reach out to @hipsterusername on Discord to help create tutorials for InvokeAI.</p>"},{"location":"contributing/#contributors","title":"Contributors","text":"<p>This project is a combined effort of dedicated people from across the world.\u00a0Check out the list of all these amazing people. We thank them for their time, hard work and effort.</p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>The InvokeAI community is a welcoming place, and we want your help in maintaining that. Please review our Code of Conduct to learn more - it's essential to maintaining a respectful and inclusive environment.</p> <p>By making a contribution to this project, you certify that:</p> <ol> <li>The contribution was created in whole or in part by you and you have the right to submit it under the open-source license indicated in this project\u2019s GitHub repository; or</li> <li>The contribution is based upon previous work that, to the best of your knowledge, is covered under an appropriate open-source license and you have the right under that license to submit that work with modifications, whether created in whole or in part by you, under the same open-source license (unless you are permitted to submit under a different license); or</li> <li>The contribution was provided directly to you by some other person who certified (1) or (2) and you have not modified it; or</li> <li>You understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information you submit with it, including your sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open-source license(s) involved.</li> </ol> <p>This disclaimer is not a license and does not grant any rights or permissions. You must obtain necessary permissions and licenses, including from third parties, before contributing to this project.</p> <p>This disclaimer is provided \"as is\" without warranty of any kind, whether expressed or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose, or non-infringement. In no event shall the authors or copyright holders be liable for any claim, damages, or other liability, whether in an action of contract, tort, or otherwise, arising from, out of, or in connection with the contribution or the use or other dealings in the contribution.</p>"},{"location":"contributing/ARCHITECTURE/","title":"Invoke.AI Architecture","text":"<pre><code>flowchart TB\n\n  subgraph apps[Applications]\n    webui[WebUI]\n    cli[CLI]\n\n  subgraph webapi[Web API]\n    api[HTTP API]\n    sio[Socket.IO]\n  end\n\n  end\n\n  subgraph invoke[Invoke]\n    direction LR\n    invoker\n    services\n    sessions\n    invocations\n  end\n\n  subgraph core[AI Core]\n    Generate\n  end\n\n  webui --&gt; webapi\n  webapi --&gt; invoke\n  cli --&gt; invoke\n\n  invoker --&gt; services &amp; sessions\n  invocations --&gt; services\n  sessions --&gt; invocations\n\n  services --&gt; core\n\n  %% Styles\n  classDef sg fill:#5028C8,font-weight:bold,stroke-width:2,color:#fff,stroke:#14141A\n  classDef default stroke-width:2px,stroke:#F6B314,color:#fff,fill:#14141A\n\n  class apps,webapi,invoke,core sg\n</code></pre>"},{"location":"contributing/ARCHITECTURE/#applications","title":"Applications","text":"<p>Applications are built on top of the invoke framework. They should construct <code>invoker</code> and then interact through it. They should avoid interacting directly with core code in order to support a variety of configurations.</p>"},{"location":"contributing/ARCHITECTURE/#web-ui","title":"Web UI","text":"<p>The Web UI is built on top of an HTTP API built with FastAPI and Socket.IO. The frontend code is found in <code>/invokeai/frontend</code> and the backend code is found in <code>/invokeai/app/api_app.py</code> and <code>/invokeai/app/api/</code>. The code is further organized as such:</p> Component Description api_app.py Sets up the API app, annotates the OpenAPI spec with additional data, and runs the API dependencies Creates all invoker services and the invoker, and provides them to the API events An eventing system that could in the future be adapted to support horizontal scale-out sockets The Socket.IO interface - handles listening to and emitting session events (events are defined in the events service module) routers API definitions for different areas of API functionality"},{"location":"contributing/ARCHITECTURE/#cli","title":"CLI","text":"<p>The CLI is built automatically from invocation metadata, and also supports invocation piping and auto-linking. Code is available in <code>/invokeai/frontend/cli</code>.</p>"},{"location":"contributing/ARCHITECTURE/#invoke","title":"Invoke","text":"<p>The Invoke framework provides the interface to the underlying AI systems and is built with flexibility and extensibility in mind. There are four major concepts: invoker, sessions, invocations, and services.</p>"},{"location":"contributing/ARCHITECTURE/#invoker","title":"Invoker","text":"<p>The invoker (<code>/invokeai/app/services/invoker.py</code>) is the primary interface through which applications interact with the framework. Its primary purpose is to create, manage, and invoke sessions. It also maintains two sets of services: - invocation services, which are used by invocations to interact with core functionality. - invoker services, which are used by the invoker to manage sessions and manage the invocation queue.</p>"},{"location":"contributing/ARCHITECTURE/#sessions","title":"Sessions","text":"<p>Invocations and links between them form a graph, which is maintained in a session. Sessions can be queued for invocation, which will execute their graph (either the next ready invocation, or all invocations). Sessions also maintain execution history for the graph (including storage of any outputs). An invocation may be added to a session at any time, and there is capability to add and entire graph at once, as well as to automatically link new invocations to previous invocations. Invocations can not be deleted or modified once added.</p> <p>The session graph does not support looping. This is left as an application problem to prevent additional complexity in the graph.</p>"},{"location":"contributing/ARCHITECTURE/#invocations","title":"Invocations","text":"<p>Invocations represent individual units of execution, with inputs and outputs. All invocations are located in <code>/invokeai/app/invocations</code>, and are all automatically discovered and made available in the applications. These are the primary way to expose new functionality in Invoke.AI, and the implementation guide explains how to add new invocations.</p>"},{"location":"contributing/ARCHITECTURE/#services","title":"Services","text":"<p>Services provide invocations access AI Core functionality and other necessary functionality (e.g. image storage). These are available in <code>/invokeai/app/services</code>. As a general rule, new services should provide an interface as an abstract base class, and may provide a lightweight local implementation by default in their module. The goal for all services should be to enable the usage of different implementations (e.g. using cloud storage for image storage), but should not load any module dependencies unless that implementation has been used (i.e. don't import anything that won't be used, especially if it's expensive to import).</p>"},{"location":"contributing/ARCHITECTURE/#ai-core","title":"AI Core","text":"<p>The AI Core is represented by the rest of the code base (i.e. the code outside of <code>/invokeai/app/</code>).</p>"},{"location":"contributing/DOWNLOAD_QUEUE/","title":"The InvokeAI Download Queue","text":"<p>The DownloadQueueService provides a multithreaded parallel download queue for arbitrary URLs, with queue prioritization, event handling, and restart capabilities.</p>"},{"location":"contributing/DOWNLOAD_QUEUE/#simple-example","title":"Simple Example","text":"<pre><code>from invokeai.app.services.download import DownloadQueueService, TqdmProgress\n\ndownload_queue = DownloadQueueService()\nfor url in ['https://github.com/invoke-ai/InvokeAI/blob/main/invokeai/assets/a-painting-of-a-fire.png?raw=true',\n            'https://github.com/invoke-ai/InvokeAI/blob/main/invokeai/assets/birdhouse.png?raw=true',\n            'https://github.com/invoke-ai/InvokeAI/blob/main/invokeai/assets/missing.png',\n            'https://civitai.com/api/download/models/152309?type=Model&amp;format=SafeTensor',\n            ]:\n\n    # urls start downloading as soon as download() is called\n    download_queue.download(source=url,\n                            dest='/tmp/downloads',\n                            on_progress=TqdmProgress().update\n                            )\n\ndownload_queue.join()  # wait for all downloads to finish\nfor job in download_queue.list_jobs():\n    print(job.model_dump_json(exclude_none=True, indent=4),\"\\n\")\n</code></pre> <p>Output:</p> <pre><code>{\n    \"source\": \"https://github.com/invoke-ai/InvokeAI/blob/main/invokeai/assets/a-painting-of-a-fire.png?raw=true\",\n    \"dest\": \"/tmp/downloads\",\n    \"id\": 0,\n    \"priority\": 10,\n    \"status\": \"completed\",\n    \"download_path\": \"/tmp/downloads/a-painting-of-a-fire.png\",\n    \"job_started\": \"2023-12-04T05:34:41.742174\",\n    \"job_ended\": \"2023-12-04T05:34:42.592035\",\n    \"bytes\": 666734,\n    \"total_bytes\": 666734\n} \n\n{\n    \"source\": \"https://github.com/invoke-ai/InvokeAI/blob/main/invokeai/assets/birdhouse.png?raw=true\",\n    \"dest\": \"/tmp/downloads\",\n    \"id\": 1,\n    \"priority\": 10,\n    \"status\": \"completed\",\n    \"download_path\": \"/tmp/downloads/birdhouse.png\",\n    \"job_started\": \"2023-12-04T05:34:41.741975\",\n    \"job_ended\": \"2023-12-04T05:34:42.652841\",\n    \"bytes\": 774949,\n    \"total_bytes\": 774949\n}\n\n{\n    \"source\": \"https://github.com/invoke-ai/InvokeAI/blob/main/invokeai/assets/missing.png\",\n    \"dest\": \"/tmp/downloads\",\n    \"id\": 2,\n    \"priority\": 10,\n    \"status\": \"error\",\n    \"job_started\": \"2023-12-04T05:34:41.742079\",\n    \"job_ended\": \"2023-12-04T05:34:42.147625\",\n    \"bytes\": 0,\n    \"total_bytes\": 0,\n    \"error_type\": \"HTTPError(Not Found)\",\n    \"error\": \"Traceback (most recent call last):\\n  File \\\"/home/lstein/Projects/InvokeAI/invokeai/app/services/download/download_default.py\\\", line 182, in _download_next_item\\n    self._do_download(job)\\n  File \\\"/home/lstein/Projects/InvokeAI/invokeai/app/services/download/download_default.py\\\", line 206, in _do_download\\n    raise HTTPError(resp.reason)\\nrequests.exceptions.HTTPError: Not Found\\n\"\n}\n\n{\n    \"source\": \"https://civitai.com/api/download/models/152309?type=Model&amp;format=SafeTensor\",\n    \"dest\": \"/tmp/downloads\",\n    \"id\": 3,\n    \"priority\": 10,\n    \"status\": \"completed\",\n    \"download_path\": \"/tmp/downloads/xl_more_art-full_v1.safetensors\",\n    \"job_started\": \"2023-12-04T05:34:42.147645\",\n    \"job_ended\": \"2023-12-04T05:34:43.735990\",\n    \"bytes\": 719020768,\n    \"total_bytes\": 719020768\n} \n</code></pre>"},{"location":"contributing/DOWNLOAD_QUEUE/#the-api","title":"The API","text":"<p>The default download queue is <code>DownloadQueueService</code>, an implementation of ABC <code>DownloadQueueServiceBase</code>. It juggles multiple background download requests and provides facilities for interrogating and cancelling the requests. Access to a current or past download task is mediated via <code>DownloadJob</code> objects which report the current status of a job request</p>"},{"location":"contributing/DOWNLOAD_QUEUE/#the-queue-object","title":"The Queue Object","text":"<p>A default download queue is located in <code>ApiDependencies.invoker.services.download_queue</code>. However, you can create additional instances if you need to isolate your queue from the main one.</p> <pre><code>queue = DownloadQueueService(event_bus=events)\n</code></pre> <p><code>DownloadQueueService()</code> takes three optional arguments:</p> Argument Type Default Description <code>max_parallel_dl</code> int 5 Maximum number of simultaneous downloads allowed <code>event_bus</code> EventServiceBase None System-wide FastAPI event bus for reporting download events <code>requests_session</code> requests.sessions.Session None An alternative requests Session object to use for the download <p><code>max_parallel_dl</code> specifies how many download jobs are allowed to run simultaneously. Each will run in a different thread of execution.</p> <p><code>event_bus</code> is an EventServiceBase, typically the one created at InvokeAI startup. If present, download events are periodically emitted on this bus to allow clients to follow download progress.</p> <p><code>requests_session</code> is a url library requests Session object. It is used for testing.</p>"},{"location":"contributing/DOWNLOAD_QUEUE/#the-job-object","title":"The Job object","text":"<p>The queue operates on a series of download job objects. These objects specify the source and destination of the download, and keep track of the progress of the download.</p> <p>Two job types are defined. <code>DownloadJob</code> and <code>MultiFileDownloadJob</code>. The former is a pydantic object with the following fields:</p> Field Type Default Description Fields passed in at job creation time <code>source</code> AnyHttpUrl Where to download from <code>dest</code> Path Where to download to <code>access_token</code> str [optional] string containing authentication token for access <code>on_start</code> Callable [optional] callback when the download starts <code>on_progress</code> Callable [optional] callback called at intervals during download progress <code>on_complete</code> Callable [optional] callback called after successful download completion <code>on_error</code> Callable [optional] callback called after an error occurs <code>id</code> int auto assigned Job ID, an integer &gt;= 0 <code>priority</code> int 10 Job priority. Lower priorities run before higher priorities Fields updated over the course of the download task <code>status</code> DownloadJobStatus Status code <code>download_path</code> Path Path to the location of the downloaded file <code>job_started</code> float Timestamp for when the job started running <code>job_ended</code> float Timestamp for when the job completed or errored out <code>job_sequence</code> int A counter that is incremented each time a model is dequeued <code>bytes</code> int 0 Bytes downloaded so far <code>total_bytes</code> int 0 Total size of the file at the remote site <code>error_type</code> str String version of the exception that caused an error during download <code>error</code> str String version of the traceback associated with an error <code>cancelled</code> bool False Set to true if the job was cancelled by the caller <p>When you create a job, you can assign it a <code>priority</code>. If multiple jobs are queued, the job with the lowest priority runs first.</p> <p>Every job has a <code>source</code> and a <code>dest</code>. <code>source</code> is a pydantic.networks AnyHttpUrl object. The <code>dest</code> is a path on the local filesystem that specifies the destination for the downloaded object. Its semantics are described below.</p> <p>When the job is submitted, it is assigned a numeric <code>id</code>. The id can then be used to fetch the job object from the queue.</p> <p>The <code>status</code> field is updated by the queue to indicate where the job is in its lifecycle. Values are defined in the string enum <code>DownloadJobStatus</code>, a symbol available from <code>invokeai.app.services.download_manager</code>. Possible values are:</p> Value String Value ** Description ** <code>WAITING</code> waiting Job is on the queue but not yet running <code>RUNNING</code> running The download is started <code>COMPLETED</code> completed Job has finished its work without an error <code>ERROR</code> error Job encountered an error and will not run again <p><code>job_started</code> and <code>job_ended</code> indicate when the job was started (using a python timestamp) and when it completed.</p> <p>In case of an error, the job's status will be set to <code>DownloadJobStatus.ERROR</code>, the text of the Exception that caused the error will be placed in the <code>error_type</code> field and the traceback that led to the error will be in <code>error</code>.</p> <p>A cancelled job will have status <code>DownloadJobStatus.ERROR</code> and an <code>error_type</code> field of \"DownloadJobCancelledException\". In addition, the job's <code>cancelled</code> property will be set to True.</p> <p>The <code>MultiFileDownloadJob</code> is used for diffusers model downloads, which contain multiple files and directories under a common root:</p> Field Type Default Description Fields passed in at job creation time <code>download_parts</code> Set[DownloadJob] Component download jobs <code>dest</code> Path Where to download to <code>on_start</code> Callable [optional] callback when the download starts <code>on_progress</code> Callable [optional] callback called at intervals during download progress <code>on_complete</code> Callable [optional] callback called after successful download completion <code>on_error</code> Callable [optional] callback called after an error occurs <code>id</code> int auto assigned Job ID, an integer &gt;= 0 Fields updated over the course of the download task <code>status</code> DownloadJobStatus Status code <code>download_path</code> Path Path to the root of the downloaded files <code>bytes</code> int 0 Bytes downloaded so far <code>total_bytes</code> int 0 Total size of the file at the remote site <code>error_type</code> str String version of the exception that caused an error during download <code>error</code> str String version of the traceback associated with an error <code>cancelled</code> bool False Set to true if the job was cancelled by the caller <p>Note that the MultiFileDownloadJob does not support the <code>priority</code>, <code>job_started</code>, <code>job_ended</code> or <code>content_type</code> attributes. You can get these from the individual download jobs in <code>download_parts</code>.</p>"},{"location":"contributing/DOWNLOAD_QUEUE/#callbacks","title":"Callbacks","text":"<p>Download jobs can be associated with a series of callbacks, each with the signature <code>Callable[[\"DownloadJob\"], None]</code>. The callbacks are assigned using optional arguments <code>on_start</code>, <code>on_progress</code>, <code>on_complete</code> and <code>on_error</code>. When the corresponding event occurs, the callback wil be invoked and passed the job. The callback will be run in a <code>try:</code> context in the same thread as the download job. Any exceptions that occur during execution of the callback will be caught and converted into a log error message, thereby allowing the download to continue.</p>"},{"location":"contributing/DOWNLOAD_QUEUE/#tqdmprogress","title":"<code>TqdmProgress</code>","text":"<p>The <code>invokeai.app.services.download.download_default</code> module defines a class named <code>TqdmProgress</code> which can be used as an <code>on_progress</code> handler to display a completion bar in the console. Use as follows:</p> <pre><code>from invokeai.app.services.download import TqdmProgress\n\ndownload_queue.download(source='http://some.server.somewhere/some_file',\n                        dest='/tmp/downloads',\n                        on_progress=TqdmProgress().update\n                        )\n</code></pre>"},{"location":"contributing/DOWNLOAD_QUEUE/#events","title":"Events","text":"<p>If the queue was initialized with the InvokeAI event bus (the case when using <code>ApiDependencies.invoker.services.download_queue</code>), then download events will also be issued on the bus. The events are:</p> <ul> <li> <p><code>download_started</code> -- This is issued when a job is taken off the queue and a request is made to the remote server for the URL headers, but before any data has been downloaded. The event payload will contain the keys <code>source</code> and <code>download_path</code>. The latter contains the path that the URL will be downloaded to.</p> </li> <li> <p><code>download_progress -- This is issued periodically as the download runs. The payload contains the keys</code>source<code>,</code>download_path<code>,</code>current_bytes<code>and</code>total_bytes`. The latter two fields can be used to display the percent complete.</p> </li> <li> <p><code>download_complete</code> -- This is issued when the download completes successfully. The payload contains the keys <code>source</code>, <code>download_path</code> and <code>total_bytes</code>.</p> </li> <li> <p><code>download_error</code> -- This is issued when the download stops because of an error condition. The payload contains the fields <code>error_type</code> and <code>error</code>. The former is the text representation of the exception, and the latter is a traceback showing where the error occurred.</p> </li> </ul>"},{"location":"contributing/DOWNLOAD_QUEUE/#job-control","title":"Job control","text":"<p>To create a job call the queue's <code>download()</code> method. You can list all jobs using <code>list_jobs()</code>, fetch a single job by its with <code>id_to_job()</code>, cancel a running job with <code>cancel_job()</code>, cancel all running jobs with <code>cancel_all_jobs()</code>, and wait for all jobs to finish with <code>join()</code>.</p>"},{"location":"contributing/DOWNLOAD_QUEUE/#job-queuedownloadsource-dest-priority-access_token-on_start-on_progress-on_complete-on_cancelled-on_error","title":"job = queue.download(source, dest, priority, access_token, on_start, on_progress, on_complete, on_cancelled, on_error)","text":"<p>Create a new download job and put it on the queue, returning the DownloadJob object.</p>"},{"location":"contributing/DOWNLOAD_QUEUE/#multifile_job-queuemultifile_downloadparts-dest-access_token-on_start-on_progress-on_complete-on_cancelled-on_error","title":"multifile_job = queue.multifile_download(parts, dest, access_token, on_start, on_progress, on_complete, on_cancelled, on_error)","text":"<p>This is similar to download(), but instead of taking a single source, it accepts a <code>parts</code> argument consisting of a list of <code>RemoteModelFile</code> objects. Each part corresponds to a URL/Path pair, where the URL is the location of the remote file, and the Path is the destination.</p> <p><code>RemoteModelFile</code> can be imported from <code>invokeai.backend.model_manager.metadata</code>, and consists of a url/path pair. Note that the path must be relative.</p> <p>The method returns a <code>MultiFileDownloadJob</code>.</p> <pre><code>from invokeai.backend.model_manager.metadata import RemoteModelFile\nremote_file_1 = RemoteModelFile(url='http://www.foo.bar/my/pytorch_model.safetensors'',\n                                path='my_model/textencoder/pytorch_model.safetensors'\n                          )\nremote_file_2 = RemoteModelFile(url='http://www.bar.baz/vae.ckpt',\n                                path='my_model/vae/diffusers_model.safetensors'\n                          )\njob = queue.multifile_download(parts=[remote_file_1, remote_file_2],\n                               dest='/tmp/downloads',\n                               on_progress=TqdmProgress().update)\nqueue.wait_for_job(job)\nprint(f\"The files were downloaded to {job.download_path}\")\n</code></pre>"},{"location":"contributing/DOWNLOAD_QUEUE/#jobs-queuelist_jobs","title":"jobs = queue.list_jobs()","text":"<p>Return a list of all active and inactive <code>DownloadJob</code>s.</p>"},{"location":"contributing/DOWNLOAD_QUEUE/#job-queueid_to_jobid","title":"job = queue.id_to_job(id)","text":"<p>Return the job corresponding to given ID.</p> <p>Return a list of all active and inactive <code>DownloadJob</code>s.</p>"},{"location":"contributing/DOWNLOAD_QUEUE/#queueprune_jobs","title":"queue.prune_jobs()","text":"<p>Remove inactive (complete or errored) jobs from the listing returned by <code>list_jobs()</code>.</p>"},{"location":"contributing/DOWNLOAD_QUEUE/#queuejoin","title":"queue.join()","text":"<p>Block until all pending jobs have run to completion or errored out.</p>"},{"location":"contributing/HOTKEYS/","title":"Hotkeys System","text":"<p>This document describes the technical implementation of the customizable hotkeys system in InvokeAI.</p> <p>Note: For user-facing documentation on how to use customizable hotkeys, see Hotkeys Feature Documentation.</p>"},{"location":"contributing/HOTKEYS/#overview","title":"Overview","text":"<p>The hotkeys system allows users to customize keyboard shortcuts throughout the application. All hotkeys are: - Centrally defined and managed - Customizable by users - Persisted across sessions - Type-safe and validated</p>"},{"location":"contributing/HOTKEYS/#architecture","title":"Architecture","text":"<p>The customizable hotkeys feature is built on top of the existing hotkey system with the following components:</p>"},{"location":"contributing/HOTKEYS/#1-hotkeys-state-slice-hotkeysslicets","title":"1. Hotkeys State Slice (<code>hotkeysSlice.ts</code>)","text":"<p>Location: <code>invokeai/frontend/web/src/features/system/store/hotkeysSlice.ts</code></p> <p>Responsibilities: - Stores custom hotkey mappings in Redux state - Persisted to IndexedDB using <code>redux-remember</code> - Provides actions to change, reset individual, or reset all hotkeys</p> <p>State Shape: <pre><code>{\n  _version: 1,\n  customHotkeys: {\n    'app.invoke': ['mod+enter'],\n    'canvas.undo': ['mod+z'],\n    // ...\n  }\n}\n</code></pre></p> <p>Actions: - <code>hotkeyChanged(id, hotkeys)</code> - Update a single hotkey - <code>hotkeyReset(id)</code> - Reset a single hotkey to default - <code>allHotkeysReset()</code> - Reset all hotkeys to defaults</p>"},{"location":"contributing/HOTKEYS/#2-usehotkeydata-hook-usehotkeydatats","title":"2. useHotkeyData Hook (<code>useHotkeyData.ts</code>)","text":"<p>Location: <code>invokeai/frontend/web/src/features/system/components/HotkeysModal/useHotkeyData.ts</code></p> <p>Responsibilities: - Defines all default hotkeys - Merges default hotkeys with custom hotkeys from the store - Returns the effective hotkeys that should be used throughout the app - Provides platform-specific key translations (Ctrl/Cmd, Alt/Option)</p> <p>Key Functions: - <code>useHotkeyData()</code> - Returns all hotkeys organized by category - <code>useRegisteredHotkeys()</code> - Hook to register a hotkey in a component</p>"},{"location":"contributing/HOTKEYS/#3-hotkeyeditor-component-hotkeyeditortsx","title":"3. HotkeyEditor Component (<code>HotkeyEditor.tsx</code>)","text":"<p>Location: <code>invokeai/frontend/web/src/features/system/components/HotkeysModal/HotkeyEditor.tsx</code></p> <p>Features: - Inline editor with input field - Modifier buttons (Mod, Ctrl, Shift, Alt) for quick insertion - Live preview of hotkey combinations - Validation with visual feedback - Help tooltip with syntax examples - Save/cancel/reset buttons</p> <p>Smart Features: - Automatic <code>+</code> insertion between modifiers - Cursor position preservation - Validation prevents invalid combinations (e.g., modifier-only keys)</p>"},{"location":"contributing/HOTKEYS/#4-hotkeysmodal-component-hotkeysmodaltsx","title":"4. HotkeysModal Component (<code>HotkeysModal.tsx</code>)","text":"<p>Location: <code>invokeai/frontend/web/src/features/system/components/HotkeysModal/HotkeysModal.tsx</code></p> <p>Features: - View Mode / Edit Mode toggle - Search functionality - Category-based organization - Shows HotkeyEditor components when in edit mode - \"Reset All to Default\" button in edit mode</p>"},{"location":"contributing/HOTKEYS/#data-flow","title":"Data Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 1. User opens Hotkeys Modal                                 \u2502\n\u2502 2. User clicks \"Edit Mode\" button                           \u2502\n\u2502 3. User clicks edit icon next to a hotkey                   \u2502\n\u2502 4. User enters new hotkey(s) using editor                   \u2502\n\u2502 5. User clicks save or presses Enter                        \u2502\n\u2502 6. Custom hotkey stored via hotkeyChanged() action          \u2502\n\u2502 7. Redux state persisted to IndexedDB (redux-remember)      \u2502\n\u2502 8. useHotkeyData() hook picks up the change                 \u2502\n\u2502 9. All components using useRegisteredHotkeys() get update   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"contributing/HOTKEYS/#hotkey-format","title":"Hotkey Format","text":"<p>Hotkeys use the format from <code>react-hotkeys-hook</code> library:</p> <ul> <li>Modifiers: <code>mod</code>, <code>ctrl</code>, <code>shift</code>, <code>alt</code>, <code>meta</code></li> <li>Keys: Letters, numbers, function keys, special keys</li> <li>Separator: <code>+</code> between keys in a combination</li> <li>Multiple hotkeys: Comma-separated (e.g., <code>mod+a, ctrl+b</code>)</li> </ul> <p>Examples: - <code>mod+enter</code> - Mod key + Enter - <code>shift+x</code> - Shift + X - <code>ctrl+shift+a</code> - Control + Shift + A - <code>f1, f2</code> - F1 or F2 (alternatives)</p>"},{"location":"contributing/HOTKEYS/#developer-guide","title":"Developer Guide","text":""},{"location":"contributing/HOTKEYS/#using-hotkeys-in-components","title":"Using Hotkeys in Components","text":"<p>To use a hotkey in a component:</p> <pre><code>import { useRegisteredHotkeys } from 'features/system/components/HotkeysModal/useHotkeyData';\n\nconst MyComponent = () =&gt; {\n  const handleAction = useCallback(() =&gt; {\n    // Your action here\n  }, []);\n\n  // This automatically uses custom hotkeys if configured\n  useRegisteredHotkeys({\n    id: 'myAction',\n    category: 'app', // or 'canvas', 'viewer', 'gallery', 'workflows'\n    callback: handleAction,\n    options: { enabled: true, preventDefault: true },\n    dependencies: [handleAction]\n  });\n\n  // ...\n};\n</code></pre> <p>Options: - <code>enabled</code> - Whether the hotkey is active - <code>preventDefault</code> - Prevent default browser behavior - <code>enableOnFormTags</code> - Allow hotkey in form elements (default: false)</p>"},{"location":"contributing/HOTKEYS/#adding-new-hotkeys","title":"Adding New Hotkeys","text":"<p>To add a new hotkey to the system:</p>"},{"location":"contributing/HOTKEYS/#1-add-translation-strings","title":"1. Add Translation Strings","text":"<p>In <code>invokeai/frontend/web/public/locales/en.json</code>:</p> <pre><code>{\n  \"hotkeys\": {\n    \"app\": {\n      \"myAction\": {\n        \"title\": \"My Action\",\n        \"desc\": \"Description of what this hotkey does\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"contributing/HOTKEYS/#2-register-the-hotkey","title":"2. Register the Hotkey","text":"<p>In <code>invokeai/frontend/web/src/features/system/components/HotkeysModal/useHotkeyData.ts</code>:</p> <pre><code>// Inside the appropriate category builder function\naddHotkey('app', 'myAction', ['mod+k']); // Default binding\n</code></pre>"},{"location":"contributing/HOTKEYS/#3-use-the-hotkey","title":"3. Use the Hotkey","text":"<p>In your component:</p> <pre><code>useRegisteredHotkeys({\n  id: 'myAction',\n  category: 'app',\n  callback: handleMyAction,\n  options: { enabled: true },\n  dependencies: [handleMyAction]\n});\n</code></pre>"},{"location":"contributing/HOTKEYS/#hotkey-categories","title":"Hotkey Categories","text":"<p>Current categories: - app - Global application hotkeys - canvas - Canvas/drawing operations - viewer - Image viewer operations - gallery - Gallery/image grid operations - workflows - Node workflow editor</p> <p>To add a new category, update <code>useHotkeyData.ts</code> and add translations.</p>"},{"location":"contributing/HOTKEYS/#testing","title":"Testing","text":"<p>Tests are located in <code>invokeai/frontend/web/src/features/system/store/hotkeysSlice.test.ts</code>.</p> <p>Test Coverage: - Adding custom hotkeys - Updating existing custom hotkeys - Resetting individual hotkeys - Resetting all hotkeys - State persistence and migration</p> <p>Run tests with:</p> <pre><code>cd invokeai/frontend/web\npnpm test:no-watch\n</code></pre>"},{"location":"contributing/HOTKEYS/#persistence","title":"Persistence","text":"<p>Custom hotkeys are persisted using the same mechanism as other app settings:</p> <ul> <li>Stored in Redux state under the <code>hotkeys</code> slice</li> <li>Persisted to IndexedDB via <code>redux-remember</code></li> <li>Automatically loaded when the app starts</li> <li>Survives page refreshes and browser restarts</li> <li>Includes migration support for state schema changes</li> </ul> <p>State Location: - IndexedDB database: <code>invoke</code> - Store key: <code>hotkeys</code></p>"},{"location":"contributing/HOTKEYS/#dependencies","title":"Dependencies","text":"<ul> <li>react-hotkeys-hook (v4.5.0) - Core hotkey handling</li> <li>reduxjs/toolkit - State management</li> <li>redux-remember - Persistence</li> <li>zod - State validation</li> </ul>"},{"location":"contributing/HOTKEYS/#best-practices","title":"Best Practices","text":"<ol> <li>Use <code>mod</code> instead of <code>ctrl</code> - Automatically maps to Cmd on Mac, Ctrl elsewhere</li> <li>Provide descriptive translations - Help users understand what each hotkey does</li> <li>Avoid conflicts - Check existing hotkeys before adding new ones</li> <li>Use preventDefault - Prevent browser default behavior when appropriate</li> <li>Check enabled state - Only activate hotkeys when the action is available</li> <li>Use dependencies correctly - Ensure callbacks are stable with useCallback</li> </ol>"},{"location":"contributing/HOTKEYS/#common-patterns","title":"Common Patterns","text":""},{"location":"contributing/HOTKEYS/#conditional-hotkeys","title":"Conditional Hotkeys","text":"<pre><code>useRegisteredHotkeys({\n  id: 'save',\n  category: 'app',\n  callback: handleSave,\n  options: {\n    enabled: hasUnsavedChanges &amp;&amp; !isLoading, // Only when valid\n    preventDefault: true\n  },\n  dependencies: [hasUnsavedChanges, isLoading, handleSave]\n});\n</code></pre>"},{"location":"contributing/HOTKEYS/#multiple-hotkeys-for-same-action","title":"Multiple Hotkeys for Same Action","text":"<pre><code>// In useHotkeyData.ts\naddHotkey('canvas', 'redo', ['mod+shift+z', 'mod+y']); // Two alternatives\n</code></pre>"},{"location":"contributing/HOTKEYS/#focus-scoped-hotkeys","title":"Focus-Scoped Hotkeys","text":"<pre><code>import { useFocusRegion } from 'common/hooks/focus';\n\nconst MyComponent = () =&gt; {\n  const focusRegionRef = useFocusRegion('myRegion');\n\n  // Hotkey only works when this region has focus\n  useRegisteredHotkeys({\n    id: 'myAction',\n    category: 'app',\n    callback: handleAction,\n    options: { enabled: true }\n  });\n\n  return &lt;div ref={focusRegionRef}&gt;...&lt;/div&gt;;\n};\n</code></pre>"},{"location":"contributing/INVOCATIONS/","title":"Nodes","text":"<p>Features in InvokeAI are added in the form of modular nodes systems called Invocations.</p> <p>An Invocation is simply a single operation that takes in some inputs and gives out some outputs. We can then chain multiple Invocations together to create more complex functionality.</p>"},{"location":"contributing/INVOCATIONS/#invocations-directory","title":"Invocations Directory","text":"<p>InvokeAI Nodes can be found in the <code>invokeai/app/invocations</code> directory. These can be used as examples to create your own nodes.</p> <p>New nodes should be added to a subfolder in <code>nodes</code> direction found at the root level of the InvokeAI installation location. Nodes added to this folder will be able to be used upon application startup.</p> <p>Example <code>nodes</code> subfolder structure:</p> <pre><code>\u251c\u2500\u2500 __init__.py # Invoke-managed custom node loader\n\u2502\n\u251c\u2500\u2500 cool_node\n\u2502   \u251c\u2500\u2500 __init__.py # see example below\n\u2502   \u2514\u2500\u2500 cool_node.py\n\u2502\n\u2514\u2500\u2500 my_node_pack\n    \u251c\u2500\u2500 __init__.py # see example below\n    \u251c\u2500\u2500 tasty_node.py\n    \u251c\u2500\u2500 bodacious_node.py\n    \u251c\u2500\u2500 utils.py\n    \u2514\u2500\u2500 extra_nodes\n        \u2514\u2500\u2500 fancy_node.py\n</code></pre> <p>Each node folder must have an <code>__init__.py</code> file that imports its nodes. Only nodes imported in the <code>__init__.py</code> file are loaded. See the README in the nodes folder for more examples:</p> <pre><code>from .cool_node import ResizeInvocation\n</code></pre>"},{"location":"contributing/INVOCATIONS/#creating-a-new-invocation","title":"Creating A New Invocation","text":"<p>In order to understand the process of creating a new Invocation, let us actually create one.</p> <p>In our example, let us create an Invocation that will take in an image, resize it and output the resized image.</p> <p>The first set of things we need to do when creating a new Invocation are -</p> <ul> <li>Create a new class that derives from a predefined parent class called   <code>BaseInvocation</code>.</li> <li>Every Invocation must have a <code>docstring</code> that describes what this Invocation   does.</li> <li>While not strictly required, we suggest every invocation class name ends in   \"Invocation\", eg \"CropImageInvocation\".</li> <li>Every Invocation must use the <code>@invocation</code> decorator to provide its unique   invocation type. You may also provide its title, tags and category using the   decorator.</li> <li>Invocations are strictly typed. We make use of the native   typing library and the   installed pydantic library for   validation.</li> </ul> <p>So let us do that.</p> <pre><code>from invokeai.invocation_api import (\n    BaseInvocation,\n    invocation,\n)\n\n@invocation('resize')\nclass ResizeInvocation(BaseInvocation):\n    '''Resizes an image'''\n</code></pre> <p>That's great.</p> <p>Now we have setup the base of our new Invocation. Let us think about what inputs our Invocation takes.</p> <ul> <li>We need an <code>image</code> that we are going to resize.</li> <li>We will need new <code>width</code> and <code>height</code> values to which we need to resize the   image to.</li> </ul>"},{"location":"contributing/INVOCATIONS/#inputs","title":"Inputs","text":"<p>Every Invocation input must be defined using the <code>InputField</code> function. This is a wrapper around the pydantic <code>Field</code> function, which handles a few extra things and provides type hints. Like everything else, this should be strictly typed and defined.</p> <p>So let us create these inputs for our Invocation. First up, the <code>image</code> input we need. Generally, we can use standard variable types in Python but InvokeAI already has a custom <code>ImageField</code> type that handles all the stuff that is needed for image inputs.</p> <p>But what is this <code>ImageField</code> ..? It is a special class type specifically written to handle how images are dealt with in InvokeAI. We will cover how to create your own custom field types later in this guide. For now, let's go ahead and use it.</p> <pre><code>from invokeai.invocation_api import (\n    BaseInvocation,\n    ImageField,\n    InputField,\n    invocation,\n)\n\n@invocation('resize')\nclass ResizeInvocation(BaseInvocation):\n\n    # Inputs\n    image: ImageField = InputField(description=\"The input image\")\n</code></pre> <p>Let us break down our input code.</p> <pre><code>image: ImageField = InputField(description=\"The input image\")\n</code></pre> Part Value Description Name <code>image</code> The variable that will hold our image Type Hint <code>ImageField</code> The types for our field. Indicates that the image must be an <code>ImageField</code> type. Field <code>InputField(description=\"The input image\")</code> The image variable is an <code>InputField</code> which needs a description. <p>Great. Now let us create our other inputs for <code>width</code> and <code>height</code></p> <pre><code>from invokeai.invocation_api import (\n    BaseInvocation,\n    ImageField,\n    InputField,\n    invocation,\n)\n\n@invocation('resize')\nclass ResizeInvocation(BaseInvocation):\n    '''Resizes an image'''\n\n    image: ImageField = InputField(description=\"The input image\")\n    width: int = InputField(default=512, ge=64, le=2048, description=\"Width of the new image\")\n    height: int = InputField(default=512, ge=64, le=2048, description=\"Height of the new image\")\n</code></pre> <p>As you might have noticed, we added two new arguments to the <code>InputField</code> definition for <code>width</code> and <code>height</code>, called <code>gt</code> and <code>le</code>. They stand for greater than or equal to and less than or equal to.</p> <p>These impose constraints on those fields, and will raise an exception if the values do not meet the constraints. Field constraints are provided by pydantic, so anything you see in the pydantic docs will work.</p> <p>Note: Any time it is possible to define constraints for our field, we should do it so the frontend has more information on how to parse this field.</p> <p>Perfect. We now have our inputs. Let us do something with these.</p>"},{"location":"contributing/INVOCATIONS/#invoke-function","title":"Invoke Function","text":"<p>The <code>invoke</code> function is where all the magic happens. This function provides you the <code>context</code> parameter that is of the type <code>InvocationContext</code> which will give you access to the current context of the generation and all the other services that are provided by it by InvokeAI.</p> <p>Let us create this function first.</p> <pre><code>from invokeai.invocation_api import (\n    BaseInvocation,\n    ImageField,\n    InputField,\n    InvocationContext,\n    invocation,\n)\n\n@invocation('resize')\nclass ResizeInvocation(BaseInvocation):\n    '''Resizes an image'''\n\n    image: ImageField = InputField(description=\"The input image\")\n    width: int = InputField(default=512, ge=64, le=2048, description=\"Width of the new image\")\n    height: int = InputField(default=512, ge=64, le=2048, description=\"Height of the new image\")\n\n    def invoke(self, context: InvocationContext):\n        pass\n</code></pre>"},{"location":"contributing/INVOCATIONS/#outputs","title":"Outputs","text":"<p>The output of our Invocation will be whatever is returned by this <code>invoke</code> function. Like with our inputs, we need to strongly type and define our outputs too.</p> <p>What is our output going to be? Another image. Normally you'd have to create a type for this but InvokeAI already offers you an <code>ImageOutput</code> type that handles all the necessary info related to image outputs. So let us use that.</p> <p>We will cover how to create your own output types later in this guide.</p> <pre><code>from invokeai.invocation_api import (\n    BaseInvocation,\n    ImageField,\n    InputField,\n    InvocationContext,\n    invocation,\n)\n\nfrom invokeai.app.invocations.image import ImageOutput\n\n@invocation('resize')\nclass ResizeInvocation(BaseInvocation):\n    '''Resizes an image'''\n\n    image: ImageField = InputField(description=\"The input image\")\n    width: int = InputField(default=512, ge=64, le=2048, description=\"Width of the new image\")\n    height: int = InputField(default=512, ge=64, le=2048, description=\"Height of the new image\")\n\n    def invoke(self, context: InvocationContext) -&gt; ImageOutput:\n        pass\n</code></pre> <p>Perfect. Now that we have our Invocation setup, let us do what we want to do.</p> <ul> <li>We will first load the image using one of the services provided by InvokeAI to   load the image.</li> <li>We will resize the image using <code>PIL</code> to our input data.</li> <li>We will output this image in the format we set above.</li> </ul> <p>So let's do that.</p> <pre><code>from invokeai.invocation_api import (\n    BaseInvocation,\n    ImageField,\n    InputField,\n    InvocationContext,\n    invocation,\n)\n\nfrom invokeai.app.invocations.image import ImageOutput\n\n@invocation(\"resize\")\nclass ResizeInvocation(BaseInvocation):\n    \"\"\"Resizes an image\"\"\"\n\n    image: ImageField = InputField(description=\"The input image\")\n    width: int = InputField(default=512, ge=64, le=2048, description=\"Width of the new image\")\n    height: int = InputField(default=512, ge=64, le=2048, description=\"Height of the new image\")\n\n    def invoke(self, context: InvocationContext) -&gt; ImageOutput:\n        # Load the input image as a PIL image\n        image = context.images.get_pil(self.image.image_name)\n\n        # Resize the image\n        resized_image = image.resize((self.width, self.height))\n\n        # Save the image\n        image_dto = context.images.save(image=resized_image)\n\n        # Return an ImageOutput\n        return ImageOutput.build(image_dto)\n</code></pre> <p>Note: Do not be overwhelmed by the <code>ImageOutput</code> process. InvokeAI has a certain way that the images need to be dispatched in order to be stored and read correctly. In 99% of the cases when dealing with an image output, you can simply copy-paste the template above.</p>"},{"location":"contributing/INVOCATIONS/#customization","title":"Customization","text":"<p>We can use the <code>@invocation</code> decorator to provide some additional info to the UI, like a custom title, tags and category.</p> <p>We also encourage providing a version. This must be a semver version string (\"\\(MAJOR.\\)MINOR.$PATCH\"). The UI will let users know if their workflow is using a mismatched version of the node.</p> <pre><code>@invocation(\"resize\", title=\"My Resizer\", tags=[\"resize\", \"image\"], category=\"My Invocations\", version=\"1.0.0\")\nclass ResizeInvocation(BaseInvocation):\n    \"\"\"Resizes an image\"\"\"\n\n    image: ImageField = InputField(description=\"The input image\")\n    ...\n</code></pre> <p>That's it. You made your own Resize Invocation.</p>"},{"location":"contributing/INVOCATIONS/#result","title":"Result","text":"<p>Once you make your Invocation correctly, the rest of the process is fully automated for you.</p> <p>When you launch InvokeAI, you can go to <code>http://localhost:9090/docs</code> and see your new Invocation show up there with all the relevant info.</p> <p></p> <p>When you launch the frontend UI, you can go to the Node Editor tab and find your new Invocation ready to be used.</p> <p></p>"},{"location":"contributing/INVOCATIONS/#contributing-nodes","title":"Contributing Nodes","text":"<p>Once you've created a Node, the next step is to share it with the community! The best way to do this is to submit a Pull Request to add the Node to the Community Nodes list. If you're not sure how to do that, take a look a at our contributing nodes overview.</p>"},{"location":"contributing/INVOCATIONS/#advanced","title":"Advanced","text":""},{"location":"contributing/INVOCATIONS/#custom-output-types","title":"Custom Output Types","text":"<p>Like with custom inputs, sometimes you might find yourself needing custom outputs that InvokeAI does not provide. We can easily set one up.</p> <p>Now that you are familiar with Invocations and Inputs, let us use that knowledge to create an output that has an <code>image</code> field, a <code>color</code> field and a <code>string</code> field.</p> <ul> <li>An invocation output is a class that derives from the parent class of   <code>BaseInvocationOutput</code>.</li> <li>All invocation outputs must use the <code>@invocation_output</code> decorator to provide   their unique output type.</li> <li>Output fields must use the provided <code>OutputField</code> function. This is very   similar to the <code>InputField</code> function described earlier - it's a wrapper around   <code>pydantic</code>'s <code>Field()</code>.</li> <li>It is not mandatory but we recommend using names ending with <code>Output</code> for   output types.</li> <li>It is not mandatory but we highly recommend adding a <code>docstring</code> to describe   what your output type is for.</li> </ul> <p>Now that we know the basic rules for creating a new output type, let us go ahead and make it.</p> <pre><code>from .baseinvocation import BaseInvocationOutput, OutputField, invocation_output\nfrom .primitives import ImageField, ColorField\n\n@invocation_output('image_color_string_output')\nclass ImageColorStringOutput(BaseInvocationOutput):\n    '''Base class for nodes that output a single image'''\n\n    image: ImageField = OutputField(description=\"The image\")\n    color: ColorField = OutputField(description=\"The color\")\n    text: str = OutputField(description=\"The string\")\n</code></pre> <p>That's all there is to it.</p>"},{"location":"contributing/INVOCATIONS/#custom-input-fields","title":"Custom Input Fields","text":"<p>Now that you know how to create your own Invocations, let us dive into slightly more advanced topics.</p> <p>While creating your own Invocations, you might run into a scenario where the existing fields in InvokeAI do not meet your requirements. In such cases, you can create your own fields.</p> <p>Let us create one as an example. Let us say we want to create a color input field that represents a color code. But before we start on that here are some general good practices to keep in mind.</p>"},{"location":"contributing/INVOCATIONS/#best-practices","title":"Best Practices","text":"<ul> <li>There is no naming convention for input fields but we highly recommend that   you name it something appropriate like <code>ColorField</code>.</li> <li>It is not mandatory but it is heavily recommended to add a relevant   <code>docstring</code> to describe your field.</li> <li>Keep your field in the same file as the Invocation that it is made for or in   another file where it is relevant.</li> </ul> <p>All input types a class that derive from the <code>BaseModel</code> type from <code>pydantic</code>. So let's create one.</p> <pre><code>from pydantic import BaseModel\n\nclass ColorField(BaseModel):\n    '''A field that holds the rgba values of a color'''\n    pass\n</code></pre> <p>Perfect. Now let us create the properties for our field. This is similar to how you created input fields for your Invocation. All the same rules apply. Let us create four fields representing the red\u00ae, blue(b), green(g) and alpha(a) channel of the color.</p> <p>Technically, the properties are also called fields - but in this case, it refers to a <code>pydantic</code> field.</p> <pre><code>class ColorField(BaseModel):\n    '''A field that holds the rgba values of a color'''\n    r: int = Field(ge=0, le=255, description=\"The red channel\")\n    g: int = Field(ge=0, le=255, description=\"The green channel\")\n    b: int = Field(ge=0, le=255, description=\"The blue channel\")\n    a: int = Field(ge=0, le=255, description=\"The alpha channel\")\n</code></pre> <p>That's it. We now have a new input field type that we can use in our Invocations like this.</p> <pre><code>color: ColorField = InputField(default=ColorField(r=0, g=0, b=0, a=0), description='Background color of an image')\n</code></pre>"},{"location":"contributing/INVOCATIONS/#using-the-custom-field","title":"Using the custom field","text":"<p>When you start the UI, your custom field will be automatically recognized.</p> <p>Custom fields only support connection inputs in the Workflow Editor.</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/","title":"Local Development","text":"<p>If you want to contribute, you will need to set up a local development environment.</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/#documentation","title":"Documentation","text":"<p>We use mkdocs for our documentation with the material theme. Documentation is written in markdown files under the <code>./docs</code> folder and then built into a static website for hosting with GitHub Pages at invoke-ai.github.io/InvokeAI.</p> <p>To contribute to the documentation you'll need to install the dependencies. Note the use of <code>\"</code>.</p> <pre><code>pip install \".[docs]\"\n</code></pre> <p>Now, to run the documentation locally with hot-reloading for changes made.</p> <pre><code>mkdocs serve\n</code></pre> <p>You'll then be prompted to connect to <code>http://127.0.0.1:8080</code> in order to access.</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/#backend","title":"Backend","text":"<p>The backend is contained within the <code>./invokeai/backend</code> and <code>./invokeai/app</code> directories. To get started please install the development dependencies.</p> <p>From the root of the repository run the following command. Note the use of <code>\"</code>.</p> <pre><code>pip install \".[dev,test]\"\n</code></pre> <p>These are optional groups of packages which are defined within the <code>pyproject.toml</code> and will be required for testing the changes you make to the code.</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/#tests","title":"Tests","text":"<p>See the tests documentation for information about running and writing tests.</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/#reloading-changes","title":"Reloading Changes","text":"<p>Experimenting with changes to the Python source code is a drag if you have to re-start the server \u2014 and re-load those multi-gigabyte models \u2014 after every change.</p> <p>For a faster development workflow, add the <code>--dev_reload</code> flag when starting the server. The server will watch for changes to all the Python files in the <code>invokeai</code> directory and apply those changes to the running server on the fly.</p> <p>This will allow you to avoid restarting the server (and reloading models) in most cases, but there are some caveats; see the jurigged documentation for details.</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/#front-end","title":"Front End","text":""},{"location":"contributing/LOCAL_DEVELOPMENT/#invoke-ui","title":"Invoke UI","text":"<p>https://invoke-ai.github.io/InvokeAI/contributing/frontend/</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/#developing-invokeai-in-vscode","title":"Developing InvokeAI in VSCode","text":"<p>VSCode offers some nice tools:</p> <ul> <li>python debugger</li> <li>automatic <code>venv</code> activation</li> <li>remote dev (e.g. run InvokeAI on a beefy linux desktop while you type in   comfort on your macbook)</li> </ul>"},{"location":"contributing/LOCAL_DEVELOPMENT/#setup","title":"Setup","text":"<p>You'll need the Python and Pylance extensions installed first.</p> <p>It's also really handy to install the <code>Jupyter</code> extensions:</p> <ul> <li>Jupyter</li> <li>Jupyter Cell Tags</li> <li>Jupyter Notebook Renderers</li> <li>Jupyter Slide Show</li> </ul>"},{"location":"contributing/LOCAL_DEVELOPMENT/#invokeai-workspace","title":"InvokeAI workspace","text":"<p>Creating a VSCode workspace for working on InvokeAI is highly recommended. It can hold InvokeAI-specific settings and configs.</p> <p>To make a workspace:</p> <ul> <li>Open the InvokeAI repo dir in VSCode</li> <li><code>File</code> &gt; <code>Save Workspace As</code> &gt; save it outside the repo</li> </ul>"},{"location":"contributing/LOCAL_DEVELOPMENT/#default-python-interpreter-ie-automatic-virtual-environment-activation","title":"Default python interpreter (i.e. automatic virtual environment activation)","text":"<ul> <li>Use command palette to run command   <code>Preferences: Open Workspace Settings (JSON)</code></li> <li>Add <code>python.defaultInterpreterPath</code> to <code>settings</code>, pointing to your <code>venv</code>'s   python</li> </ul> <p>Should look something like this:</p> <pre><code>{\n  // I like to have all InvokeAI-related folders in my workspace\n  \"folders\": [\n    {\n      // repo root\n      \"path\": \"InvokeAI\"\n    },\n    {\n      // InvokeAI root dir, where `invokeai.yaml` lives\n      \"path\": \"/path/to/invokeai_root\"\n    }\n  ],\n  \"settings\": {\n    // Where your InvokeAI `venv`'s python executable lives\n    \"python.defaultInterpreterPath\": \"/path/to/invokeai_root/.venv/bin/python\"\n  }\n}\n</code></pre> <p>Now when you open the VSCode integrated terminal, or do anything that needs to run python, it will automatically be in your InvokeAI virtual environment.</p> <p>Bonus: When you create a Jupyter notebook, when you run it, you'll be prompted for the python interpreter to run in. This will default to your <code>venv</code> python, and so you'll have access to the same python environment as the InvokeAI app.</p> <p>This is super handy.</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/#enabling-type-checking-with-pylance","title":"Enabling Type-Checking with Pylance","text":"<p>We use python's typing system in InvokeAI. PR reviews will include checking that types are present and correct. We don't enforce types with <code>mypy</code> at this time, but that is on the horizon.</p> <p>Using a code analysis tool to automatically type check your code (and types) is very important when writing with types. These tools provide immediate feedback in your editor when types are incorrect, and following their suggestions lead to fewer runtime bugs.</p> <p>Pylance, installed at the beginning of this guide, is the de-facto python LSP (language server protocol). It provides type checking in the editor (among many other features). Once installed, you do need to enable type checking manually:</p> <ul> <li>Open a python file</li> <li>Look along the status bar in VSCode for <code>{ } Python</code></li> <li>Click the <code>{ }</code></li> <li>Turn type checking on - basic is fine</li> </ul> <p>You'll now see red squiggly lines where type issues are detected. Hover your cursor over the indicated symbols to see what's wrong.</p> <p>In 99% of cases when the type checker says there is a problem, there really is a problem, and you should take some time to understand and resolve what it is pointing out.</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/#debugging-configs-with-launchjson","title":"Debugging configs with <code>launch.json</code>","text":"<p>Debugging configs are managed in a <code>launch.json</code> file. Like most VSCode configs, these can be scoped to a workspace or folder.</p> <p>Follow the official guide to set up your <code>launch.json</code> and try it out.</p> <p>Now we can create the InvokeAI debugging configs:</p> <pre><code>{\n  // Use IntelliSense to learn about possible attributes.\n  // Hover to view descriptions of existing attributes.\n  // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      // Run the InvokeAI backend &amp; serve the pre-built UI\n      \"name\": \"InvokeAI Web\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      \"program\": \"scripts/invokeai-web.py\",\n      \"args\": [\n        // Your InvokeAI root dir (where `invokeai.yaml` lives)\n        \"--root\",\n        \"/path/to/invokeai_root\",\n        // Access the app from anywhere on your local network\n        \"--host\",\n        \"0.0.0.0\"\n      ],\n      \"justMyCode\": true\n    },\n    {\n      // Run the nodes-based CLI\n      \"name\": \"InvokeAI CLI\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      \"program\": \"scripts/invokeai-cli.py\",\n      \"justMyCode\": true\n    },\n    {\n      // Run tests\n      \"name\": \"InvokeAI Test\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      \"module\": \"pytest\",\n      \"args\": [\"--capture=no\"],\n      \"justMyCode\": true\n    },\n    {\n      // Run a single test\n      \"name\": \"InvokeAI Single Test\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      \"module\": \"pytest\",\n      \"args\": [\n        // Change this to point to the specific test you are working on\n        \"tests/nodes/test_invoker.py\"\n      ],\n      \"justMyCode\": true\n    },\n    {\n      // This is the default, useful to just run a single file\n      \"name\": \"Python: File\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      \"program\": \"${file}\",\n      \"justMyCode\": true\n    }\n  ]\n}\n</code></pre> <p>You'll see these configs in the debugging configs drop down. Running them will start InvokeAI with attached debugger, in the correct environment, and work just like the normal app.</p> <p>Enjoy debugging InvokeAI with ease (not that we have any bugs of course).</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/#remote-dev","title":"Remote dev","text":"<p>This is very easy to set up and provides the same very smooth experience as local development. Environments and debugging, as set up above, just work, though you'd need to recreate the workspace and debugging configs on the remote.</p> <p>Consult the official guide to get it set up.</p> <p>Suggest using VSCode's included settings sync so that your remote dev host has all the same app settings and extensions automatically.</p>"},{"location":"contributing/LOCAL_DEVELOPMENT/#one-remote-dev-gotcha","title":"One remote dev gotcha","text":"<p>I've found the automatic port forwarding to be very flakey. You can disable it in <code>Preferences: Open Remote Settings (ssh: hostname)</code>. Search for <code>remote.autoForwardPorts</code> and untick the box.</p> <p>To forward ports very reliably, use SSH on the remote dev client (e.g. your macbook). Here's how to forward both backend API port (<code>9090</code>) and the frontend live dev server port (<code>5173</code>):</p> <pre><code>ssh \\\n    -L 9090:localhost:9090 \\\n    -L 5173:localhost:5173 \\\n    user@remote-dev-host\n</code></pre> <p>The forwarding stops when you close the terminal window, so suggest to do this outside the VSCode integrated terminal in case you need to restart VSCode for an extension update or something</p> <p>Now, on your remote dev client, you can open <code>localhost:9090</code> and access the UI, now served from the remote dev host, just the same as if it was running on the client.</p>"},{"location":"contributing/MODEL_MANAGER/","title":"Introduction to the Model Manager V2","text":"<p>The Model Manager is responsible for organizing the various machine learning models used by InvokeAI. It consists of a series of interdependent services that together handle the full lifecycle of a model. These are the:</p> <ul> <li> <p>ModelRecordServiceBase Responsible for managing model metadata and   configuration information. Among other things, the record service   tracks the type of the model, its provenance, and where it can be   found on disk.</p> </li> <li> <p>ModelInstallServiceBase A service for installing models to   disk. It uses <code>DownloadQueueServiceBase</code> to download models and   their metadata, and <code>ModelRecordServiceBase</code> to store that   information. It is also responsible for managing the InvokeAI   <code>models</code> directory and its contents.</p> </li> <li> <p>DownloadQueueServiceBase   A multithreaded downloader responsible   for downloading models from a remote source to disk. The download   queue has special methods for downloading repo_id folders from   Hugging Face, as well as discriminating among model versions in   Civitai, but can be used for arbitrary content.</p> </li> <li> <p>ModelLoadServiceBase   Responsible for loading a model from disk   into RAM and VRAM and getting it ready for inference.</p> </li> </ul>"},{"location":"contributing/MODEL_MANAGER/#location-of-the-code","title":"Location of the Code","text":"<p>The four main services can be found in <code>invokeai/app/services</code> in the following directories:</p> <ul> <li><code>invokeai/app/services/model_records/</code></li> <li><code>invokeai/app/services/model_install/</code></li> <li><code>invokeai/app/services/downloads/</code></li> <li><code>invokeai/app/services/model_load/</code></li> </ul> <p>Code related to the FastAPI web API can be found in <code>invokeai/app/api/routers/model_manager_v2.py</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#whats-in-a-model-the-modelrecordservice","title":"What's in a Model? The ModelRecordService","text":"<p>The <code>ModelRecordService</code> manages the model's metadata. It supports a hierarchy of pydantic metadata \"config\" objects, which become increasingly specialized to support particular model types.</p>"},{"location":"contributing/MODEL_MANAGER/#modelconfigbase","title":"ModelConfigBase","text":"<p>All model metadata classes inherit from this pydantic class. it provides the following fields:</p> Field Name Type Description <code>key</code> str Unique identifier for the model <code>name</code> str Name of the model (not unique) <code>model_type</code> ModelType The type of the model <code>model_format</code> ModelFormat The format of the model (e.g. \"diffusers\"); also used as a Union discriminator <code>base_model</code> BaseModelType The base model that the model is compatible with <code>path</code> str Location of model on disk <code>hash</code> str Hash of the model <code>description</code> str Human-readable description of the model (optional) <code>source</code> str Model's source URL or repo id (optional) <p>The <code>key</code> is a unique 32-character random ID which was generated at install time. The <code>hash</code> field stores a hash of the model's contents at install time obtained by sampling several parts of the model's files using the <code>imohash</code> library. Over the course of the model's lifetime it may be transformed in various ways, such as changing its precision or converting it from a .safetensors to a diffusers model.</p> <p><code>ModelType</code>, <code>ModelFormat</code> and <code>BaseModelType</code> are string enums that are defined in <code>invokeai.backend.model_manager.config</code>. They are also imported by, and can be reexported from, <code>invokeai.app.services.model_manager.model_records</code>:</p> <pre><code>from invokeai.app.services.model_records import ModelType, ModelFormat, BaseModelType\n</code></pre> <p>The <code>path</code> field can be absolute or relative. If relative, it is taken to be relative to the <code>models_dir</code> setting in the user's <code>invokeai.yaml</code> file.</p>"},{"location":"contributing/MODEL_MANAGER/#checkpointconfig","title":"CheckpointConfig","text":"<p>This adds support for checkpoint configurations, and adds the following field:</p> Field Name Type Description <code>config</code> str Path to the checkpoint's config file <p><code>config</code> is the path to the checkpoint's config file. If relative, it is taken to be relative to the InvokeAI root directory (e.g. <code>configs/stable-diffusion/v1-inference.yaml</code>)</p>"},{"location":"contributing/MODEL_MANAGER/#mainconfig","title":"MainConfig","text":"<p>This adds support for \"main\" Stable Diffusion models, and adds these fields:</p> Field Name Type Description <code>vae</code> str Path to a VAE to use instead of the burnt-in one <code>variant</code> ModelVariantType Model variant type, such as \"inpainting\" <p><code>vae</code> can be an absolute or relative path. If relative, its base is taken to be the <code>models_dir</code> directory.</p> <p><code>variant</code> is an enumerated string class with values <code>normal</code>, <code>inpaint</code> and <code>depth</code>. If needed, it can be imported if needed from either <code>invokeai.app.services.model_records</code> or <code>invokeai.backend.model_manager.config</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#onnxsd2config","title":"ONNXSD2Config","text":"Field Name Type Description <code>prediction_type</code> SchedulerPredictionType Scheduler prediction type to use, e.g. \"epsilon\" <code>upcast_attention</code> bool Model requires its attention module to be upcast <p>The <code>SchedulerPredictionType</code> enum can be imported from either <code>invokeai.app.services.model_records</code> or <code>invokeai.backend.model_manager.config</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#other-config-classes","title":"Other config classes","text":"<p>There are a series of such classes each discriminated by their <code>ModelFormat</code>, including <code>LoRAConfig</code>, <code>IPAdapterConfig</code>, and so forth. These are rarely needed outside the model manager's internal code, but available in <code>invokeai.backend.model_manager.config</code> if needed. There is also a Union of all ModelConfig classes, called <code>AnyModelConfig</code> that can be imported from the same file.</p>"},{"location":"contributing/MODEL_MANAGER/#limitations-of-the-data-model","title":"Limitations of the Data Model","text":"<p>The config hierarchy has a major limitation in its handling of the base model type. Each model can only be compatible with one base model, which breaks down in the event of models that are compatible with two or more base models. For example, SD-1 VAEs also work with SD-2 models. A partial workaround is to use <code>BaseModelType.Any</code>, which indicates that the model is compatible with any of the base models. This works OK for some models, such as the IP Adapter image encoders, but is an all-or-nothing proposition.</p>"},{"location":"contributing/MODEL_MANAGER/#reading-and-writing-model-configuration-records","title":"Reading and Writing Model Configuration Records","text":"<p>The <code>ModelRecordService</code> provides the ability to retrieve model configuration records from SQL or YAML databases, update them, and write them back.</p> <p>A application-wide <code>ModelRecordService</code> is created during API initialization and can be retrieved within an invocation from the <code>InvocationContext</code> object:</p> <pre><code>store = context.services.model_manager.store\n</code></pre> <p>or from elsewhere in the code by accessing <code>ApiDependencies.invoker.services.model_manager.store</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#creating-a-modelrecordservice","title":"Creating a <code>ModelRecordService</code>","text":"<p>To create a new <code>ModelRecordService</code> database or open an existing one, you can directly create either a <code>ModelRecordServiceSQL</code> or a <code>ModelRecordServiceFile</code> object:</p> <pre><code>from invokeai.app.services.model_records import ModelRecordServiceSQL, ModelRecordServiceFile\n\nstore = ModelRecordServiceSQL.from_connection(connection, lock)\nstore = ModelRecordServiceSQL.from_db_file('/path/to/sqlite_database.db')\nstore = ModelRecordServiceFile.from_db_file('/path/to/database.yaml')\n</code></pre> <p>The <code>from_connection()</code> form is only available from the <code>ModelRecordServiceSQL</code> class, and is used to manage records in a previously-opened SQLITE3 database using a <code>sqlite3.connection</code> object and a <code>threading.lock</code> object. It is intended for the specific use case of storing the record information in the main InvokeAI database, usually <code>databases/invokeai.db</code>.</p> <p>The <code>from_db_file()</code> methods can be used to open new connections to the named database files. If the file doesn't exist, it will be created and initialized.</p> <p>As a convenience, <code>ModelRecordServiceBase</code> offers two methods, <code>from_db_file</code> and <code>open</code>, which will return either a SQL or File implementation depending on the context. The former looks at the file extension to determine whether to open the file as a SQL database (\".db\") or as a file database (\".yaml\"). If the file exists, but is either the wrong type or does not contain the expected schema metainformation, then an appropriate <code>AssertionError</code> will be raised:</p> <pre><code>store = ModelRecordServiceBase.from_db_file('/path/to/a/file.{yaml,db}')\n</code></pre> <p>The <code>ModelRecordServiceBase.open()</code> method is specifically designed for use in the InvokeAI web server. Its signature is:</p> <pre><code>def open(\n       cls,\n    config: InvokeAIAppConfig,\n    conn: Optional[sqlite3.Connection] = None,\n    lock: Optional[threading.Lock] = None\n    ) -&gt; Union[ModelRecordServiceSQL, ModelRecordServiceFile]:\n</code></pre> <p>The way it works is as follows:</p> <ol> <li>Retrieve the value of the <code>model_config_db</code> option from the user's  <code>invokeai.yaml</code> config file.</li> <li>If <code>model_config_db</code> is <code>auto</code> (the default), then:</li> <li>Use the values of <code>conn</code> and <code>lock</code> to return a <code>ModelRecordServiceSQL</code> object   opened on the passed connection and lock.</li> <li>Open up a new connection to <code>databases/invokeai.db</code> if <code>conn</code>      and/or <code>lock</code> are missing (see note below).</li> <li>If <code>model_config_db</code> is a Path, then use <code>from_db_file</code>    to return the appropriate type of ModelRecordService.</li> <li>If <code>model_config_db</code> is None, then retrieve the legacy    <code>conf_path</code> option from <code>invokeai.yaml</code> and use the Path    indicated there. This will default to <code>configs/models.yaml</code>.</li> </ol> <p>So a typical startup pattern would be:</p> <pre><code>import sqlite3\nfrom invokeai.app.services.thread import lock\nfrom invokeai.app.services.model_records import ModelRecordServiceBase\nfrom invokeai.app.services.config import InvokeAIAppConfig\n\nconfig = InvokeAIAppConfig.get_config()\ndb_conn = sqlite3.connect(config.db_path.as_posix(), check_same_thread=False)\nstore = ModelRecordServiceBase.open(config, db_conn, lock)\n</code></pre>"},{"location":"contributing/MODEL_MANAGER/#fetching-a-models-configuration-from-modelrecordservicebase","title":"Fetching a Model's Configuration from <code>ModelRecordServiceBase</code>","text":"<p>Configurations can be retrieved in several ways.</p>"},{"location":"contributing/MODEL_MANAGER/#get_modelkey-anymodelconfig","title":"get_model(key) -&gt; AnyModelConfig","text":"<p>The basic functionality is to call the record store object's <code>get_model()</code> method with the desired model's unique key. It returns the appropriate subclass of ModelConfigBase:</p> <pre><code>model_conf = store.get_model('f13dd932c0c35c22dcb8d6cda4203764')\nprint(model_conf.path)\n\n&gt;&gt; '/tmp/models/ckpts/v1-5-pruned-emaonly.safetensors'\n</code></pre> <p>If the key is unrecognized, this call raises an <code>UnknownModelException</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#existskey-anymodelconfig","title":"exists(key) -&gt; AnyModelConfig","text":"<p>Returns True if a model with the given key exists in the database.</p>"},{"location":"contributing/MODEL_MANAGER/#search_by_pathpath-anymodelconfig","title":"search_by_path(path) -&gt; AnyModelConfig","text":"<p>Returns the configuration of the model whose path is <code>path</code>. The path is matched using a simple string comparison and won't correctly match models referred to by different paths (e.g. using symbolic links).</p>"},{"location":"contributing/MODEL_MANAGER/#search_by_namename-base-type-listanymodelconfig","title":"search_by_name(name, base, type) -&gt; List[AnyModelConfig]","text":"<p>This method searches for models that match some combination of <code>name</code>, <code>BaseType</code> and <code>ModelType</code>. Calling without any arguments will return all the models in the database.</p>"},{"location":"contributing/MODEL_MANAGER/#all_models-listanymodelconfig","title":"all_models() -&gt; List[AnyModelConfig]","text":"<p>Return all the model configs in the database. Exactly equivalent to calling <code>search_by_name()</code> with no arguments.</p>"},{"location":"contributing/MODEL_MANAGER/#search_by_tagtags-listanymodelconfig","title":"search_by_tag(tags) -&gt; List[AnyModelConfig]","text":"<p><code>tags</code> is a list of strings. This method returns a list of model configs that contain all of the given tags. Examples:</p> <pre><code># find all models that are marked as both SFW and as generating\n# background scenery\nconfigs = store.search_by_tag(['sfw', 'scenery'])\n</code></pre> <p>Note that only tags are not searchable in this way. Other fields can be searched using a filter:</p> <pre><code>commercializable_models = [x for x in store.all_models() \\\n                           if x.license.contains('allowCommercialUse=Sell')]\n</code></pre>"},{"location":"contributing/MODEL_MANAGER/#version-str","title":"version() -&gt; str","text":"<p>Returns the version of the database, currently at <code>3.2</code></p>"},{"location":"contributing/MODEL_MANAGER/#model_info_by_namename-base_model-model_type-modelconfigbase","title":"model_info_by_name(name, base_model, model_type) -&gt; ModelConfigBase","text":"<p>This method exists to ease the transition from the previous version of the model manager, in which <code>get_model()</code> took the three arguments shown above. This looks for a unique model identified by name, base model and model type and returns it.</p> <p>The method will generate a <code>DuplicateModelException</code> if there are more than one models that share the same type, base and name. While unlikely, it is certainly possible to have a situation in which the user had added two models with the same name, base and type, one located at path <code>/foo/my_model</code> and the other at <code>/bar/my_model</code>. It is strongly recommended to search for models using <code>search_by_name()</code>, which can return multiple results, and then to select the desired model and pass its key to <code>get_model()</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#writing-model-configs-to-the-database","title":"Writing model configs to the database","text":"<p>Several methods allow you to create and update stored model config records.</p>"},{"location":"contributing/MODEL_MANAGER/#add_modelkey-config-anymodelconfig","title":"add_model(key, config) -&gt; AnyModelConfig","text":"<p>Given a key and a configuration, this will add the model's configuration record to the database. <code>config</code> can either be a subclass of <code>ModelConfigBase</code> (i.e. any class listed in <code>AnyModelConfig</code>), or a <code>dict</code> of key/value pairs. In the latter case, the correct configuration class will be picked by Pydantic's discriminated union mechanism.</p> <p>If successful, the method will return the appropriate subclass of <code>ModelConfigBase</code>. It will raise a <code>DuplicateModelException</code> if a model with the same key is already in the database, or an <code>InvalidModelConfigException</code> if a dict was passed and Pydantic experienced a parse or validation error.</p>"},{"location":"contributing/MODEL_MANAGER/#update_modelkey-config-anymodelconfig","title":"update_model(key, config) -&gt; AnyModelConfig","text":"<p>Given a key and a configuration, this will update the model configuration record in the database. <code>config</code> can be either a instance of <code>ModelConfigBase</code>, or a sparse <code>dict</code> containing the fields to be updated. This will return an <code>AnyModelConfig</code> on success, or raise <code>InvalidModelConfigException</code> or <code>UnknownModelException</code> exceptions on failure.</p>"},{"location":"contributing/MODEL_MANAGER/#model-installation","title":"Model installation","text":"<p>The <code>ModelInstallService</code> class implements the <code>ModelInstallServiceBase</code> abstract base class, and provides a one-stop shop for all your model install needs. It provides the following functionality:</p> <ul> <li> <p>Registering a model config record for a model already located on the   local filesystem, without moving it or changing its path.</p> </li> <li> <p>Installing a model alreadiy located on the local filesystem, by   moving it into the InvokeAI root directory under the   <code>models</code> folder (or wherever config parameter <code>models_dir</code>   specifies).</p> </li> <li> <p>Probing of models to determine their type, base type and other key   information.</p> </li> <li> <p>Interface with the InvokeAI event bus to provide status updates on   the download, installation and registration process.</p> </li> <li> <p>Downloading a model from an arbitrary URL and installing it in   <code>models_dir</code>.</p> </li> <li> <p>Special handling for HuggingFace repo_ids to recursively download   the contents of the repository, paying attention to alternative   variants such as fp16.</p> </li> <li> <p>Saving tags and other metadata about the model into the invokeai database   when fetching from a repo that provides that type of information,   (currently only HuggingFace).</p> </li> </ul>"},{"location":"contributing/MODEL_MANAGER/#initializing-the-installer","title":"Initializing the installer","text":"<p>A default installer is created at InvokeAI api startup time and stored in <code>ApiDependencies.invoker.services.model_install</code> and can also be retrieved from an invocation's <code>context</code> argument with <code>context.services.model_install</code>.</p> <p>In the event you wish to create a new installer, you may use the following initialization pattern:</p> <pre><code>from invokeai.app.services.config import get_config\nfrom invokeai.app.services.model_records import ModelRecordServiceSQL\nfrom invokeai.app.services.model_install import ModelInstallService\nfrom invokeai.app.services.download import DownloadQueueService\nfrom invokeai.app.services.shared.sqlite.sqlite_database import SqliteDatabase\nfrom invokeai.backend.util.logging import InvokeAILogger\n\nconfig = get_config()\n\nlogger = InvokeAILogger.get_logger(config=config)\ndb = SqliteDatabase(config.db_path, logger)\nrecord_store = ModelRecordServiceSQL(db, logger)\nqueue = DownloadQueueService()\nqueue.start()\n\ninstaller = ModelInstallService(app_config=config,\n                                record_store=record_store,\n                                download_queue=queue\n                                )\ninstaller.start()\n</code></pre> <p>The full form of <code>ModelInstallService()</code> takes the following required parameters:</p> Argument Type Description <code>app_config</code> InvokeAIAppConfig InvokeAI app configuration object <code>record_store</code> ModelRecordServiceBase Config record storage database <code>download_queue</code> DownloadQueueServiceBase Download queue object <code>session</code> Optional[requests.Session] Swap in a different Session object (usually for debugging) <p>Once initialized, the installer will provide the following methods:</p>"},{"location":"contributing/MODEL_MANAGER/#install_job-installerheuristic_importsource-config-access_token","title":"install_job = installer.heuristic_import(source, [config], [access_token])","text":"<p>This is a simplified interface to the installer which takes a source string, an optional model configuration dictionary and an optional access token.</p> <p>The <code>source</code> is a string that can be any of these forms</p> <ol> <li>A path on the local filesystem (<code>C:\\\\users\\\\fred\\\\model.safetensors</code>)</li> <li>A Url pointing to a single downloadable model file (<code>https://civitai.com/models/58390/detail-tweaker-lora-lora</code>)</li> <li>A HuggingFace repo_id with any of the following formats:</li> <li><code>model/name</code> -- entire model</li> <li><code>model/name:fp32</code> -- entire model, using the fp32 variant</li> <li><code>model/name:fp16:vae</code> -- vae submodel, using the fp16 variant</li> <li><code>model/name::vae</code> -- vae submodel, using default precision</li> <li><code>model/name:fp16:path/to/model.safetensors</code> -- an individual model file, fp16 variant</li> <li><code>model/name::path/to/model.safetensors</code> -- an individual model file, default variant</li> </ol> <p>Note that by specifying a relative path to the top of the HuggingFace repo, you can download and install arbitrary models files.</p> <p>The variant, if not provided, will be automatically filled in with <code>fp32</code> if the user has requested full precision, and <code>fp16</code> otherwise. If a variant that does not exist is requested, then the method will install whatever HuggingFace returns as its default revision.</p> <p><code>config</code> is an optional dict of values that will override the autoprobed values for model type, base, scheduler prediction type, and so forth. See Model configuration and probing for details.</p> <p><code>access_token</code> is an optional access token for accessing resources that need authentication.</p> <p>The method will return a <code>ModelInstallJob</code>. This object is discussed at length in the following section.</p>"},{"location":"contributing/MODEL_MANAGER/#install_job-installerimport_model","title":"install_job = installer.import_model()","text":"<p>The <code>import_model()</code> method is the core of the installer. The following illustrates basic usage:</p> <pre><code>from invokeai.app.services.model_install import (\n LocalModelSource,\n HFModelSource,\n URLModelSource,\n)\n\nsource1 = LocalModelSource(path='/opt/models/sushi.safetensors')   # a local safetensors file\nsource2 = LocalModelSource(path='/opt/models/sushi_diffusers')     # a local diffusers folder\n\nsource3 = HFModelSource(repo_id='runwayml/stable-diffusion-v1-5')  # a repo_id\nsource4 = HFModelSource(repo_id='runwayml/stable-diffusion-v1-5', subfolder='vae')  # a subfolder within a repo_id\nsource5 = HFModelSource(repo_id='runwayml/stable-diffusion-v1-5', variant='fp16')   # a named variant of a HF model\nsource6 = HFModelSource(repo_id='runwayml/stable-diffusion-v1-5', subfolder='OrangeMix/OrangeMix1.ckpt')   # path to an individual model file\n\nsource7 = URLModelSource(url='https://civitai.com/api/download/models/63006')       # model located at a URL\nsource8 = URLModelSource(url='https://civitai.com/api/download/models/63006', access_token='letmein') # with an access token\n\nfor source in [source1, source2, source3, source4, source5, source6, source7]:\n   install_job = installer.install_model(source)\n\nsource2job = installer.wait_for_installs(timeout=120)\nfor source in sources:\n    job = source2job[source]\n if job.complete:\n  model_config = job.config_out\n  model_key = model_config.key\n  print(f\"{source} installed as {model_key}\")\n elif job.errored:\n     print(f\"{source}: {job.error_type}.\\nStack trace:\\n{job.error}\")\n</code></pre> <p>As shown here, the <code>import_model()</code> method accepts a variety of sources, including local safetensors files, local diffusers folders, HuggingFace repo_ids with and without a subfolder designation, Civitai model URLs and arbitrary URLs that point to checkpoint files (but not to folders).</p> <p>Each call to <code>import_model()</code> return a <code>ModelInstallJob</code> job, an object which tracks the progress of the install.</p> <p>If a remote model is requested, the model's files are downloaded in parallel across a multiple set of threads using the download queue. During the download process, the <code>ModelInstallJob</code> is updated to provide status and progress information. After the files (if any) are downloaded, the remainder of the installation runs in a single serialized background thread. These are the model probing, file copying, and config record database update steps.</p> <p>Multiple install jobs can be queued up. You may block until all install jobs are completed (or errored) by calling the <code>wait_for_installs()</code> method as shown in the code example. <code>wait_for_installs()</code> will return a <code>dict</code> that maps the requested source to its job. This object can be interrogated to determine its status. If the job errored out, then the error type and details can be recovered from <code>job.error_type</code> and <code>job.error</code>.</p> <p>The full list of arguments to <code>import_model()</code> is as follows:</p> Argument Type Default Description <code>source</code> ModelSource None The source of the model, Path, URL or repo_id <code>config</code> Dict[str, Any] None Override all or a portion of model's probed attributes <p>The next few sections describe the various types of ModelSource that can be passed to <code>import_model()</code>.</p> <p><code>config</code> can be used to override all or a portion of the configuration attributes returned by the model prober. See the section below for details.</p>"},{"location":"contributing/MODEL_MANAGER/#localmodelsource","title":"LocalModelSource","text":"<p>This is used for a model that is located on a locally-accessible Posix filesystem, such as a local disk or networked fileshare.</p> Argument Type Default Description <code>path</code> str Path None <code>inplace</code> bool False If set, the model file(s) will be left in their location; otherwise they will be copied into the InvokeAI root's <code>models</code> directory"},{"location":"contributing/MODEL_MANAGER/#urlmodelsource","title":"URLModelSource","text":"<p>This is used for a single-file model that is accessible via a URL. The fields are:</p> Argument Type Default Description <code>url</code> AnyHttpUrl None The URL for the model file. <code>access_token</code> str None An access token needed to gain access to this file. <p>The <code>AnyHttpUrl</code> class can be imported from <code>pydantic.networks</code>.</p> <p>Ordinarily, no metadata is retrieved from these sources. However, there is special-case code in the installer that looks for HuggingFace and fetches the corresponding model metadata from the corresponding repo.</p>"},{"location":"contributing/MODEL_MANAGER/#hfmodelsource","title":"HFModelSource","text":"<p>HuggingFace has the most complicated <code>ModelSource</code> structure:</p> Argument Type Default Description <code>repo_id</code> str None The ID of the desired model. <code>variant</code> ModelRepoVariant ModelRepoVariant('fp16') The desired variant. <code>subfolder</code> Path None Look for the model in a subfolder of the repo. <code>access_token</code> str None An access token needed to gain access to a subscriber's-only model. <p>The <code>repo_id</code> is the repository ID, such as <code>stabilityai/sdxl-turbo</code>.</p> <p>The <code>variant</code> is one of the various diffusers formats that HuggingFace supports and is used to pick out from the hodgepodge of files that in a typical HuggingFace repository the particular components needed for a complete diffusers model. <code>ModelRepoVariant</code> is an enum that can be imported from <code>invokeai.backend.model_manager</code> and has the following values:</p> Name String Value ModelRepoVariant.DEFAULT \"default\" ModelRepoVariant.FP16 \"fp16\" ModelRepoVariant.FP32 \"fp32\" ModelRepoVariant.ONNX \"onnx\" ModelRepoVariant.OPENVINO \"openvino\" ModelRepoVariant.FLAX \"flax\" <p>You can also pass the string forms to <code>variant</code> directly. Note that InvokeAI may not be able to load and run all variants. At the current time, specifying <code>ModelRepoVariant.DEFAULT</code> will retrieve model files that are unqualified, e.g. <code>pytorch_model.safetensors</code> rather than <code>pytorch_model.fp16.safetensors</code>. These are usually the 32-bit safetensors forms of the model.</p> <p>If <code>subfolder</code> is specified, then the requested model resides in a subfolder of the main model repository. This is typically used to fetch and install VAEs.</p> <p>Some models require you to be registered with HuggingFace and logged in. To download these files, you must provide an <code>access_token</code>. Internally, if no access token is provided, then <code>HfFolder.get_token()</code> will be called to fill it in with the cached one.</p>"},{"location":"contributing/MODEL_MANAGER/#monitoring-the-install-job-process","title":"Monitoring the install job process","text":"<p>When you create an install job with <code>import_model()</code>, it launches the download and installation process in the background and returns a <code>ModelInstallJob</code> object for monitoring the process.</p> <p>The <code>ModelInstallJob</code> class has the following structure:</p> Attribute Type Description <code>id</code> <code>int</code> Integer ID for this job <code>status</code> <code>InstallStatus</code> An enum of [<code>waiting</code>, <code>downloading</code>, <code>running</code>, <code>completed</code>, <code>error</code> and <code>cancelled</code>] <code>config_in</code> <code>dict</code> Overriding configuration values provided by the caller <code>config_out</code> <code>AnyModelConfig</code> After successful completion, contains the configuration record written to the database <code>inplace</code> <code>boolean</code> True if the caller asked to install the model in place using its local path <code>source</code> <code>ModelSource</code> The local path, remote URL or repo_id of the model to be installed <code>local_path</code> <code>Path</code> If a remote model, holds the path of the model after it is downloaded; if a local model, same as <code>source</code> <code>error_type</code> <code>str</code> Name of the exception that led to an error status <code>error</code> <code>str</code> Traceback of the error <p>If the <code>event_bus</code> argument was provided, events will also be broadcast to the InvokeAI event bus. The events will appear on the bus as an event of type <code>EventServiceBase.model_event</code>, a timestamp and the following event names:</p>"},{"location":"contributing/MODEL_MANAGER/#model_install_downloading","title":"<code>model_install_downloading</code>","text":"<p>For remote models only, <code>model_install_downloading</code> events will be issued at regular intervals as the download progresses. The event's payload contains the following keys:</p> Key Type Description <code>source</code> str String representation of the requested source <code>local_path</code> str String representation of the path to the downloading model (usually a temporary directory) <code>bytes</code> int How many bytes downloaded so far <code>total_bytes</code> int Total size of all the files that make up the model <code>parts</code> List[Dict] Information on the progress of the individual files that make up the model <p>The parts is a list of dictionaries that give information on each of the components pieces of the download. The dictionary's keys are <code>source</code>, <code>local_path</code>, <code>bytes</code> and <code>total_bytes</code>, and correspond to the like-named keys in the main event.</p> <p>Note that downloading events will not be issued for local models, and that downloading events occur before the running event.</p>"},{"location":"contributing/MODEL_MANAGER/#model_install_running","title":"<code>model_install_running</code>","text":"<p><code>model_install_running</code> is issued when all the required downloads have completed (if applicable) and the model probing, copying and registration process has now started.</p> <p>The payload will contain the key <code>source</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#model_install_completed","title":"<code>model_install_completed</code>","text":"<p><code>model_install_completed</code> is issued once at the end of a successful installation. The payload will contain the keys <code>source</code>, <code>total_bytes</code> and <code>key</code>, where <code>key</code> is the ID under which the model has been registered.</p>"},{"location":"contributing/MODEL_MANAGER/#model_install_error","title":"<code>model_install_error</code>","text":"<p><code>model_install_error</code> is emitted if the installation process fails for some reason. The payload will contain the keys <code>source</code>, <code>error_type</code> and <code>error</code>. <code>error_type</code> is a short message indicating the nature of the error, and <code>error</code> is the long traceback to help debug the problem.</p>"},{"location":"contributing/MODEL_MANAGER/#model_install_cancelled","title":"<code>model_install_cancelled</code>","text":"<p><code>model_install_cancelled</code> is issued if the model installation is cancelled, or if one or more of its files' downloads are cancelled. The payload will contain <code>source</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#following-the-model-status","title":"Following the model status","text":"<p>You may poll the <code>ModelInstallJob</code> object returned by <code>import_model()</code> to ascertain the state of the install. The job status can be read from the job's <code>status</code> attribute, an <code>InstallStatus</code> enum which has the enumerated values <code>WAITING</code>, <code>DOWNLOADING</code>, <code>RUNNING</code>, <code>COMPLETED</code>, <code>ERROR</code> and <code>CANCELLED</code>.</p> <p>For convenience, install jobs also provided the following boolean properties: <code>waiting</code>, <code>downloading</code>, <code>running</code>, <code>complete</code>, <code>errored</code> and <code>cancelled</code>, as well as <code>in_terminal_state</code>. The last will return True if the job is in the complete, errored or cancelled states.</p>"},{"location":"contributing/MODEL_MANAGER/#model-configuration-and-probing","title":"Model configuration and probing","text":"<p>The install service uses the <code>invokeai.backend.model_manager.probe</code> module during import to determine the model's type, base type, and other configuration parameters. Among other things, it assigns a default name and description for the model based on probed fields.</p> <p>When downloading remote models is implemented, additional configuration information, such as list of trigger terms, will be retrieved from the HuggingFace and Civitai model repositories.</p> <p>The probed values can be overridden by providing a dictionary in the optional <code>config</code> argument passed to <code>import_model()</code>. You may provide overriding values for any of the model's configuration attributes. Here is an example of setting the <code>SchedulerPredictionType</code> and <code>name</code> for an sd-2 model:</p> <pre><code>install_job = installer.import_model(\n               source=HFModelSource(repo_id='stabilityai/stable-diffusion-2-1',variant='fp32'),\n      config=dict(\n            prediction_type=SchedulerPredictionType('v_prediction')\n      name='stable diffusion 2 base model',\n            )\n       )\n</code></pre>"},{"location":"contributing/MODEL_MANAGER/#other-installer-methods","title":"Other installer methods","text":"<p>This section describes additional methods provided by the installer class.</p>"},{"location":"contributing/MODEL_MANAGER/#jobs-installerwait_for_installstimeout","title":"jobs = installer.wait_for_installs([timeout])","text":"<p>Block until all pending installs are completed or errored and then returns a list of completed jobs. The optional <code>timeout</code> argument will return from the call if jobs aren't completed in the specified time. An argument of 0 (the default) will block indefinitely.</p>"},{"location":"contributing/MODEL_MANAGER/#jobs-installerwait_for_jobjob-timeout","title":"jobs = installer.wait_for_job(job, [timeout])","text":"<p>Like <code>wait_for_installs()</code>, but block until a specific job has completed or errored, and then return the job.  The optional <code>timeout</code> argument will return from the call if the job doesn't complete in the specified time. An argument of 0 (the default) will block indefinitely.</p>"},{"location":"contributing/MODEL_MANAGER/#jobs-installerlist_jobs","title":"jobs = installer.list_jobs()","text":"<p>Return a list of all active and complete <code>ModelInstallJobs</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#jobs-installerget_job_by_sourcesource","title":"jobs = installer.get_job_by_source(source)","text":"<p>Return a list of <code>ModelInstallJob</code> corresponding to the indicated model source.</p>"},{"location":"contributing/MODEL_MANAGER/#jobs-installerget_job_by_idid","title":"jobs = installer.get_job_by_id(id)","text":"<p>Return a list of <code>ModelInstallJob</code> corresponding to the indicated model id.</p>"},{"location":"contributing/MODEL_MANAGER/#jobs-installercancel_jobjob","title":"jobs = installer.cancel_job(job)","text":"<p>Cancel the indicated job.</p>"},{"location":"contributing/MODEL_MANAGER/#installerprune_jobs","title":"installer.prune_jobs","text":"<p>Remove jobs that are in a terminal state (i.e. complete, errored or cancelled) from the job list returned by <code>list_jobs()</code> and <code>get_job()</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#installerapp_config-installerrecord_store-installerevent_bus","title":"installer.app_config, installer.record_store, installer.event_bus","text":"<p>Properties that provide access to the installer's <code>InvokeAIAppConfig</code>, <code>ModelRecordServiceBase</code> and <code>EventServiceBase</code> objects.</p>"},{"location":"contributing/MODEL_MANAGER/#key-installerregister_pathmodel_path-config-key-installerinstall_pathmodel_path-config","title":"key = installer.register_path(model_path, config), key = installer.install_path(model_path, config)","text":"<p>These methods bypass the download queue and directly register or install the model at the indicated path, returning the unique ID for the installed model.</p> <p>Both methods accept a Path object corresponding to a checkpoint or diffusers folder, and an optional dict of config attributes to use to override the values derived from model probing.</p> <p>The difference between <code>register_path()</code> and <code>install_path()</code> is that the former creates a model configuration record without changing the location of the model in the filesystem. The latter makes a copy of the model inside the InvokeAI models directory before registering it.</p>"},{"location":"contributing/MODEL_MANAGER/#installerunregisterkey","title":"installer.unregister(key)","text":"<p>This will remove the model config record for the model at key, and is equivalent to <code>installer.record_store.del_model(key)</code></p>"},{"location":"contributing/MODEL_MANAGER/#installerdeletekey","title":"installer.delete(key)","text":"<p>This is similar to <code>unregister()</code> but has the additional effect of conditionally deleting the underlying model file(s) if they reside within the InvokeAI models directory</p>"},{"location":"contributing/MODEL_MANAGER/#installerunconditionally_deletekey","title":"installer.unconditionally_delete(key)","text":"<p>This method is similar to <code>unregister()</code>, but also unconditionally deletes the corresponding model weights file(s), regardless of whether they are inside or outside the InvokeAI models hierarchy.</p>"},{"location":"contributing/MODEL_MANAGER/#path-installerdownload_and_cacheremote_source-access_token-timeout","title":"path = installer.download_and_cache(remote_source, [access_token], [timeout])","text":"<p>This utility routine will download the model file located at source, cache it, and return the path to the cached file. It does not attempt to determine the model type, probe its configuration values, or register it with the models database.</p> <p>You may provide an access token if the remote source requires authorization. The call will block indefinitely until the file is completely downloaded, cancelled or raises an error of some sort. If you provide a timeout (in seconds), the call will raise a <code>TimeoutError</code> exception if the download hasn't completed in the specified period.</p> <p>You may use this mechanism to request any type of file, not just a model. The file will be stored in a subdirectory of <code>INVOKEAI_ROOT/models/.cache</code>. If the requested file is found in the cache, its path will be returned without redownloading it.</p> <p>Be aware that the models cache is cleared of infrequently-used files and directories at regular intervals when the size of the cache exceeds the value specified in Invoke's <code>convert_cache</code> configuration variable.</p>"},{"location":"contributing/MODEL_MANAGER/#installerstartinvoker","title":"installer.start(invoker)","text":"<p>The <code>start</code> method is called by the API initialization routines when the API starts up. Its effect is to call <code>sync_to_config()</code> to synchronize the model record store database with what's currently on disk.</p>"},{"location":"contributing/MODEL_MANAGER/#get-on-line-the-download-queue","title":"Get on line: The Download Queue","text":"<p>InvokeAI can download arbitrary files using a multithreaded background download queue. Internally, the download queue is used for installing models located at remote locations. The queue is implemented by the <code>DownloadQueueService</code> defined in <code>invokeai.app.services.download_manager</code>. However, most of the implementation is spread out among several files in <code>invokeai/backend/model_manager/download/*</code></p> <p>A default download queue is located in <code>ApiDependencies.invoker.services.download_queue</code>. However, you can create additional instances if you need to isolate your queue from the main one.</p>"},{"location":"contributing/MODEL_MANAGER/#a-job-for-every-task","title":"A job for every task","text":"<p>The queue operates on a series of download job objects. These objects specify the source and destination of the download, and keep track of the progress of the download. Jobs come in a variety of shapes and colors as they are progressively specialized for particular download task.</p> <p>The basic job is the <code>DownloadJobBase</code>, a pydantic object with the following fields:</p> Field Type Default Description <code>id</code> int Job ID, an integer &gt;= 0 <code>priority</code> int 10 Job priority. Lower priorities run before higher priorities <code>source</code> str Where to download from (specialized types used in subclasses) <code>destination</code> Path Where to download to <code>status</code> DownloadJobStatus Idle Job's status (see below) <code>event_handlers</code> List[DownloadEventHandler] Event handlers (see below) <code>job_started</code> float Timestamp for when the job started running <code>job_ended</code> float Timestamp for when the job completed or errored out <code>job_sequence</code> int A counter that is incremented each time a model is dequeued <code>error</code> Exception A copy of the Exception that caused an error during download <p>When you create a job, you can assign it a <code>priority</code>. If multiple jobs are queued, the job with the lowest priority runs first. (Don't blame me! The Unix developers came up with this convention.)</p> <p>Every job has a <code>source</code> and a <code>destination</code>. <code>source</code> is a string in the base class, but subclassses redefine it more specifically.</p> <p>The <code>destination</code> must be the Path to a file or directory on the local filesystem. If the Path points to a new or existing file, then the source will be stored under that filename. If the Path ponts to an existing directory, then the downloaded file will be stored inside the directory, usually using the name assigned to it at the remote site in the <code>content-disposition</code> http field.</p> <p>When the job is submitted, it is assigned a numeric <code>id</code>. The id can then be used to control the job, such as starting, stopping and cancelling its download.</p> <p>The <code>status</code> field is updated by the queue to indicate where the job is in its lifecycle. Values are defined in the string enum <code>DownloadJobStatus</code>, a symbol available from <code>invokeai.app.services.download_manager</code>. Possible values are:</p> Value String Value Description <code>IDLE</code> idle Job created, but not submitted to the queue <code>ENQUEUED</code> enqueued Job is patiently waiting on the queue <code>RUNNING</code> running Job is running! <code>PAUSED</code> paused Job was paused and can be restarted <code>COMPLETED</code> completed Job has finished its work without an error <code>ERROR</code> error Job encountered an error and will not run again <code>CANCELLED</code> cancelled Job was cancelled and will not run (again) <p><code>job_started</code>, <code>job_ended</code> and <code>job_sequence</code> indicate when the job was started (using a python timestamp), when it completed, and the order in which it was taken off the queue. These are mostly used for debugging and performance testing.</p> <p>In case of an error, the Exception that caused the error will be placed in the <code>error</code> field, and the job's status will be set to <code>DownloadJobStatus.ERROR</code>.</p> <p>After an error occurs, any partially downloaded files will be deleted from disk, unless <code>preserve_partial_downloads</code> was set to True at job creation time (or set to True any time before the error occurred). Note that since all InvokeAI model install operations involve downloading files to a temporary directory that has a limited lifetime, this flag is not used by the model installer.</p> <p>There are a series of subclasses of <code>DownloadJobBase</code> that provide support for specific types of downloads. These are:</p>"},{"location":"contributing/MODEL_MANAGER/#downloadjobpath","title":"DownloadJobPath","text":"<p>This subclass redefines <code>source</code> to be a filesystem Path. It is used to move a file or directory from the <code>source</code> to the <code>destination</code> paths in the background using a uniform event-based infrastructure.</p>"},{"location":"contributing/MODEL_MANAGER/#downloadjobremotesource","title":"DownloadJobRemoteSource","text":"<p>This subclass adds the following fields to the job:</p> Field Type Default Description <code>bytes</code> int 0 bytes downloaded so far <code>total_bytes</code> int 0 total size to download <code>access_token</code> Any None an authorization token to present to the remote source <p>The job will start out with 0/0 in its bytes/total_bytes fields. Once it starts running, <code>total_bytes</code> will be populated from information provided in the HTTP download header (if available), and the number of bytes downloaded so far will be progressively incremented.</p>"},{"location":"contributing/MODEL_MANAGER/#downloadjoburl","title":"DownloadJobURL","text":"<p>This is a subclass of <code>DownloadJobBase</code>. It redefines <code>source</code> to be a Pydantic <code>AnyHttpUrl</code> object, which enforces URL validation checking on the field.</p> <p>Note that the installer service defines an additional subclass of <code>DownloadJobRemoteSource</code> that accepts HuggingFace repo_ids in addition to URLs. This is discussed later in this document.</p>"},{"location":"contributing/MODEL_MANAGER/#event-handlers","title":"Event handlers","text":"<p>While a job is being downloaded, the queue will emit events at periodic intervals. A typical series of events during a successful download session will look like this:</p> <ul> <li>enqueued</li> <li>running</li> <li>running</li> <li>running</li> <li>completed</li> </ul> <p>There will be a single enqueued event, followed by one or more running events, and finally one <code>completed</code>, <code>error</code> or <code>cancelled</code> events.</p> <p>It is possible for a caller to pause download temporarily, in which case the events may look something like this:</p> <ul> <li>enqueued</li> <li>running</li> <li>running</li> <li>paused</li> <li>running</li> <li>completed</li> </ul> <p>The download queue logs when downloads start and end (unless <code>quiet</code> is set to True at initialization time) but doesn't log any progress events. You will probably want to be alerted to events during the download job and provide more user feedback. In order to intercept and respond to events you may install a series of one or more event handlers in the job. Whenever the job's status changes, the chain of event handlers is traversed and executed in the same thread that the download job is running in.</p> <p>Event handlers have the signature <code>Callable[[\"DownloadJobBase\"], None]</code>, i.e.</p> <pre><code>def handler(job: DownloadJobBase):\n   pass\n</code></pre> <p>A typical handler will examine <code>job.status</code> and decide if there's something to be done. This can include cancelling or erroring the job, but more typically is used to report on the job status to the user interface or to perform certain actions on successful completion of the job.</p> <p>Event handlers can be attached to a job at creation time. In addition, you can create a series of default handlers that are attached to the queue object itself. These handlers will be executed for each job after the job's own handlers (if any) have run.</p> <p>During a download, running events are issued every time roughly 1% of the file is transferred. This is to provide just enough granularity to update a tqdm progress bar smoothly.</p> <p>Handlers can be added to a job after the fact using the job's <code>add_event_handler</code> method:</p> <pre><code>job.add_event_handler(my_handler)\n</code></pre> <p>All handlers can be cleared using the job's <code>clear_event_handlers()</code> method. Note that it might be a good idea to pause the job before altering its handlers.</p>"},{"location":"contributing/MODEL_MANAGER/#creating-a-download-queue-object","title":"Creating a download queue object","text":"<p>The <code>DownloadQueueService</code> constructor takes the following arguments:</p> Argument Type Default Description <code>event_handlers</code> List[DownloadEventHandler] [] Event handlers <code>max_parallel_dl</code> int 5 Maximum number of simultaneous downloads allowed <code>requests_session</code> requests.sessions.Session None An alternative requests Session object to use for the download <code>quiet</code> bool False Do work quietly without issuing log messages <p>A typical initialization sequence will look like:</p> <pre><code>from invokeai.app.services.download_manager import DownloadQueueService\n\ndef log_download_event(job: DownloadJobBase):\n logger.info(f'job={job.id}: status={job.status}')\n\nqueue = DownloadQueueService(\n       event_handlers=[log_download_event]\n        )\n</code></pre> <p>Event handlers can be provided to the queue at initialization time as shown in the example. These will be automatically appended to the handler list for any job that is submitted to this queue.</p> <p><code>max_parallel_dl</code> sets the number of simultaneous active downloads that are allowed. The default of five has not been benchmarked in any way, but seems to give acceptable performance.</p> <p><code>requests_session</code> can be used to provide a <code>requests</code> module Session object that will be used to stream remote URLs to disk. This facility was added for use in the module's unit tests to simulate a remote web server, but may be useful in other contexts.</p> <p><code>quiet</code> will prevent the queue from issuing any log messages at the INFO or higher levels.</p>"},{"location":"contributing/MODEL_MANAGER/#submitting-a-download-job","title":"Submitting a download job","text":"<p>You can submit a download job to the queue either by creating the job manually and passing it to the queue's <code>submit_download_job()</code> method, or using the <code>create_download_job()</code> method, which will do the same thing on your behalf.</p> <p>To use the former method, follow this example:</p> <pre><code>job = DownloadJobRemoteSource(\n         source='http://www.civitai.com/models/13456',\n   destination='/tmp/models/',\n   event_handlers=[my_handler1, my_handler2], # if desired\n   )\nqueue.submit_download_job(job, start=True)\n</code></pre> <p><code>submit_download_job()</code> takes just two arguments: the job to submit, and a flag indicating whether to immediately start the job (defaulting to True). If you choose not to start the job immediately, you can start it later by calling the queue's <code>start_job()</code> or <code>start_all_jobs()</code> methods, which are described later.</p> <p>To have the queue create the job for you, follow this example instead:</p> <pre><code>job = queue.create_download_job(\n         source='http://www.civitai.com/models/13456',\n   destdir='/tmp/models/',\n   filename='my_model.safetensors',\n   event_handlers=[my_handler1, my_handler2], # if desired\n   start=True,\n )\n</code></pre> <p>The <code>filename</code> argument forces the downloader to use the specified name for the file rather than the name provided by the remote source, and is equivalent to manually specifying a destination of `/tmp/models/my_model.safetensors' in the submitted job.</p> <p>Here is the full list of arguments that can be provided to <code>create_download_job()</code>:</p> Argument Type Default Description <code>source</code> Union[str, Path, AnyHttpUrl] Download remote or local source <code>destdir</code> Path Destination directory for downloaded file <code>filename</code> Path None Filename for downloaded file <code>start</code> bool True Enqueue the job immediately <code>priority</code> int 10 Starting priority for this job <code>access_token</code> str None Authorization token for this resource <code>event_handlers</code> List[DownloadEventHandler] [] Event handlers for this job <p>Internally, <code>create_download_job()</code> has a little bit of internal logic that looks at the type of the source and selects the right subclass of <code>DownloadJobBase</code> to create and enqueue.</p> <p>TODO: move this logic into its own method for overriding in subclasses.</p>"},{"location":"contributing/MODEL_MANAGER/#job-control","title":"Job control","text":"<p>Prior to completion, jobs can be controlled with a series of queue method calls. Do not attempt to modify jobs by directly writing to their fields, as this is likely to lead to unexpected results.</p> <p>Any method that accepts a job argument may raise an <code>UnknownJobIDException</code> if the job has not yet been submitted to the queue or was not created by this queue.</p>"},{"location":"contributing/MODEL_MANAGER/#queuejoin","title":"queue.join()","text":"<p>This method will block until all the active jobs in the queue have reached a terminal state (completed, errored or cancelled).</p>"},{"location":"contributing/MODEL_MANAGER/#queuewait_for_jobjob-timeout","title":"queue.wait_for_job(job, [timeout])","text":"<p>This method will block until the indicated job has reached a terminal state (completed, errored or cancelled). If the optional timeout is provided, the call will block for at most timeout seconds, and raise a TimeoutError otherwise.</p>"},{"location":"contributing/MODEL_MANAGER/#jobs-queuelist_jobs","title":"jobs = queue.list_jobs()","text":"<p>This will return a list of all jobs, including ones that have not yet been enqueued and those that have completed or errored out.</p>"},{"location":"contributing/MODEL_MANAGER/#job-queueid_to_jobint","title":"job = queue.id_to_job(int)","text":"<p>This method allows you to recover a submitted job using its ID.</p>"},{"location":"contributing/MODEL_MANAGER/#queueprune_jobs","title":"queue.prune_jobs()","text":"<p>Remove completed and errored jobs from the job list.</p>"},{"location":"contributing/MODEL_MANAGER/#queuestart_jobjob","title":"queue.start_job(job)","text":"<p>If the job was submitted with <code>start=False</code>, then it can be started using this method.</p>"},{"location":"contributing/MODEL_MANAGER/#queuepause_jobjob","title":"queue.pause_job(job)","text":"<p>This will temporarily pause the job, if possible. It can later be restarted and pick up where it left off using <code>queue.start_job()</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#queuecancel_jobjob","title":"queue.cancel_job(job)","text":"<p>This will cancel the job if possible and clean up temporary files and other resources that it might have been using.</p>"},{"location":"contributing/MODEL_MANAGER/#queuestart_all_jobs-queuepause_all_jobs-queuecancel_all_jobs","title":"queue.start_all_jobs(), queue.pause_all_jobs(), queue.cancel_all_jobs()","text":"<p>This will start/pause/cancel all jobs that have been submitted to the queue and have not yet reached a terminal state.</p>"},{"location":"contributing/MODEL_MANAGER/#this-meta-be-good-model-metadata-storage","title":"This Meta be Good: Model Metadata Storage","text":"<p>The modules found under <code>invokeai.backend.model_manager.metadata</code> provide a straightforward API for fetching model metadatda from online repositories. Currently only HuggingFace is supported. However, the modules are easily extended for additional repos, provided that they have defined APIs for metadata access.</p> <p>Metadata comprises any descriptive information that is not essential for getting the model to run. For example \"author\" is metadata, while \"type\", \"base\" and \"format\" are not. The latter fields are part of the model's config, as defined in <code>invokeai.backend.model_manager.config</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#example-usage","title":"Example Usage","text":"<pre><code>from invokeai.backend.model_manager.metadata import (\n   AnyModelRepoMetadata,\n)\n# to access the initialized sql database\nfrom invokeai.app.api.dependencies import ApiDependencies\n\nhf = HuggingFaceMetadataFetch()\n\n# fetch the metadata\nmodel_metadata = hf.from_id(\"&lt;repo_id&gt;\")\n\nassert isinstance(model_metadata, HuggingFaceMetadata)\n</code></pre>"},{"location":"contributing/MODEL_MANAGER/#structure-of-the-metadata-objects","title":"Structure of the Metadata objects","text":"<p>There is a short class hierarchy of Metadata objects, all of which descend from the Pydantic <code>BaseModel</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#modelmetadatabase","title":"<code>ModelMetadataBase</code>","text":"<p>This is the common base class for metadata:</p> Field Name Type Description <code>name</code> str Repository's name for the model <code>author</code> str Model's author <code>tags</code> Set[str] Model tags <p>Note that the model config record also has a <code>name</code> field. It is intended that the config record version be locally customizable, while the metadata version is read-only. However, enforcing this is expected to be part of the business logic.</p> <p>Descendents of the base add additional fields.</p>"},{"location":"contributing/MODEL_MANAGER/#huggingfacemetadata","title":"<code>HuggingFaceMetadata</code>","text":"<p>This descends from <code>ModelMetadataBase</code> and adds the following fields:</p> Field Name Type Description <code>type</code> Literal[\"huggingface\"] Used for the discriminated union of metadata classes <code>id</code> str HuggingFace repo_id <code>tag_dict</code> Dict[str, Any] A dictionary of tag/value pairs provided in addition to <code>tags</code> <code>last_modified</code> datetime Date of last commit of this model to the repo <code>files</code> List[Path] List of the files in the model repo"},{"location":"contributing/MODEL_MANAGER/#anymodelrepometadata","title":"<code>AnyModelRepoMetadata</code>","text":"<p>This is a discriminated Union of <code>HuggingFaceMetadata</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#fetching-metadata-from-online-repos","title":"Fetching Metadata from Online Repos","text":"<p>The <code>HuggingFaceMetadataFetch</code> class will retrieve metadata from its corresponding repository and return <code>AnyModelRepoMetadata</code> objects. Their base class <code>ModelMetadataFetchBase</code> is an abstract class that defines two methods: <code>from_url()</code> and <code>from_id()</code>. The former accepts the type of model URLs that the user will try to cut and paste into the model import form. The latter accepts a string ID in the format recognized by the repository of choice. Both methods return an <code>AnyModelRepoMetadata</code>.</p> <p>The base class also has a class method <code>from_json()</code> which will take the JSON representation of a <code>ModelMetadata</code> object, validate it, and return the corresponding <code>AnyModelRepoMetadata</code> object.</p> <p>When initializing one of the metadata fetching classes, you may provide a <code>requests.Session</code> argument. This allows you to customize the low-level HTTP fetch requests and is used, for instance, in the testing suite to avoid hitting the internet.</p> <p>The HuggingFace fetcher subclass add additional repo-specific fetching methods:</p>"},{"location":"contributing/MODEL_MANAGER/#huggingfacemetadatafetch","title":"HuggingFaceMetadataFetch","text":"<p>This overrides its base class <code>from_json()</code> method to return a <code>HuggingFaceMetadata</code> object directly.</p>"},{"location":"contributing/MODEL_MANAGER/#metadata-storage","title":"Metadata Storage","text":"<p>The <code>ModelConfigBase</code> stores this response in the <code>source_api_response</code> field as a JSON blob.</p>"},{"location":"contributing/MODEL_MANAGER/#the-lowdown-on-the-modelloadservice","title":"The Lowdown on the ModelLoadService","text":"<p>The <code>ModelLoadService</code> is responsible for loading a named model into memory so that it can be used for inference. Despite the fact that it does a lot under the covers, it is very straightforward to use.</p> <p>An application-wide model loader is created at API initialization time and stored in <code>ApiDependencies.invoker.services.model_loader</code>. However, you can create alternative instances if you wish.</p>"},{"location":"contributing/MODEL_MANAGER/#creating-a-modelloadservice-object","title":"Creating a ModelLoadService object","text":"<p>The class is defined in <code>invokeai.app.services.model_load</code>. It is initialized with an InvokeAIAppConfig object, from which it gets configuration information such as the user's desired GPU and precision, and with a previously-created <code>ModelRecordServiceBase</code> object, from which it loads the requested model's configuration information.</p> <p>Here is a typical initialization pattern:</p> <pre><code>from invokeai.app.services.config import InvokeAIAppConfig\nfrom invokeai.app.services.model_load import ModelLoadService, ModelLoaderRegistry\n\nconfig = InvokeAIAppConfig.get_config()\nram_cache = ModelCache(\n max_cache_size=config.ram_cache_size, max_vram_cache_size=config.vram_cache_size, logger=logger\n)\nconvert_cache = ModelConvertCache(\n cache_path=config.models_convert_cache_path, max_size=config.convert_cache_size\n)\nloader = ModelLoadService(\n app_config=config,\n ram_cache=ram_cache,\n convert_cache=convert_cache,\n registry=ModelLoaderRegistry\n)\n</code></pre>"},{"location":"contributing/MODEL_MANAGER/#load_modelmodel_config-submodel_type-context-loadedmodel","title":"load_model(model_config, [submodel_type], [context]) -&gt; LoadedModel","text":"<p>The <code>load_model()</code> method takes an <code>AnyModelConfig</code> returned by <code>ModelRecordService.get_model()</code> and returns the corresponding loaded model.  It loads the model into memory, gets the model ready for use, and returns a <code>LoadedModel</code> object.</p> <p>The optional second argument, <code>subtype</code> is a <code>SubModelType</code> string enum, such as \"vae\". It is mandatory when used with a main model, and is used to select which part of the main model to load.</p> <p>The optional third argument, <code>context</code> can be provided by an invocation to trigger model load event reporting. See below for details.</p> <p>The returned <code>LoadedModel</code> object contains a copy of the configuration record returned by the model record <code>get_model()</code> method, as well as the in-memory loaded model:</p> Attribute Name Type Description <code>config</code> AnyModelConfig A copy of the model's configuration record for retrieving base type, etc. <code>model</code> AnyModel The instantiated model (details below)"},{"location":"contributing/MODEL_MANAGER/#get_model_by_keykey-submodel-loadedmodel","title":"get_model_by_key(key, [submodel]) -&gt; LoadedModel","text":"<p>The <code>get_model_by_key()</code> method will retrieve the model using its unique database key. For example:</p> <p>loaded_model = loader.get_model_by_key('f13dd932c0c35c22dcb8d6cda4203764', SubModelType('vae'))</p> <p><code>get_model_by_key()</code> may raise any of the following exceptions:</p> <ul> <li><code>UnknownModelException</code>   -- key not in database</li> <li><code>ModelNotFoundException</code>  -- key in database but model not found at path</li> <li><code>NotImplementedException</code> -- the loader doesn't know how to load this type of model</li> </ul>"},{"location":"contributing/MODEL_MANAGER/#using-the-loaded-model-in-inference","title":"Using the Loaded Model in Inference","text":"<p><code>LoadedModel</code> acts as a context manager. The context loads the model into the execution device (e.g. VRAM on CUDA systems), locks the model in the execution device for the duration of the context, and returns the model. Use it like this:</p> <pre><code>loaded_model_= loader.get_model_by_key('f13dd932c0c35c22dcb8d6cda4203764', SubModelType('vae'))\nwith loaded_model as vae:\n image = vae.decode(latents)[0]\n</code></pre> <p>The object returned by the LoadedModel context manager is an <code>AnyModel</code>, which is a Union of <code>ModelMixin</code>, <code>torch.nn.Module</code>, <code>IAIOnnxRuntimeModel</code>, <code>IPAdapter</code>, <code>IPAdapterPlus</code>, and <code>EmbeddingModelRaw</code>. <code>ModelMixin</code> is the base class of all diffusers models, <code>EmbeddingModelRaw</code> is used for LoRA and TextualInversion models. The others are obvious.</p> <p>In addition, you may call <code>LoadedModel.model_on_device()</code>, a context manager that returns a tuple of the model's state dict in CPU and the model itself in VRAM. It is used to optimize the LoRA patching and unpatching process:</p> <pre><code>loaded_model_= loader.get_model_by_key('f13dd932c0c35c22dcb8d6cda4203764', SubModelType('vae'))\nwith loaded_model.model_on_device() as (state_dict, vae):\n image = vae.decode(latents)[0]\n</code></pre> <p>Since not all models have state dicts, the <code>state_dict</code> return value can be None.</p>"},{"location":"contributing/MODEL_MANAGER/#emitting-model-loading-events","title":"Emitting model loading events","text":"<p>When the <code>context</code> argument is passed to <code>load_model_*()</code>, it will retrieve the invocation event bus from the passed <code>InvocationContext</code> object to emit events on the invocation bus. The two events are \"model_load_started\" and \"model_load_completed\". Both carry the following payload:</p> <pre><code>payload=dict(\n queue_id=queue_id,\n queue_item_id=queue_item_id,\n queue_batch_id=queue_batch_id,\n graph_execution_state_id=graph_execution_state_id,\n model_key=model_key,\n submodel_type=submodel,\n hash=model_info.hash,\n location=str(model_info.location),\n precision=str(model_info.precision),\n)\n</code></pre>"},{"location":"contributing/MODEL_MANAGER/#adding-model-loaders","title":"Adding Model Loaders","text":"<p>Model loaders are small classes that inherit from the <code>ModelLoader</code> base class. They typically implement one method <code>_load_model()</code> whose signature is:</p> <pre><code>def _load_model(\n    self,\n    model_path: Path,\n    model_variant: Optional[ModelRepoVariant] = None,\n    submodel_type: Optional[SubModelType] = None,\n) -&gt; AnyModel:\n</code></pre> <p><code>_load_model()</code> will be passed the path to the model on disk, an optional repository variant (used by the diffusers loaders to select, e.g.  the <code>fp16</code> variant, and an optional submodel_type for main and onnx models.</p> <p>To install a new loader, place it in <code>invokeai/backend/model_manager/load/model_loaders</code>. Inherit from <code>ModelLoader</code> and use the <code>@ModelLoaderRegistry.register()</code> decorator to indicate what type of models the loader can handle.</p> <p>Here is a complete example from <code>generic_diffusers.py</code>, which is able to load several different diffusers types:</p> <pre><code>from pathlib import Path\nfrom typing import Optional\n\nfrom invokeai.backend.model_manager import (\n    AnyModel,\n    BaseModelType,\n    ModelFormat,\n    ModelRepoVariant,\n    ModelType,\n    SubModelType,\n)\nfrom .. import ModelLoader, ModelLoaderRegistry\n\n\n@ModelLoaderRegistry.register(base=BaseModelType.Any, type=ModelType.CLIPVision, format=ModelFormat.Diffusers)\n@ModelLoaderRegistry.register(base=BaseModelType.Any, type=ModelType.T2IAdapter, format=ModelFormat.Diffusers)\nclass GenericDiffusersLoader(ModelLoader):\n    \"\"\"Class to load simple diffusers models.\"\"\"\n\n    def _load_model(\n        self,\n        model_path: Path,\n        model_variant: Optional[ModelRepoVariant] = None,\n        submodel_type: Optional[SubModelType] = None,\n    ) -&gt; AnyModel:\n        model_class = self._get_hf_load_class(model_path)\n        if submodel_type is not None:\n            raise Exception(f\"There are no submodels in models of type {model_class}\")\n        variant = model_variant.value if model_variant else None\n        result: AnyModel = model_class.from_pretrained(model_path, torch_dtype=self._torch_dtype, variant=variant)  # type: ignore\n        return result\n</code></pre> <p>Note that a loader can register itself to handle several different model types. An exception will be raised if more than one loader tries to register the same model type.</p>"},{"location":"contributing/MODEL_MANAGER/#conversion","title":"Conversion","text":"<p>Some models require conversion to diffusers format before they can be loaded. These loaders should override two additional methods:</p> <pre><code>_needs_conversion(self, config: AnyModelConfig, model_path: Path, dest_path: Path) -&gt; bool\n_convert_model(self, config: AnyModelConfig, model_path: Path, output_path: Path) -&gt; Path:\n</code></pre> <p>The first method accepts the model configuration, the path to where the unmodified model is currently installed, and a proposed destination for the converted model. This method returns True if the model needs to be converted. It typically does this by comparing the last modification time of the original model file to the modification time of the converted model. In some cases you will also want to check the modification date of the configuration record, in the event that the user has changed something like the scheduler prediction type that will require the model to be re-converted. See <code>controlnet.py</code> for an example of this logic.</p> <p>The second method accepts the model configuration, the path to the original model on disk, and the desired output path for the converted model. It does whatever it needs to do to get the model into diffusers format, and returns the Path of the resulting model. (The path should ordinarily be the same as <code>output_path</code>.)</p>"},{"location":"contributing/MODEL_MANAGER/#the-modelmanagerservice-object","title":"The ModelManagerService object","text":"<p>For convenience, the API provides a <code>ModelManagerService</code> object which gives a single point of access to the major model manager services. This object is created at initialization time and can be found in the global <code>ApiDependencies.invoker.services.model_manager</code> object, or in <code>context.services.model_manager</code> from within an invocation.</p> <p>In the examples below, we have retrieved the manager using:</p> <pre><code>mm = ApiDependencies.invoker.services.model_manager\n</code></pre> <p>The following properties and methods will be available:</p>"},{"location":"contributing/MODEL_MANAGER/#mmstore","title":"mm.store","text":"<p>This retrieves the <code>ModelRecordService</code> associated with the manager. Example:</p> <pre><code>configs = mm.store.get_model_by_attr(name='stable-diffusion-v1-5')\n</code></pre>"},{"location":"contributing/MODEL_MANAGER/#mminstall","title":"mm.install","text":"<p>This retrieves the <code>ModelInstallService</code> associated with the manager. Example:</p> <pre><code>job = mm.install.heuristic_import(`https://civitai.com/models/58390/detail-tweaker-lora-lora`)\n</code></pre>"},{"location":"contributing/MODEL_MANAGER/#mmload","title":"mm.load","text":"<p>This retrieves the <code>ModelLoaderService</code> associated with the manager. Example:</p> <pre><code>configs = mm.store.get_model_by_attr(name='stable-diffusion-v1-5')\nassert len(configs) &gt; 0\n\nloaded_model = mm.load.load_model(configs[0])\n</code></pre> <p>The model manager also offers a few convenience shortcuts for loading models:</p>"},{"location":"contributing/MODEL_MANAGER/#mmload_model_by_configmodel_config-submodel-context-loadedmodel","title":"mm.load_model_by_config(model_config, [submodel], [context]) -&gt; LoadedModel","text":"<p>Same as <code>mm.load.load_model()</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#mmload_model_by_attrmodel_name-base_model-model_type-submodel-context-loadedmodel","title":"mm.load_model_by_attr(model_name, base_model, model_type, [submodel], [context]) -&gt; LoadedModel","text":"<p>This accepts the combination of the model's name, type and base, which it passes to the model record config store for retrieval. If a unique model config is found, this method returns a <code>LoadedModel</code>. It can raise the following exceptions:</p> <pre><code>UnknownModelException -- model with these attributes not known\nNotImplementedException -- the loader doesn't know how to load this type of model\nValueError -- more than one model matches this combination of base/type/name\n</code></pre>"},{"location":"contributing/MODEL_MANAGER/#mmload_model_by_keykey-submodel-context-loadedmodel","title":"mm.load_model_by_key(key, [submodel], [context]) -&gt; LoadedModel","text":"<p>This method takes a model key, looks it up using the <code>ModelRecordServiceBase</code> object in <code>mm.store</code>, and passes the returned model configuration to <code>load_model_by_config()</code>.  It may raise a <code>NotImplementedException</code>.</p>"},{"location":"contributing/MODEL_MANAGER/#invocation-context-model-manager-api","title":"Invocation Context Model Manager API","text":"<p>Within invocations, the following methods are available from the <code>InvocationContext</code> object:</p>"},{"location":"contributing/MODEL_MANAGER/#contextdownload_and_cache_modelsource-path","title":"context.download_and_cache_model(source) -&gt; Path","text":"<p>This method accepts a <code>source</code> of a remote model, downloads and caches it locally, and then returns a Path to the local model. The source can be a direct download URL or a HuggingFace repo_id.</p> <p>In the case of HuggingFace repo_id, the following variants are recognized:</p> <ul> <li>stabilityai/stable-diffusion-v4           -- default model</li> <li>stabilityai/stable-diffusion-v4:fp16      -- fp16 variant</li> <li>stabilityai/stable-diffusion-v4:fp16:vae  -- the fp16 vae subfolder</li> <li>stabilityai/stable-diffusion-v4:onnx:vae  -- the onnx variant vae subfolder</li> </ul> <p>You can also point at an arbitrary individual file within a repo_id directory using this syntax:</p> <ul> <li>stabilityai/stable-diffusion-v4::/checkpoints/sd4.safetensors</li> </ul>"},{"location":"contributing/MODEL_MANAGER/#contextload_local_modelmodel_path-loader-loadedmodel","title":"context.load_local_model(model_path, [loader]) -&gt; LoadedModel","text":"<p>This method loads a local model from the indicated path, returning a <code>LoadedModel</code>. The optional loader is a Callable that accepts a Path to the object, and returns a <code>AnyModel</code> object. If no loader is provided, then the method will use <code>torch.load()</code> for a .ckpt or .bin checkpoint file, <code>safetensors.torch.load_file()</code> for a safetensors checkpoint file, or <code>cls.from_pretrained()</code> for a directory that looks like a diffusers directory.</p>"},{"location":"contributing/MODEL_MANAGER/#contextload_remote_modelsource-loader-loadedmodel","title":"context.load_remote_model(source, [loader]) -&gt; LoadedModel","text":"<p>This method accepts a <code>source</code> of a remote model, downloads and caches it locally, loads it, and returns a <code>LoadedModel</code>. The source can be a direct download URL or a HuggingFace repo_id.</p> <p>In the case of HuggingFace repo_id, the following variants are recognized:</p> <ul> <li>stabilityai/stable-diffusion-v4           -- default model</li> <li>stabilityai/stable-diffusion-v4:fp16      -- fp16 variant</li> <li>stabilityai/stable-diffusion-v4:fp16:vae  -- the fp16 vae subfolder</li> <li>stabilityai/stable-diffusion-v4:onnx:vae  -- the onnx variant vae subfolder</li> </ul> <p>You can also point at an arbitrary individual file within a repo_id directory using this syntax:</p> <ul> <li>stabilityai/stable-diffusion-v4::/checkpoints/sd4.safetensors</li> </ul>"},{"location":"contributing/NEW_MODEL_INTEGRATION/","title":"InvokeAI - New Model Type Integration Checklist","text":"<p>This documentation describes all the steps required to integrate a new model type into InvokeAI. The implementations of FLUX.1, FLUX.2 Klein, SD3, SDXL, and Z-Image serve as references.</p>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Backend: Model Manager</li> <li>Backend: Model Configs</li> <li>Backend: Model Loader</li> <li>Backend: Invocations</li> <li>Backend: Sampling and Denoise</li> <li>Frontend: Graph Building</li> <li>Frontend: State Management</li> <li>Frontend: Parameter Recall</li> <li>Metadata and Generation Modes</li> <li>Starter Models</li> <li>Optional Features</li> </ol>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#1-backend-model-manager","title":"1. Backend: Model Manager","text":""},{"location":"contributing/NEW_MODEL_INTEGRATION/#11-add-basemodeltype","title":"1.1 Add BaseModelType","text":"<p>File: <code>invokeai/backend/model_manager/taxonomy.py</code></p> <pre><code>class BaseModelType(str, Enum):\n    # Existing types\n    StableDiffusion1 = \"sd-1\"\n    StableDiffusion2 = \"sd-2\"\n    StableDiffusionXL = \"sdxl\"\n    Flux = \"flux\"\n    Flux2 = \"flux2\"        # FLUX.2 Klein\n    SD3 = \"sd-3\"\n    ZImage = \"z-image\"\n    # NEW:\n    NewModel = \"newmodel\"\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#12-add-variant-type-if-needed","title":"1.2 Add Variant Type (if needed)","text":"<p>File: <code>invokeai/backend/model_manager/taxonomy.py</code></p> <pre><code># Examples of existing variants:\nclass FluxVariantType(str, Enum):\n    Schnell = \"schnell\"\n    Dev = \"dev\"\n    DevFill = \"dev_fill\"\n\nclass Flux2VariantType(str, Enum):\n    Klein4B = \"klein_4b\"    # Qwen3 4B encoder\n    Klein9B = \"klein_9b\"    # Qwen3 8B distilled\n    Klein9BBase = \"klein_9b_base\"\n\n# NEW (if needed):\nclass NewModelVariantType(str, Enum):\n    VariantA = \"variant_a\"\n    VariantB = \"variant_b\"\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#13-define-default-settings","title":"1.3 Define Default Settings","text":"<p>File: <code>invokeai/backend/model_manager/configs/main.py</code></p> <pre><code>class MainModelDefaultSettings:\n    @staticmethod\n    def from_base(base: BaseModelType, variant: AnyVariant | None = None):\n        match base:\n            case BaseModelType.Flux2:\n                if variant == Flux2VariantType.Klein9BBase:\n                    return MainModelDefaultSettings(steps=28, cfg_scale=1.0, ...)\n                return MainModelDefaultSettings(steps=4, cfg_scale=1.0, ...)\n            # NEW:\n            case BaseModelType.NewModel:\n                return MainModelDefaultSettings(steps=20, cfg_scale=7.0, ...)\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#backend-model-manager-checklist","title":"Backend Model Manager Checklist","text":"<ul> <li> Extend <code>BaseModelType</code> enum (<code>taxonomy.py</code>)</li> <li> Create variant enum if needed (<code>taxonomy.py</code>)</li> <li> Update <code>AnyVariant</code> union (<code>taxonomy.py</code>)</li> <li> Add default settings in <code>from_base()</code> (<code>configs/main.py</code>)</li> </ul>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#2-backend-model-configs","title":"2. Backend: Model Configs","text":""},{"location":"contributing/NEW_MODEL_INTEGRATION/#21-create-main-model-config","title":"2.1 Create Main Model Config","text":"<p>File: <code>invokeai/backend/model_manager/configs/main.py</code></p> <pre><code># Checkpoint Format\n@ModelConfigFactory.register\nclass Main_Checkpoint_NewModel_Config(Checkpoint_Config_Base):\n    type: Literal[ModelType.Main] = ModelType.Main\n    base: Literal[BaseModelType.NewModel] = BaseModelType.NewModel\n    format: Literal[ModelFormat.Checkpoint] = ModelFormat.Checkpoint\n    variant: NewModelVariantType = NewModelVariantType.VariantA\n\n    @classmethod\n    def from_model_on_disk(cls, mod: ModelOnDisk, override_fields: dict) -&gt; Self:\n        if not cls._validate_is_newmodel(mod):\n            raise NotAMatchError(\"Not a NewModel\")\n        variant = cls._get_variant_or_raise(mod)\n        return cls(..., variant=variant)\n\n# Diffusers Format\n@ModelConfigFactory.register\nclass Main_Diffusers_NewModel_Config(Diffusers_Config_Base):\n    type: Literal[ModelType.Main] = ModelType.Main\n    base: Literal[BaseModelType.NewModel] = BaseModelType.NewModel\n    format: Literal[ModelFormat.Diffusers] = ModelFormat.Diffusers\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#22-detection-helper-functions","title":"2.2 Detection Helper Functions","text":"<p>File: <code>invokeai/backend/model_manager/configs/main.py</code></p> <pre><code>def _is_newmodel(state_dict: dict) -&gt; bool:\n    \"\"\"Detect if state dict belongs to NewModel architecture.\"\"\"\n    # Example FLUX.2 Klein detection:\n    # - context_embedder.weight shape[1] &gt; 4096 (Qwen3 vs T5)\n    # - img_in.weight shape[1] == 128 (32 latent channels \u00d7 4)\n    required_keys = [\"transformer_blocks.0.attn.to_q.weight\", ...]\n    return all(key in state_dict for key in required_keys)\n\ndef _get_newmodel_variant(state_dict: dict) -&gt; NewModelVariantType:\n    \"\"\"Determine variant from state dict.\"\"\"\n    # Example FLUX.2: context_in_dim distinguishes Klein 4B/9B\n    context_dim = state_dict[\"context_embedder.weight\"].shape[1]\n    if context_dim == 7680:\n        return NewModelVariantType.VariantA\n    return NewModelVariantType.VariantB\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#23-vae-config-if-custom-vae","title":"2.3 VAE Config (if custom VAE)","text":"<p>File: <code>invokeai/backend/model_manager/configs/vae.py</code></p> <pre><code>@ModelConfigFactory.register\nclass VAE_Checkpoint_NewModel_Config(VAE_Checkpoint_Base):\n    type: Literal[ModelType.VAE] = ModelType.VAE\n    base: Literal[BaseModelType.NewModel] = BaseModelType.NewModel\n\n    @classmethod\n    def from_model_on_disk(cls, mod: ModelOnDisk, ...) -&gt; Self:\n        if not _is_newmodel_vae(mod.state_dict):\n            raise NotAMatchError()\n        return cls(...)\n\ndef _is_newmodel_vae(state_dict: dict) -&gt; bool:\n    # Example FLUX.2: Check for BN layers (bn.running_mean)\n    return \"encoder.bn.running_mean\" in state_dict\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#24-text-encoder-config-if-custom-encoder","title":"2.4 Text Encoder Config (if custom encoder)","text":"<p>File: <code>invokeai/backend/model_manager/configs/[encoder_type].py</code></p> <pre><code>def _has_newmodel_encoder_keys(state_dict: dict) -&gt; bool:\n    \"\"\"Check if state dict contains NewModel encoder keys.\"\"\"\n    required_keys = [\"model.layers.0.\", \"model.embed_tokens.weight\"]\n    return any(\n        key.startswith(indicator) or key == indicator\n        for key in state_dict.keys()\n        for indicator in required_keys\n        if isinstance(key, str)\n    )\n\n@ModelConfigFactory.register\nclass NewModelEncoder_Checkpoint_Config(Checkpoint_Config_Base):\n    \"\"\"Configuration for single-file NewModel Encoder models.\"\"\"\n\n    base: Literal[BaseModelType.Any] = Field(default=BaseModelType.Any)\n    type: Literal[ModelType.NewModelEncoder] = Field(default=ModelType.NewModelEncoder)\n    format: Literal[ModelFormat.Checkpoint] = Field(default=ModelFormat.Checkpoint)\n\n    @classmethod\n    def from_model_on_disk(cls, mod: ModelOnDisk, override_fields: dict) -&gt; Self:\n        raise_if_not_file(mod)\n        raise_for_override_fields(cls, override_fields)\n\n        if not _has_newmodel_encoder_keys(mod.load_state_dict()):\n            raise NotAMatchError(\"state dict does not look like a NewModel encoder\")\n\n        return cls(**override_fields)\n\n@ModelConfigFactory.register\nclass NewModelEncoder_Diffusers_Config(Config_Base):\n    \"\"\"Configuration for NewModel Encoder in diffusers directory format.\"\"\"\n\n    base: Literal[BaseModelType.Any] = Field(default=BaseModelType.Any)\n    type: Literal[ModelType.NewModelEncoder] = Field(default=ModelType.NewModelEncoder)\n    format: Literal[ModelFormat.Diffusers] = Field(default=ModelFormat.Diffusers)\n\n    @classmethod\n    def from_model_on_disk(cls, mod: ModelOnDisk, override_fields: dict) -&gt; Self:\n        raise_if_not_dir(mod)\n        raise_for_override_fields(cls, override_fields)\n\n        # Check for text_encoder config\n        config_path = mod.path / \"text_encoder\" / \"config.json\"\n        if not config_path.exists():\n            raise NotAMatchError(f\"config file not found: {config_path}\")\n\n        raise_for_class_name(config_path, {\"NewModelForCausalLM\"})\n\n        return cls(**override_fields)\n</code></pre> <p>Examples of existing implementations: - <code>t5_encoder.py</code> - T5 Encoder for FLUX.1, SD3 - <code>qwen3_encoder.py</code> - Qwen3 Encoder for FLUX.2 Klein, Z-Image - <code>clip_embed.py</code> - CLIP Encoder for SDXL, SD3</p>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#25-update-anymodelconfig-union","title":"2.5 Update AnyModelConfig Union","text":"<p>File: <code>invokeai/backend/model_manager/configs/factory.py</code></p> <pre><code>AnyModelConfig = Annotated[\n    # ... existing configs\n    Main_Checkpoint_NewModel_Config |\n    Main_Diffusers_NewModel_Config |\n    VAE_Checkpoint_NewModel_Config,\n    Discriminator(...)\n]\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#backend-model-configs-checklist","title":"Backend Model Configs Checklist","text":"<ul> <li> Create main checkpoint config (<code>configs/main.py</code>)</li> <li> Create main diffusers config (<code>configs/main.py</code>)</li> <li> Create detection helper functions (<code>_is_newmodel()</code>, <code>_get_variant()</code>)</li> <li> Create VAE config if custom VAE (<code>configs/vae.py</code>)</li> <li> Create text encoder config if custom encoder</li> <li> Update <code>AnyModelConfig</code> union (<code>configs/factory.py</code>)</li> </ul>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#3-backend-model-loader","title":"3. Backend: Model Loader","text":""},{"location":"contributing/NEW_MODEL_INTEGRATION/#31-create-model-loader","title":"3.1 Create Model Loader","text":"<p>File: <code>invokeai/backend/model_manager/load/model_loaders/[newmodel].py</code></p> <pre><code>@ModelLoaderRegistry.register(\n    base=BaseModelType.NewModel,\n    type=ModelType.Main,\n    format=ModelFormat.Checkpoint\n)\nclass NewModelLoader(ModelLoader):\n    def _load_model(self, config: AnyModelConfig, submodel_type: SubModelType | None) -&gt; AnyModel:\n        # Load and convert state dict\n        state_dict = self._load_state_dict(config.path)\n\n        # If format conversion needed (e.g., BFL \u2192 Diffusers):\n        if self._is_bfl_format(state_dict):\n            state_dict = self._convert_bfl_to_diffusers(state_dict)\n\n        # Instantiate model\n        model = NewModelTransformer(config=model_config)\n        model.load_state_dict(state_dict)\n        return model\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#32-vae-loader-if-custom-vae","title":"3.2 VAE Loader (if custom VAE)","text":"<p>File: <code>invokeai/backend/model_manager/load/model_loaders/[newmodel].py</code></p> <pre><code>@ModelLoaderRegistry.register(\n    base=BaseModelType.NewModel,\n    type=ModelType.VAE,\n    format=ModelFormat.Checkpoint\n)\nclass NewModelVAELoader(ModelLoader):\n    def _load_model(self, config, submodel_type) -&gt; AnyModel:\n        # Example FLUX.2: AutoencoderKLFlux2 with BN layers\n        from diffusers import AutoencoderKLFlux2\n        vae = AutoencoderKLFlux2.from_single_file(config.path)\n        return vae\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#33-text-encoder-loader-if-custom-encoder","title":"3.3 Text Encoder Loader (if custom encoder)","text":"<p>File: <code>invokeai/backend/model_manager/load/model_loaders/[newmodel].py</code></p> <pre><code>@ModelLoaderRegistry.register(\n    base=BaseModelType.Any,\n    type=ModelType.NewModelEncoder,\n    format=ModelFormat.Checkpoint\n)\nclass NewModelEncoderLoader(ModelLoader):\n    \"\"\"Load single-file NewModel Encoder models.\"\"\"\n\n    def _load_model(self, config: AnyModelConfig, submodel_type: SubModelType | None) -&gt; AnyModel:\n        match submodel_type:\n            case SubModelType.TextEncoder:\n                return self._load_text_encoder(config)\n            case SubModelType.Tokenizer:\n                # Load tokenizer from HuggingFace or local path\n                return AutoTokenizer.from_pretrained(\"org/newmodel-base\")\n\n        raise ValueError(f\"Unsupported submodel: {submodel_type}\")\n\n    def _load_text_encoder(self, config: AnyModelConfig) -&gt; AnyModel:\n        from safetensors.torch import load_file\n        from transformers import NewModelConfig, NewModelForCausalLM\n\n        # Load state dict and determine model configuration\n        sd = load_file(config.path)\n\n        # Detect model architecture from weights\n        layer_count = self._count_layers(sd)\n        hidden_size = sd[\"model.embed_tokens.weight\"].shape[1]\n\n        # Create model with detected configuration\n        model_config = NewModelConfig(\n            hidden_size=hidden_size,\n            num_hidden_layers=layer_count,\n            # ... other config parameters\n        )\n\n        with accelerate.init_empty_weights():\n            model = NewModelForCausalLM(model_config)\n\n        model.load_state_dict(sd, assign=True)\n        return model\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#backend-model-loader-checklist","title":"Backend Model Loader Checklist","text":"<ul> <li> Create and register main model loader</li> <li> Create VAE loader if custom VAE</li> <li> Create text encoder loader if custom encoder</li> <li> Implement state dict conversion if needed (different formats)</li> <li> Implement submodel loading (Diffusers format)</li> </ul>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#4-backend-invocations","title":"4. Backend: Invocations","text":""},{"location":"contributing/NEW_MODEL_INTEGRATION/#41-model-loader-invocation","title":"4.1 Model Loader Invocation","text":"<p>File: <code>invokeai/app/invocations/[newmodel]_model_loader.py</code></p> <pre><code>@invocation(\"newmodel_model_loader\", title=\"NewModel Loader\", ...)\nclass NewModelModelLoaderInvocation(BaseInvocation):\n    model: ModelIdentifierField = InputField(description=\"Main model\")\n    vae_model: ModelIdentifierField | None = InputField(default=None)\n    encoder_model: ModelIdentifierField | None = InputField(default=None)\n\n    def invoke(self, context: InvocationContext) -&gt; NewModelLoaderOutput:\n        # Load transformer\n        transformer = self.model.model_copy(\n            update={\"submodel_type\": SubModelType.Transformer}\n        )\n        # Load VAE (from main model or separately)\n        if self.vae_model:\n            vae = self.vae_model.model_copy(...)\n        else:\n            vae = self.model.model_copy(\n                update={\"submodel_type\": SubModelType.VAE}\n            )\n        return NewModelLoaderOutput(transformer=transformer, vae=vae, ...)\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#42-text-encoder-invocation","title":"4.2 Text Encoder Invocation","text":"<p>File: <code>invokeai/app/invocations/[newmodel]_text_encoder.py</code></p> <pre><code>@invocation(\"newmodel_text_encode\", title=\"NewModel Text Encoder\", ...)\nclass NewModelTextEncoderInvocation(BaseInvocation):\n    prompt: str = InputField()\n    encoder: EncoderField = InputField()\n\n    def invoke(self, context: InvocationContext) -&gt; ConditioningOutput:\n        # 1. Tokenize the prompt\n        with context.models.load(self.encoder.tokenizer) as tokenizer:\n            input_ids = tokenizer(\n                self.prompt,\n                return_tensors=\"pt\",\n                padding=\"max_length\",\n                max_length=256,\n                truncation=True\n            ).input_ids\n\n        # 2. Run encoder and extract hidden states\n        # Example FLUX.2 Klein/Z-Image: Extract specific layers and stack them\n        # Different models use different layer extraction strategies:\n        # - Some use the final hidden state only\n        # - Others stack multiple intermediate layers for richer representations\n        with context.models.load(self.encoder.text_encoder) as encoder:\n            outputs = encoder(input_ids, output_hidden_states=True)\n            hidden_states = outputs.hidden_states\n\n            # Stack layers 9, 18, 27 to create combined text embedding\n            # This captures features at different abstraction levels\n            # Shape: (batch, seq_len, hidden_size) -&gt; (batch, seq_len, hidden_size * 3)\n            stacked_embeddings = torch.cat([\n                hidden_states[9],\n                hidden_states[18],\n                hidden_states[27]\n            ], dim=-1)\n\n        # 3. Create conditioning data structure\n        # The stacked embeddings become the text conditioning that guides denoising\n        conditioning_data = ConditioningFieldData(\n            conditionings=[\n                BasicConditioningInfo(embeds=stacked_embeddings)\n            ]\n        )\n\n        # 4. Save conditioning to context and return reference\n        conditioning_name = context.conditioning.save(conditioning_data)\n        return ConditioningOutput(\n            conditioning=ConditioningField(conditioning_name=conditioning_name)\n        )\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#43-denoise-invocation","title":"4.3 Denoise Invocation","text":"<p>File: <code>invokeai/app/invocations/[newmodel]_denoise.py</code></p> <pre><code>@invocation(\"newmodel_denoise\", title=\"NewModel Denoise\", ...)\nclass NewModelDenoiseInvocation(BaseInvocation):\n    # Standard Fields\n    latents: LatentsField | None = InputField(default=None)\n    positive_conditioning: ConditioningField = InputField()\n    negative_conditioning: ConditioningField | None = InputField(default=None)\n\n    # Model Fields\n    transformer: TransformerField = InputField()\n\n    # Denoise Parameters\n    denoising_start: float = InputField(default=0.0, ge=0, le=1)\n    denoising_end: float = InputField(default=1.0, ge=0, le=1)\n    steps: int = InputField(default=20, ge=1)\n    cfg_scale: float = InputField(default=7.0)\n\n    # Image-to-Image / Inpainting\n    denoise_mask: DenoiseMaskField | None = InputField(default=None)\n\n    # Scheduler (if model-specific)\n    scheduler: Literal[\"euler\", \"heun\", \"lcm\"] = InputField(default=\"euler\")\n\n    def invoke(self, context: InvocationContext) -&gt; LatentsOutput:\n        # 1. Generate noise\n        noise = get_noise_newmodel(seed, height, width, ...)\n\n        # 2. Pack latents (if needed)\n        x = pack_newmodel(latents)\n\n        # 3. Compute schedule\n        timesteps = get_schedule_newmodel(num_steps, denoising_start, denoising_end)\n\n        # 4. Denoising loop\n        x = denoise(\n            model=transformer,\n            x=x,\n            timesteps=timesteps,\n            conditioning=conditioning,\n            cfg_scale=self.cfg_scale,\n            inpaint_extension=inpaint_extension,  # For inpainting\n        )\n\n        # 5. Unpack latents\n        latents = unpack_newmodel(x)\n\n        return LatentsOutput(latents=latents)\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#44-vae-encode-invocation","title":"4.4 VAE Encode Invocation","text":"<p>File: <code>invokeai/app/invocations/[newmodel]_vae_encode.py</code></p> <pre><code>@invocation(\"newmodel_vae_encode\", title=\"Image to Latents - NewModel\", ...)\nclass NewModelVaeEncodeInvocation(BaseInvocation):\n    image: ImageField = InputField()\n    vae: VAEField = InputField()\n\n    def invoke(self, context: InvocationContext) -&gt; LatentsOutput:\n        image = context.images.get_pil(self.image.image_name)\n        image_tensor = image_resized_to_grid_as_tensor(image.convert(\"RGB\"))\n\n        with context.models.load(self.vae.vae) as vae:\n            latent_dist = vae.encode(image_tensor)\n            latents = latent_dist.mode()  # Deterministic\n\n        return LatentsOutput(latents=latents)\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#45-vae-decode-invocation","title":"4.5 VAE Decode Invocation","text":"<p>File: <code>invokeai/app/invocations/[newmodel]_vae_decode.py</code></p> <pre><code>@invocation(\"newmodel_vae_decode\", title=\"Latents to Image - NewModel\", ...)\nclass NewModelVaeDecodeInvocation(BaseInvocation):\n    latents: LatentsField = InputField()\n    vae: VAEField = InputField()\n\n    def invoke(self, context: InvocationContext) -&gt; ImageOutput:\n        latents = context.tensors.load(self.latents.latents_name)\n\n        with context.models.load(self.vae.vae) as vae:\n            # Example FLUX.2: BN denormalization before decode\n            if hasattr(vae, \"bn\"):\n                latents = self._bn_denormalize(latents, vae)\n\n            image = vae.decode(latents).sample\n            image = (image / 2 + 0.5).clamp(0, 1)\n\n        return ImageOutput(image=image)\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#backend-invocations-checklist","title":"Backend Invocations Checklist","text":"<ul> <li> Model loader invocation (<code>[newmodel]_model_loader.py</code>)</li> <li> Text encoder invocation (<code>[newmodel]_text_encoder.py</code>)</li> <li> Denoise invocation (<code>[newmodel]_denoise.py</code>)</li> <li> VAE encode invocation (<code>[newmodel]_vae_encode.py</code>)</li> <li> VAE decode invocation (<code>[newmodel]_vae_decode.py</code>)</li> <li> Define output classes (e.g., <code>NewModelLoaderOutput</code>)</li> <li> Define field classes if needed (e.g., <code>NewModelEncoderField</code>)</li> </ul>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#5-backend-sampling-and-denoise","title":"5. Backend: Sampling and Denoise","text":""},{"location":"contributing/NEW_MODEL_INTEGRATION/#51-sampling-utilities","title":"5.1 Sampling Utilities","text":"<p>File: <code>invokeai/backend/[newmodel]/sampling_utils.py</code></p> <pre><code>def get_noise_newmodel(\n    num_samples: int,\n    height: int,\n    width: int,\n    seed: int,\n    device: torch.device,\n    dtype: torch.dtype,\n) -&gt; torch.Tensor:\n    \"\"\"Generate noise for NewModel.\n\n    Example FLUX.2: 32 latent channels (vs 16 for FLUX.1)\n    \"\"\"\n    latent_channels = 32  # Model-specific\n    latent_h = height // 8\n    latent_w = width // 8\n\n    generator = torch.Generator(device=device).manual_seed(seed)\n    return torch.randn(\n        (num_samples, latent_channels, latent_h, latent_w),\n        generator=generator,\n        device=device,\n        dtype=dtype,\n    )\n\ndef pack_newmodel(x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Pack latents for transformer input.\n\n    Example FLUX: 2\u00d72 patches \u2192 (B, H/2*W/2, C*4)\n    \"\"\"\n    return rearrange(x, \"b c (h ph) (w pw) -&gt; b (h w) (c ph pw)\", ph=2, pw=2)\n\ndef unpack_newmodel(x: torch.Tensor, height: int, width: int) -&gt; torch.Tensor:\n    \"\"\"Unpack transformer output to latents.\"\"\"\n    return rearrange(\n        x, \"b (h w) (c ph pw) -&gt; b c (h ph) (w pw)\",\n        h=height // 16, w=width // 16, ph=2, pw=2\n    )\n\ndef get_schedule_newmodel(\n    num_steps: int,\n    denoising_start: float = 0.0,\n    denoising_end: float = 1.0,\n) -&gt; list[float]:\n    \"\"\"Create timestep schedule.\n\n    Example FLUX.2 Klein: Linear schedule from 1.0 \u2192 0.0\n    \"\"\"\n    start_step = int(num_steps * denoising_start)\n    end_step = int(num_steps * denoising_end)\n\n    sigmas = torch.linspace(1.0, 0.0, num_steps + 1)\n    return sigmas[start_step:end_step + 1].tolist()\n\ndef generate_img_ids_newmodel(batch_size: int, height: int, width: int) -&gt; torch.Tensor:\n    \"\"\"Generate position IDs for transformer.\n\n    Example FLUX.2: 4D position IDs (T, H, W, L)\n    \"\"\"\n    # Model-specific position encoding\n    pass\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#52-denoise-function","title":"5.2 Denoise Function","text":"<p>File: <code>invokeai/backend/[newmodel]/denoise.py</code></p> <pre><code>def denoise(\n    model: nn.Module,\n    img: torch.Tensor,\n    img_ids: torch.Tensor,\n    txt: torch.Tensor,\n    txt_ids: torch.Tensor,\n    timesteps: list[float],\n    cfg_scale: list[float],\n    neg_txt: torch.Tensor | None = None,\n    neg_txt_ids: torch.Tensor | None = None,\n    scheduler: Any = None,\n    inpaint_extension: RectifiedFlowInpaintExtension | None = None,\n    step_callback: Callable | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Main denoising loop.\n\n    Example FLUX.2 Klein:\n    - No guidance_embeds (unlike FLUX.1 Dev)\n    - Supports Euler, Heun, LCM schedulers\n    - Integration with RectifiedFlowInpaintExtension\n    \"\"\"\n    total_steps = len(timesteps) - 1\n\n    for step_index in range(total_steps):\n        t_curr = timesteps[step_index]\n        t_prev = timesteps[step_index + 1]\n\n        # CFG\n        if cfg_scale[step_index] &gt; 1.0 and neg_txt is not None:\n            pred_pos = model(img, t_curr, txt, txt_ids, img_ids)\n            pred_neg = model(img, t_curr, neg_txt, neg_txt_ids, img_ids)\n            pred = pred_neg + cfg_scale[step_index] * (pred_pos - pred_neg)\n        else:\n            pred = model(img, t_curr, txt, txt_ids, img_ids)\n\n        # Scheduler step or manual Euler\n        if scheduler is not None:\n            img = scheduler.step(pred, t_curr, img).prev_sample\n        else:\n            # Manual Euler: x = x + (t_prev - t_curr) * pred\n            img = img + (t_prev - t_curr) * pred\n\n        # Inpainting merge\n        if inpaint_extension is not None:\n            img = inpaint_extension.merge_intermediate_latents_with_init_latents(img, t_prev)\n\n        # Progress callback\n        if step_callback:\n            step_callback(PipelineIntermediateState(step=step_index + 1, ...))\n\n    return img\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#53-scheduler-if-model-specific","title":"5.3 Scheduler (if model-specific)","text":"<p>File: <code>invokeai/backend/[newmodel]/schedulers.py</code> or use existing</p> <pre><code># Existing schedulers in invokeai/backend/flux/schedulers.py:\n# - FlowMatchEulerDiscreteScheduler\n# - FlowMatchHeunDiscreteScheduler\n# - FlowMatchLCMScheduler\n\nNEWMODEL_SCHEDULER_MAP = {\n    \"euler\": FlowMatchEulerDiscreteScheduler,\n    \"heun\": FlowMatchHeunDiscreteScheduler,\n    \"lcm\": FlowMatchLCMScheduler,\n}\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#backend-sampling-and-denoise-checklist","title":"Backend Sampling and Denoise Checklist","text":"<ul> <li> Noise generation (<code>get_noise_newmodel()</code>)</li> <li> Pack/unpack functions (if transformer-based)</li> <li> Schedule generation (<code>get_schedule_newmodel()</code>)</li> <li> Position ID generation (if needed)</li> <li> Implement denoise loop</li> <li> Scheduler integration</li> <li> Inpaint extension integration</li> <li> Progress callbacks</li> </ul>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#6-frontend-graph-building","title":"6. Frontend: Graph Building","text":""},{"location":"contributing/NEW_MODEL_INTEGRATION/#61-create-graph-builder","title":"6.1 Create Graph Builder","text":"<p>File: <code>invokeai/frontend/web/src/features/nodes/util/graph/generation/buildNewModelGraph.ts</code></p> <pre><code>export const buildNewModelGraph = async (arg: GraphBuilderArg): Promise&lt;GraphBuilderResult&gt; =&gt; {\n  const { state, manager } = arg;\n  const { model } = state.params;\n\n  const g = new Graph();\n\n  // 1. Model Loader\n  const modelLoader = g.addNode({\n    id: NEWMODEL_MODEL_LOADER,\n    type: 'newmodel_model_loader',\n    model: Graph.getModelMetadataField(model),\n  });\n\n  // 2. Text Encoder\n  const positivePrompt = g.addNode({\n    id: POSITIVE_CONDITIONING,\n    type: 'newmodel_text_encode',\n    prompt: positivePromptText,\n  });\n  g.addEdge(modelLoader, 'encoder', positivePrompt, 'encoder');\n\n  // 3. Denoise Node\n  const denoise = g.addNode({\n    id: NEWMODEL_DENOISE,\n    type: 'newmodel_denoise',\n    steps,\n    cfg_scale: cfg,\n    scheduler: newmodelScheduler,\n    denoising_start: 0,\n    denoising_end: 1,\n  });\n  g.addEdge(modelLoader, 'transformer', denoise, 'transformer');\n  g.addEdge(positivePrompt, 'conditioning', denoise, 'positive_conditioning');\n\n  // 4. VAE Decode\n  const l2i = g.addNode({\n    id: NEWMODEL_VAE_DECODE,\n    type: 'newmodel_vae_decode',\n  });\n  g.addEdge(modelLoader, 'vae', l2i, 'vae');\n  g.addEdge(denoise, 'latents', l2i, 'latents');\n\n  // 5. Generation Mode Handling\n  let canvasOutput: Invocation&lt;ImageOutputNodes&gt; = l2i;\n\n  switch (generationMode) {\n    case 'txt2img':\n      canvasOutput = addTextToImage({ g, state, denoise, l2i });\n      g.upsertMetadata({ generation_mode: 'newmodel_txt2img' });\n      break;\n    case 'img2img':\n      const i2l = g.addNode({ type: 'newmodel_vae_encode' });\n      canvasOutput = await addImageToImage({ g, state, manager, denoise, l2i, i2l, ... });\n      g.upsertMetadata({ generation_mode: 'newmodel_img2img' });\n      break;\n    case 'inpaint':\n      canvasOutput = await addInpaint({ g, state, manager, denoise, l2i, i2l, ... });\n      g.upsertMetadata({ generation_mode: 'newmodel_inpaint' });\n      break;\n    case 'outpaint':\n      canvasOutput = await addOutpaint({ g, state, manager, denoise, l2i, i2l, ... });\n      g.upsertMetadata({ generation_mode: 'newmodel_outpaint' });\n      break;\n  }\n\n  return { g, noise, denoise, posCond: positivePrompt, ... };\n};\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#62-register-graph-builder","title":"6.2 Register Graph Builder","text":"<p>File: <code>invokeai/frontend/web/src/features/queue/hooks/useEnqueueCanvas.ts</code></p> <pre><code>// In the buildGraph function (around line 47-64):\nswitch (base) {\n  case 'sd-1':\n  case 'sd-2':\n  case 'sdxl':\n    return buildSD1Graph(arg);\n  case 'flux':\n    return buildFLUXGraph(arg);\n  case 'flux2':\n    return buildFLUXGraph(arg);  // FLUX.2 uses the same builder\n  case 'sd-3':\n    return buildSD3Graph(arg);\n  case 'z-image':\n    return buildZImageGraph(arg);\n  // NEW:\n  case 'newmodel':\n    return buildNewModelGraph(arg);\n}\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#63-update-type-definitions","title":"6.3 Update Type Definitions","text":"<p>File: <code>invokeai/frontend/web/src/features/nodes/util/graph/types.ts</code></p> <pre><code>// Add node types:\nexport type ImageOutputNodes =\n  | 'l2i' | 'flux_vae_decode' | 'flux2_vae_decode'\n  | 'sd3_l2i' | 'newmodel_vae_decode';\n\nexport type LatentToImageNodes =\n  | 'l2i' | 'flux_vae_decode' | 'flux2_vae_decode'\n  | 'sd3_l2i' | 'newmodel_vae_decode';\n\nexport type ImageToLatentsNodes =\n  | 'i2l' | 'flux_vae_encode' | 'flux2_vae_encode'\n  | 'sd3_i2l' | 'newmodel_vae_encode';\n\nexport type DenoiseLatentsNodes =\n  | 'denoise_latents' | 'flux_denoise' | 'flux2_denoise'\n  | 'sd3_denoise' | 'newmodel_denoise';\n\nexport type MainModelLoaderNodes =\n  | 'main_model_loader' | 'flux_model_loader' | 'flux2_klein_model_loader'\n  | 'sd3_model_loader' | 'newmodel_model_loader';\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#64-update-generation-mode-utilities","title":"6.4 Update Generation Mode Utilities","text":"<p>Files: - <code>invokeai/frontend/web/src/features/nodes/util/graph/generation/addImageToImage.ts</code> - <code>invokeai/frontend/web/src/features/nodes/util/graph/generation/addInpaint.ts</code> - <code>invokeai/frontend/web/src/features/nodes/util/graph/generation/addOutpaint.ts</code></p> <pre><code>// In addImageToImage.ts - extend type check:\nif (\n  denoise.type === 'cogview4_denoise' ||\n  denoise.type === 'flux_denoise' ||\n  denoise.type === 'flux2_denoise' ||\n  denoise.type === 'newmodel_denoise'  // NEW\n) {\n  // Rectified flow models: denoising_start instead of noise\n}\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#frontend-graph-building-checklist","title":"Frontend Graph Building Checklist","text":"<ul> <li> Create graph builder (<code>buildNewModelGraph.ts</code>)</li> <li> Register graph builder in useEnqueueCanvas</li> <li> Update type definitions (<code>types.ts</code>)</li> <li> Extend node type unions (ImageOutputNodes, etc.)</li> <li> Update <code>addImageToImage.ts</code></li> <li> Update <code>addInpaint.ts</code></li> <li> Update <code>addOutpaint.ts</code></li> </ul>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#7-frontend-state-management","title":"7. Frontend: State Management","text":""},{"location":"contributing/NEW_MODEL_INTEGRATION/#71-add-parameter-state","title":"7.1 Add Parameter State","text":"<p>File: <code>invokeai/frontend/web/src/features/controlLayers/store/paramsSlice.ts</code></p> <pre><code>// Extend state interface:\ninterface ParamsState {\n  // Existing fields\n  fluxScheduler: 'euler' | 'heun' | 'lcm';\n  zImageScheduler: 'euler' | 'heun' | 'lcm';\n\n  // NEW: NewModel specific parameters\n  newmodelScheduler: 'euler' | 'heun' | 'lcm';\n  newmodelVaeModel: ParameterVAEModel | null;\n  newmodelEncoderModel: ParameterModel | null;\n}\n\n// Initial state:\nconst initialState: ParamsState = {\n  newmodelScheduler: 'euler',\n  newmodelVaeModel: null,\n  newmodelEncoderModel: null,\n};\n\n// Reducers:\nreducers: {\n  setNewmodelScheduler: (state, action: PayloadAction&lt;'euler' | 'heun' | 'lcm'&gt;) =&gt; {\n    state.newmodelScheduler = action.payload;\n  },\n  newmodelVaeModelSelected: (state, action: PayloadAction&lt;ParameterVAEModel | null&gt;) =&gt; {\n    state.newmodelVaeModel = action.payload;\n  },\n  newmodelEncoderModelSelected: (state, action: PayloadAction&lt;ParameterModel | null&gt;) =&gt; {\n    state.newmodelEncoderModel = action.payload;\n  },\n}\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#72-create-selectors","title":"7.2 Create Selectors","text":"<p>File: <code>invokeai/frontend/web/src/features/controlLayers/store/paramsSlice.ts</code></p> <pre><code>// Selectors:\nexport const selectNewmodelScheduler = createSelector(\n  selectParamsSlice,\n  (params) =&gt; params.newmodelScheduler\n);\n\nexport const selectNewmodelVaeModel = createSelector(\n  selectParamsSlice,\n  (params) =&gt; params.newmodelVaeModel\n);\n\nexport const selectNewmodelEncoderModel = createSelector(\n  selectParamsSlice,\n  (params) =&gt; params.newmodelEncoderModel\n);\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#frontend-state-management-checklist","title":"Frontend State Management Checklist","text":"<ul> <li> Extend state interface for model-specific parameters</li> <li> Define initial state</li> <li> Create reducer actions</li> <li> Create selectors</li> <li> Export actions</li> </ul>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#8-frontend-parameter-recall","title":"8. Frontend: Parameter Recall","text":""},{"location":"contributing/NEW_MODEL_INTEGRATION/#81-metadata-parsing","title":"8.1 Metadata Parsing","text":"<p>File: <code>invokeai/frontend/web/src/features/metadata/parsing.tsx</code></p> <pre><code>// Add parameter recall handlers:\nconst recallNewmodelScheduler = (metadata: CoreMetadata) =&gt; {\n  if (metadata.scheduler) {\n    dispatch(setNewmodelScheduler(metadata.scheduler));\n  }\n};\n\nconst recallNewmodelVaeModel = async (metadata: CoreMetadata) =&gt; {\n  if (metadata.vae) {\n    const vaeModel = await fetchModelConfig(metadata.vae);\n    dispatch(newmodelVaeModelSelected(vaeModel));\n  }\n};\n\nconst recallNewmodelEncoderModel = async (metadata: CoreMetadata) =&gt; {\n  if (metadata.encoder_model) {\n    const encoderModel = await fetchModelConfig(metadata.encoder_model);\n    dispatch(newmodelEncoderModelSelected(encoderModel));\n  }\n};\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#frontend-parameter-recall-checklist","title":"Frontend Parameter Recall Checklist","text":"<ul> <li> Recall handlers for each model-specific parameter</li> <li> Model config fetching for submodels</li> <li> Dispatch actions for state updates</li> </ul>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#9-metadata-and-generation-modes","title":"9. Metadata and Generation Modes","text":""},{"location":"contributing/NEW_MODEL_INTEGRATION/#91-add-generation-modes","title":"9.1 Add Generation Modes","text":"<p>File: <code>invokeai/app/invocations/metadata.py</code></p> <pre><code>GENERATION_MODES = Literal[\n    # Existing modes\n    \"txt2img\", \"img2img\", \"inpaint\", \"outpaint\",\n    \"sdxl_txt2img\", \"sdxl_img2img\", \"sdxl_inpaint\", \"sdxl_outpaint\",\n    \"flux_txt2img\", \"flux_img2img\", \"flux_inpaint\", \"flux_outpaint\",\n    \"flux2_txt2img\", \"flux2_img2img\", \"flux2_inpaint\", \"flux2_outpaint\",\n    \"sd3_txt2img\", \"sd3_img2img\", \"sd3_inpaint\", \"sd3_outpaint\",\n    # NEW:\n    \"newmodel_txt2img\",\n    \"newmodel_img2img\",\n    \"newmodel_inpaint\",\n    \"newmodel_outpaint\",\n]\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#92-extend-coremetadata-if-needed","title":"9.2 Extend CoreMetadata (if needed)","text":"<p>File: <code>invokeai/app/invocations/metadata.py</code></p> <pre><code>@invocation_output(\"core_metadata_output\")\nclass CoreMetadataOutput(BaseInvocationOutput):\n    # Existing fields\n    model: ModelIdentifierField | None = None\n    steps: int | None = None\n    cfg_scale: float | None = None\n\n    # NEW: Model-specific metadata fields\n    newmodel_encoder: ModelIdentifierField | None = None\n    newmodel_custom_param: float | None = None\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#metadata-and-generation-modes-checklist","title":"Metadata and Generation Modes Checklist","text":"<ul> <li> Add generation modes to <code>GENERATION_MODES</code></li> <li> Extend CoreMetadata if model-specific fields needed</li> <li> Set metadata in graph builder (<code>g.upsertMetadata({...})</code>)</li> </ul>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#10-starter-models","title":"10. Starter Models","text":""},{"location":"contributing/NEW_MODEL_INTEGRATION/#101-define-starter-models","title":"10.1 Define Starter Models","text":"<p>File: <code>invokeai/backend/model_manager/starter_models.py</code></p> <pre><code># Main Model\nnewmodel_main = StarterModel(\n    name=\"NewModel Main\",\n    base=BaseModelType.NewModel,\n    source=\"organization/newmodel-main\",  # HuggingFace repo\n    description=\"NewModel main transformer. ~10GB\",\n    type=ModelType.Main,\n)\n\n# VAE (if separate)\nnewmodel_vae = StarterModel(\n    name=\"NewModel VAE\",\n    base=BaseModelType.NewModel,\n    source=\"organization/newmodel::vae\",  # Submodel syntax\n    description=\"NewModel VAE. ~500MB\",\n    type=ModelType.VAE,\n)\n\n# Text Encoder (if separate)\nnewmodel_encoder = StarterModel(\n    name=\"NewModel Encoder\",\n    base=BaseModelType.Any,\n    source=\"organization/newmodel::text_encoder+tokenizer\",\n    description=\"NewModel text encoder. ~5GB\",\n    type=ModelType.TextEncoder,\n)\n\n# Quantized variants\nnewmodel_fp8 = StarterModel(\n    name=\"NewModel (FP8)\",\n    base=BaseModelType.NewModel,\n    source=\"https://huggingface.co/org/newmodel-fp8/resolve/main/model.safetensors\",\n    description=\"FP8 quantized version. ~5GB\",\n    type=ModelType.Main,\n    dependencies=[newmodel_vae, newmodel_encoder],  # Dependencies!\n)\n\n# Add to STARTER_MODELS list:\nSTARTER_MODELS: list[StarterModel] = [\n    # ... existing models\n    newmodel_main,\n    newmodel_vae,\n    newmodel_encoder,\n    newmodel_fp8,\n]\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#starter-models-checklist","title":"Starter Models Checklist","text":"<ul> <li> Define main model StarterModel</li> <li> Define VAE StarterModel if separate</li> <li> Define text encoder StarterModel if separate</li> <li> Define quantized variants (FP8, GGUF, etc.)</li> <li> Set dependencies correctly</li> <li> Add to <code>STARTER_MODELS</code> list</li> </ul>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#11-optional-features","title":"11. Optional Features","text":""},{"location":"contributing/NEW_MODEL_INTEGRATION/#111-controlnet-support","title":"11.1 ControlNet Support","text":"<p>Backend Config: File: <code>invokeai/backend/model_manager/configs/controlnet.py</code></p> <pre><code>@ModelConfigFactory.register\nclass ControlNet_Checkpoint_NewModel_Config(ControlNet_Checkpoint_Base):\n    base: Literal[BaseModelType.NewModel] = BaseModelType.NewModel\n\n    @classmethod\n    def from_model_on_disk(cls, mod: ModelOnDisk, ...) -&gt; Self:\n        if not _is_newmodel_controlnet(mod.state_dict):\n            raise NotAMatchError()\n        return cls(...)\n</code></pre> <p>Backend Invocation: File: <code>invokeai/app/invocations/[newmodel]_controlnet.py</code></p> <pre><code>@invocation(\"newmodel_controlnet\", ...)\nclass NewModelControlNetInvocation(BaseInvocation):\n    image: ImageField = InputField()\n    controlnet_model: ControlNetField = InputField()\n    control_weight: float = InputField(default=1.0)\n\n    def invoke(self, context) -&gt; ControlNetOutput:\n        # Compute ControlNet conditioning\n        pass\n</code></pre> <p>Frontend Graph: <pre><code>// In buildNewModelGraph.ts:\nconst { controlNets } = await addControlNets({ g, manager, denoise });\n</code></pre></p>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#112-ip-adapter-reference-images","title":"11.2 IP-Adapter / Reference Images","text":"<p>Backend Invocation: File: <code>invokeai/app/invocations/[newmodel]_ip_adapter.py</code></p> <pre><code>@invocation(\"newmodel_ip_adapter\", ...)\nclass NewModelIPAdapterInvocation(BaseInvocation):\n    image: ImageField = InputField()\n    ip_adapter_model: IPAdapterField = InputField()\n    weight: float = InputField(default=1.0)\n</code></pre> <p>Frontend Graph: <pre><code>// In buildNewModelGraph.ts:\nconst { ipAdapters } = await addIPAdapters({ g, manager, denoise });\n</code></pre></p>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#113-lora-support","title":"11.3 LoRA Support","text":"<p>Backend Config: File: <code>invokeai/backend/model_manager/configs/lora.py</code></p> <pre><code>@ModelConfigFactory.register\nclass LoRA_LyCORIS_NewModel_Config(LoRA_LyCORIS_Base):\n    base: Literal[BaseModelType.NewModel] = BaseModelType.NewModel\n</code></pre> <p>Backend Model Loader Integration: <pre><code># In newmodel_model_loader.py:\nclass NewModelModelLoaderOutput(BaseInvocationOutput):\n    transformer: TransformerField  # TransformerField already contains loras: list[LoRAField]\n</code></pre></p> <p>Frontend Graph: <pre><code>// In buildNewModelGraph.ts:\nconst { loras } = await addLoRAs({ g, manager, denoise, modelLoader });\n</code></pre></p>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#114-scheduler-ui","title":"11.4 Scheduler UI","text":"<p>Frontend Component: File: <code>invokeai/frontend/web/src/features/parameters/components/NewModelScheduler.tsx</code></p> <pre><code>export const NewModelSchedulerSelect = () =&gt; {\n  const dispatch = useAppDispatch();\n  const scheduler = useAppSelector(selectNewmodelScheduler);\n\n  return (\n    &lt;Select\n      value={scheduler}\n      onChange={(value) =&gt; dispatch(setNewmodelScheduler(value))}\n      options={[\n        { value: 'euler', label: 'Euler' },\n        { value: 'heun', label: 'Heun' },\n        { value: 'lcm', label: 'LCM' },\n      ]}\n    /&gt;\n  );\n};\n</code></pre>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#optional-features-checklist","title":"Optional Features Checklist","text":"<p>ControlNet: - [ ] Create ControlNet config (<code>configs/controlnet.py</code>) - [ ] Create ControlNet invocation - [ ] Frontend graph integration (<code>addControlNets</code>)</p> <p>IP-Adapter: - [ ] Create IP-Adapter invocation - [ ] Create image encoder config if needed - [ ] Frontend graph integration (<code>addIPAdapters</code>)</p> <p>LoRA: - [ ] Create LoRA config (<code>configs/lora.py</code>) - [ ] LoRA loading in model loader - [ ] Frontend graph integration (<code>addLoRAs</code>)</p> <p>Scheduler: - [ ] Define scheduler constants - [ ] Frontend UI component - [ ] State management</p>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#summary-minimal-integration","title":"Summary: Minimal Integration","text":"<p>For a minimal txt2img integration, the following files are required:</p>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#backend-python","title":"Backend (Python):","text":"<ol> <li><code>invokeai/backend/model_manager/taxonomy.py</code> - BaseModelType, Variant</li> <li><code>invokeai/backend/model_manager/configs/main.py</code> - Model configs</li> <li><code>invokeai/backend/model_manager/configs/factory.py</code> - AnyModelConfig</li> <li><code>invokeai/backend/model_manager/load/model_loaders/[newmodel].py</code> - Loader</li> <li><code>invokeai/app/invocations/[newmodel]_model_loader.py</code></li> <li><code>invokeai/app/invocations/[newmodel]_text_encoder.py</code></li> <li><code>invokeai/app/invocations/[newmodel]_denoise.py</code></li> <li><code>invokeai/app/invocations/[newmodel]_vae_decode.py</code></li> <li><code>invokeai/backend/[newmodel]/sampling_utils.py</code></li> <li><code>invokeai/backend/[newmodel]/denoise.py</code></li> <li><code>invokeai/app/invocations/metadata.py</code> - Generation modes</li> </ol>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#frontend-typescript","title":"Frontend (TypeScript):","text":"<ol> <li><code>src/features/nodes/util/graph/generation/buildNewModelGraph.ts</code></li> <li><code>src/features/nodes/util/graph/types.ts</code></li> <li><code>src/features/queue/hooks/useEnqueueCanvas.ts</code></li> <li><code>src/features/controlLayers/store/paramsSlice.ts</code></li> </ol>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#additional-for-img2imginpaintoutpaint","title":"Additional for img2img/inpaint/outpaint:","text":"<ol> <li><code>invokeai/app/invocations/[newmodel]_vae_encode.py</code></li> <li><code>src/features/nodes/util/graph/generation/addImageToImage.ts</code></li> <li><code>src/features/nodes/util/graph/generation/addInpaint.ts</code></li> <li><code>src/features/nodes/util/graph/generation/addOutpaint.ts</code></li> </ol>"},{"location":"contributing/NEW_MODEL_INTEGRATION/#reference-existing-implementations","title":"Reference: Existing Implementations","text":"Feature FLUX.1 FLUX.2 Klein SD3 SDXL Z-Image Latent Channels 16 32 16 4 32 Text Encoder CLIP + T5 Qwen3 CLIP\u00d73 + T5 CLIP\u00d72 Qwen3 VAE 16ch 32ch+BN 16ch 4ch 32ch CFG Optional Optional Yes Yes Optional Guidance Embed Dev only No No No No Pack/Unpack Yes Yes No No Yes"},{"location":"contributing/PR-MERGE-POLICY/","title":"Pull Request Merge Policy","text":"<p>This document outlines the process for reviewing and merging pull requests (PRs) into the InvokeAI repository.</p>"},{"location":"contributing/PR-MERGE-POLICY/#review-process","title":"Review Process","text":""},{"location":"contributing/PR-MERGE-POLICY/#1-assignment","title":"1. Assignment","text":"<p>One of the repository maintainers will assign collaborators to review a pull request. The assigned reviewer(s) will be responsible for conducting the code review.</p>"},{"location":"contributing/PR-MERGE-POLICY/#2-review-and-iteration","title":"2. Review and Iteration","text":"<p>The assignee is responsible for: - Reviewing the PR thoroughly - Providing constructive feedback - Iterating with the PR author until the assignee is satisfied that the PR is fit to merge - Ensuring the PR meets code quality standards, follows project conventions, and doesn't introduce bugs or regressions</p>"},{"location":"contributing/PR-MERGE-POLICY/#3-approval-and-notification","title":"3. Approval and Notification","text":"<p>Once the assignee is satisfied with the PR: - The assignee approves the PR - The assignee alerts one of the maintainers that the PR is ready for merge using the #request-reviews Discord channel</p>"},{"location":"contributing/PR-MERGE-POLICY/#4-final-merge","title":"4. Final Merge","text":"<p>One of the maintainers is responsible for: - Performing a final check of the PR - Merging the PR into the appropriate branch</p> <p>Important: Collaborators are strongly discouraged from merging PRs on their own, except in case of emergency (e.g., critical bug fix and no maintainer is available).</p>"},{"location":"contributing/PR-MERGE-POLICY/#5-release-policy","title":"5. Release Policy","text":"<p>Once a feature release candidate is published, no feature PRs are to be merged into main. Only bugfixes are allowed until the final release.</p>"},{"location":"contributing/PR-MERGE-POLICY/#best-practices","title":"Best Practices","text":""},{"location":"contributing/PR-MERGE-POLICY/#clean-commit-history","title":"Clean Commit History","text":"<p>To encourage a clean development log, PR authors are encouraged to use <code>git rebase -i</code> to suppress trivial commit messages (e.g., <code>ruff</code> and <code>prettier</code> formatting fixes) after the PR is accepted but before it is merged.</p>"},{"location":"contributing/PR-MERGE-POLICY/#merge-strategy","title":"Merge Strategy","text":"<p>The maintainer will perform either a 3-way merge or squash merge when merging a PR into the <code>main</code> branch. This approach helps avoid rebase conflict hell and maintains a cleaner project history.</p>"},{"location":"contributing/PR-MERGE-POLICY/#attribution","title":"Attribution","text":"<p>The PR author should reference any papers, source code or documentation that they used while creating the code both in the PR and as comments in the code itself. If there are any licensing restrictions, these should be linked to and/or reproduced in the repo root.</p>"},{"location":"contributing/PR-MERGE-POLICY/#summary","title":"Summary","text":"<p>This policy ensures that: - All PRs receive proper review from assigned collaborators - Maintainers have final oversight before code enters the main branch - The commit history remains clean and meaningful - Merge conflicts are minimized through appropriate merge strategies</p>"},{"location":"contributing/TESTS/","title":"InvokeAI Backend Tests","text":"<p>We use <code>pytest</code> to run the backend python tests. (See pyproject.toml for the default <code>pytest</code> options.)</p>"},{"location":"contributing/TESTS/#fast-vs-slow","title":"Fast vs. Slow","text":"<p>All tests are categorized as either 'fast' (no test annotation) or 'slow' (annotated with the <code>@pytest.mark.slow</code> decorator).</p> <p>'Fast' tests are run to validate every PR, and are fast enough that they can be run routinely during development.</p> <p>'Slow' tests are currently only run manually on an ad-hoc basis. In the future, they may be automated to run nightly. Most developers are only expected to run the 'slow' tests that directly relate to the feature(s) that they are working on.</p> <p>As a rule of thumb, tests should be marked as 'slow' if there is a chance that they take &gt;1s (e.g. on a CPU-only machine with slow internet connection). Common examples of slow tests are tests that depend on downloading a model, or running model inference.</p>"},{"location":"contributing/TESTS/#running-tests","title":"Running Tests","text":"<p>Below are some common test commands: <pre><code># Run the fast tests. (This implicitly uses the configured default option: `-m \"not slow\"`.)\npytest tests/\n\n# Equivalent command to run the fast tests.\npytest tests/ -m \"not slow\"\n\n# Run the slow tests.\npytest tests/ -m \"slow\"\n\n# Run the slow tests from a specific file.\npytest tests/path/to/slow_test.py -m \"slow\"\n\n# Run all tests (fast and slow).\npytest tests -m \"\"\n</code></pre></p>"},{"location":"contributing/TESTS/#test-organization","title":"Test Organization","text":"<p>All backend tests are in the <code>tests/</code> directory. This directory mirrors the organization of the <code>invokeai/</code> directory. For example, tests for <code>invokeai/model_management/model_manager.py</code> would be found in <code>tests/model_management/test_model_manager.py</code>.</p> <p>TODO: The above statement is aspirational. A re-organization of legacy tests is required to make it true.</p>"},{"location":"contributing/TESTS/#tests-that-depend-on-models","title":"Tests that depend on models","text":"<p>There are a few things to keep in mind when adding tests that depend on models.</p> <ol> <li>If a required model is not already present, it should automatically be downloaded as part of the test setup.</li> <li>If a model is already downloaded, it should not be re-downloaded unnecessarily.</li> <li>Take reasonable care to keep the total number of models required for the tests low. Whenever possible, re-use models that are already required for other tests. If you are adding a new model, consider including a comment to explain why it is required/unique.</li> </ol> <p>There are several utilities to help with model setup for tests. Here is a sample test that depends on a model: <pre><code>import pytest\nimport torch\n\nfrom invokeai.backend.model_management.models.base import BaseModelType, ModelType\nfrom invokeai.backend.util.test_utils import install_and_load_model\n\n@pytest.mark.slow\ndef test_model(model_installer, torch_device):\n    model_info = install_and_load_model(\n        model_installer=model_installer,\n        model_path_id_or_url=\"HF/dummy_model_id\",\n        model_name=\"dummy_model\",\n        base_model=BaseModelType.StableDiffusion1,\n        model_type=ModelType.Dummy,\n    )\n\n    dummy_input = build_dummy_input(torch_device)\n\n    with torch.no_grad(), model_info as model:\n        model.to(torch_device, dtype=torch.float32)\n        output = model(dummy_input)\n\n    # Validate output...\n</code></pre></p>"},{"location":"contributing/TESTS/#test-coverage","title":"Test Coverage","text":"<p>To review test coverage, append <code>--cov</code> to your pytest command: <pre><code>pytest tests/ --cov\n</code></pre></p> <p>Test outcomes and coverage will be reported in the terminal. In addition, a more detailed report is created in both XML and HTML format in the <code>./coverage</code> folder. The HTML output is particularly helpful in identifying untested statements where coverage should be improved. The HTML report can be viewed by opening <code>./coverage/html/index.html</code>.</p> HTML coverage report output <p></p> <p></p>"},{"location":"contributing/contributors/","title":"Contributors","text":"<p>We thank all contributors for their time and hard work!</p>"},{"location":"contributing/contributors/#original-author","title":"Original Author","text":"<ul> <li>Lincoln D. Stein</li> </ul>"},{"location":"contributing/contributors/#current-core-team","title":"Current Core Team","text":"<ul> <li>@lstein (Lincoln Stein) - Co-maintainer</li> <li>@blessedcoolant - Co-maintainer</li> <li>@hipsterusername (Kent Keirsey) - Co-maintainer, CEO, Positive Vibes</li> <li>@psychedelicious (Spencer Mabrito) - Web Team Leader</li> <li>@joshistoast (Josh Corbett) - Web Development</li> <li>@cheerio (Mary Rogers) - Lead Engineer &amp; Web App Development</li> <li>@ebr (Eugene Brodsky) - Cloud/DevOps/Software engineer; your friendly neighbourhood cluster-autoscaler</li> <li>@sunija - Standalone version</li> <li>@brandon (Brandon Rising) - Platform, Infrastructure, Backend Systems</li> <li>@ryanjdick (Ryan Dick) - Machine Learning &amp; Training</li> <li>@JPPhoto - Core image generation nodes</li> <li>@dunkeroni - Image generation backend</li> <li>@SkunkWorxDark - Image generation backend</li> <li>@glimmerleaf (Devon Hopkins) - Community Wizard</li> <li>@gogurt enjoyer - Discord moderator and end user support</li> <li>@whosawhatsis - Discord moderator and end user support</li> <li>@dwringer - Discord moderator and end user support</li> <li>@526christian - Discord moderator and end user support</li> <li>@harvester62 - Discord moderator and end user support</li> </ul>"},{"location":"contributing/contributors/#honored-team-alumni","title":"Honored Team Alumni","text":"<ul> <li>@StAlKeR7779 (Sergey Borisov) - Torch stack, ONNX, model management, optimization</li> <li>@damian0815 - Attention Systems and Compel Maintainer</li> <li>@netsvetaev (Artur) - Localization support</li> <li>@Kyle0654 (Kyle Schouviller) - Node Architect and General Backend Wizard</li> <li>@tildebyte - Installation and configuration</li> <li>@mauwii (Matthias Wilde) - Installation, release, continuous integration</li> <li>@chainchompa (Jennifer Player) - Web Development &amp; Chain-Chomping</li> <li>@millu (Millun Atluri) - Community Wizard, Documentation, Node-wrangler,</li> <li>@genomancer (Gregg Helt) - Controlnet support</li> <li>@keturn (Kevin Turner) - Diffusers</li> </ul>"},{"location":"contributing/contributors/#original-compvis-stable-diffusion-authors","title":"Original CompVis (Stable Diffusion) Authors","text":"<ul> <li>Robin Rombach</li> <li>Patrick von Platen</li> <li>ablattmann</li> <li>Patrick Esser</li> <li>owenvincent</li> <li>apolinario</li> <li>Charles Packer</li> </ul>"},{"location":"contributing/dev-environment/","title":"Dev Environment","text":"<p>To make changes to Invoke's backend, frontend or documentation, you'll need to set up a dev environment.</p> <p>If you only want to make changes to the docs site, you can skip the frontend dev environment setup as described in the below guide.</p> <p>If you just want to use Invoke, you should use the launcher.</p> <p>Warning</p> <p>Invoke uses a SQLite database. When you run the application as a dev install, you accept responsibility for your database. This means making regular backups (especially before pulling) and/or fixing it yourself in the event that a PR introduces a schema change.</p> <p>If you don't need to persist your db, you can use an ephemeral in-memory database by setting <code>use_memory_db: true</code> in your <code>invokeai.yaml</code> file. You'll also want to set <code>scan_models_on_startup: true</code> so that your models are registered on startup.</p>"},{"location":"contributing/dev-environment/#setup","title":"Setup","text":"<ol> <li> <p>Run through the requirements.</p> </li> <li> <p>Fork and clone the InvokeAI repo.</p> </li> <li> <p>This repository uses Git LFS to manage large files. To ensure all assets are downloaded:</p> <ul> <li>Install git-lfs \u2192 Download here</li> <li>Enable automatic LFS fetching for this repository:     <pre><code>git config lfs.fetchinclude \"*\"\n</code></pre><ul> <li>Fetch files from LFS (only needs to be done once; subsequent <code>git pull</code> will fetch changes automatically): <pre><code>git lfs pull\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>Create an directory for user data (images, models, db, etc). This is typically at <code>~/invokeai</code>, but if you already have a non-dev install, you may want to create a separate directory for the dev install.</p> </li> <li> <p>Follow the manual install guide, with some modifications to the install command:</p> <ul> <li> <p>Use <code>.</code> instead of <code>invokeai</code> to install from the current directory. You don't need to specify the version.</p> </li> <li> <p>Add <code>-e</code> after the <code>install</code> operation to make this an editable install. That means your changes to the python code will be reflected when you restart the Invoke server.</p> </li> <li> <p>When installing the <code>invokeai</code> package, add the <code>dev</code>, <code>test</code> and <code>docs</code> package options to the package specifier. You may or may not need the <code>xformers</code> option - follow the manual install guide to figure that out. So, your package specifier will be either <code>\".[dev,test,docs]\"</code> or <code>\".[dev,test,docs,xformers]\"</code>. Note the quotes!</p> </li> </ul> <p>With the modifications made, the install command should look something like this:</p> <pre><code>uv pip install -e \".[dev,test,docs,xformers]\" --python 3.12 --python-preference only-managed --index=https://download.pytorch.org/whl/cu128 --reinstall\n</code></pre> </li> <li> <p>At this point, you should have Invoke installed, a venv set up and activated, and the server running. But you will see a warning in the terminal that no UI was found. If you go to the URL for the server, you won't get a UI.</p> <p>This is because the UI build is not distributed with the source code. You need to build it manually. End the running server instance.</p> <p>If you only want to edit the docs, you can stop here and skip to the Documentation section below.</p> </li> <li> <p>Install the frontend dev toolchain, paying attention to versions:</p> <ul> <li> <p><code>nodejs</code> (tested on LTS, v22)</p> </li> <li> <p><code>pnpm</code> (tested on v10)</p> </li> </ul> </li> <li> <p>Do a production build of the frontend:</p> <pre><code>cd &lt;PATH_TO_INVOKEAI_REPO&gt;/invokeai/frontend/web\npnpm i\npnpm build\n</code></pre> </li> <li> <p>Restart the server and navigate to the URL. You should get a UI. After making changes to the python code, restart the server to see those changes.</p> </li> </ol>"},{"location":"contributing/dev-environment/#updating-the-ui","title":"Updating the UI","text":"<p>You'll need to run <code>pnpm build</code> every time you pull in new changes.</p> <p>Another option is to skip the build and instead run the UI in dev mode:</p> <pre><code>pnpm dev\n</code></pre> <p>This starts a vite dev server for the UI at <code>127.0.0.1:5173</code>, which you will use instead of <code>127.0.0.1:9090</code>.</p> <p>The dev mode is substantially slower than the production build but may be more convenient if you just need to test things out. It will hot-reload the UI as you make changes to the frontend code. Sometimes the hot-reload doesn't work, and you need to manually refresh the browser tab.</p>"},{"location":"contributing/dev-environment/#documentation","title":"Documentation","text":"<p>The documentation is built with <code>mkdocs</code>. It provides a hot-reload dev server for the docs. Start it with <code>mkdocs serve</code>.</p>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/","title":"Recall Parameters API - LoRAs, ControlNets, and IP Adapters with Images","text":""},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#overview","title":"Overview","text":"<p>The Recall Parameters API supports recalling LoRAs, ControlNets (including T2I Adapters and Control LoRAs), and IP Adapters along with their associated weights and settings. Control Layers and IP Adapters can now include image references from the <code>INVOKEAI_ROOT/outputs/images</code> directory for fully functional control and image prompt functionality.</p>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#key-features","title":"Key Features","text":"<p>\u2705 LoRAs: Fully functional - adds to UI, queries model configs, applies weights \u2705 Control Layers: Full support with optional images from outputs/images \u2705 IP Adapters: Full support with optional reference images from outputs/images \u2705 Model Name Resolution: Automatic lookup from human-readable names to internal keys \u2705 Image Validation: Backend validates that image files exist before sending</p>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#endpoints","title":"Endpoints","text":""},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#post-apiv1recallqueue_id","title":"POST <code>/api/v1/recall/{queue_id}</code>","text":"<p>Updates recallable parameters for the frontend, including LoRAs, control adapters, and IP adapters with optional images.</p> <p>Path Parameters: - <code>queue_id</code> (string): The queue ID to associate parameters with (typically \"default\")</p> <p>Request Body:</p> <p>All fields are optional. Include only the parameters you want to update.</p> <pre><code>{\n  // Standard parameters\n  positive_prompt?: string;\n  negative_prompt?: string;\n  model?: string;           // Model name or key\n  steps?: number;\n  cfg_scale?: number;\n  width?: number;\n  height?: number;\n  seed?: number;\n  // ... other standard parameters\n\n  // LoRAs\n  loras?: Array&lt;{\n    model_name: string;     // LoRA model name\n    weight?: number;        // Default: 0.75, Range: -10 to 10\n    is_enabled?: boolean;   // Default: true\n  }&gt;;\n\n  // Control Layers (ControlNet, T2I Adapter, Control LoRA)\n  control_layers?: Array&lt;{\n    model_name: string;            // Control adapter model name\n    image_name?: string;           // Optional image filename from outputs/images\n    weight?: number;               // Default: 1.0, Range: -1 to 2\n    begin_step_percent?: number;   // Default: 0.0, Range: 0 to 1\n    end_step_percent?: number;     // Default: 1.0, Range: 0 to 1\n    control_mode?: \"balanced\" | \"more_prompt\" | \"more_control\";  // ControlNet only\n  }&gt;;\n\n  // IP Adapters\n  ip_adapters?: Array&lt;{\n    model_name: string;            // IP Adapter model name\n    image_name?: string;           // Optional reference image filename from outputs/images\n    weight?: number;               // Default: 1.0, Range: -1 to 2\n    begin_step_percent?: number;   // Default: 0.0, Range: 0 to 1\n    end_step_percent?: number;     // Default: 1.0, Range: 0 to 1\n    method?: \"full\" | \"style\" | \"composition\";  // Default: \"full\"\n    influence?: \"Lowest\" | \"Low\" | \"Medium\" | \"High\" | \"Highest\";  // Flux Redux only; default: \"highest\"\n  }&gt;;\n}\n</code></pre>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#model-name-resolution","title":"Model Name Resolution","text":"<p>The backend automatically resolves model names to their internal keys:</p> <ol> <li>Main Models: Resolved from the name to the model key</li> <li>LoRAs: Searched in the LoRA model database</li> <li>Control Adapters: Tried in order - ControlNet \u2192 T2I Adapter \u2192 Control LoRA</li> <li>IP Adapters: Searched in the IP Adapter model database</li> </ol> <p>Models that cannot be resolved are skipped with a warning in the logs.</p>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#image-file-handling","title":"Image File Handling","text":""},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#image-path-resolution","title":"Image Path Resolution","text":"<p>When you specify an <code>image_name</code>, the backend: 1. Constructs the full path: <code>{INVOKEAI_ROOT}/outputs/images/{image_name}</code> 2. Validates that the file exists 3. Includes the image reference in the event sent to the frontend 4. Logs whether the image was found or not</p>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#image-naming","title":"Image Naming","text":"<p>Images should be referenced by their filename as it appears in the outputs/images directory: - \u2705 Correct: <code>\"image_name\": \"example.png\"</code> - \u2705 Correct: <code>\"image_name\": \"my_control_image_20240110.jpg\"</code> - \u274c Incorrect: <code>\"image_name\": \"outputs/images/example.png\"</code>  (use relative filename only) - \u274c Incorrect: <code>\"image_name\": \"/full/path/to/example.png\"</code>   (use relative filename only)</p>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#frontend-behavior","title":"Frontend Behavior","text":""},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#loras","title":"LoRAs","text":"<ul> <li>Fully Supported: LoRAs are immediately added to the LoRA list in the UI</li> <li>Existing LoRAs are cleared before adding new ones</li> <li>Each LoRA's model config is fetched and applied with the specified weight</li> <li>LoRAs appear in the LoRA selector panel</li> </ul>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#control-layers-with-images","title":"Control Layers with Images","text":"<ul> <li>Fully Supported: Control layers now support images from outputs/images</li> <li>Configuration includes model, weights, step percentages, and image reference</li> <li>Image availability is logged in frontend console</li> <li>Images can be used to create actual control layers through the UI</li> </ul>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#ip-adapters-with-images","title":"IP Adapters with Images","text":"<ul> <li>Fully Supported: IP Adapters now support reference images from outputs/images</li> <li>Configuration includes model, weights, step percentages, method, and image reference</li> <li>Image availability is logged in frontend console</li> <li>Images can be used to create actual reference image layers through the UI</li> </ul>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#examples","title":"Examples","text":""},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#1-add-loras-only","title":"1. Add LoRAs Only","text":"<pre><code>curl -X POST http://localhost:9090/api/v1/recall/default \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"loras\": [\n      {\n        \"model_name\": \"add-detail-xl\",\n        \"weight\": 0.8,\n        \"is_enabled\": true\n      },\n      {\n        \"model_name\": \"sd_xl_offset_example-lora_1.0\",\n        \"weight\": 0.5,\n        \"is_enabled\": true\n      }\n    ]\n  }'\n</code></pre>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#2-configure-control-layers-with-image","title":"2. Configure Control Layers with Image","text":"<p>Replace <code>my_control_image.png</code> with an actual image filename from your outputs/images directory.</p> <pre><code>curl -X POST http://localhost:9090/api/v1/recall/default \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"control_layers\": [\n      {\n        \"model_name\": \"controlnet-canny-sdxl-1.0\",\n        \"image_name\": \"my_control_image.png\",\n        \"weight\": 0.75,\n        \"begin_step_percent\": 0.0,\n        \"end_step_percent\": 0.8,\n        \"control_mode\": \"balanced\"\n      }\n    ]\n  }'\n</code></pre>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#3-configure-ip-adapters-with-reference-image","title":"3. Configure IP Adapters with Reference Image","text":"<p>Replace <code>reference_face.png</code> with an actual image filename from your outputs/images directory.</p> <pre><code>curl -X POST http://localhost:9090/api/v1/recall/default \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"ip_adapters\": [\n      {\n        \"model_name\": \"ip-adapter-plus-face_sd15\",\n        \"image_name\": \"reference_face.png\",\n        \"weight\": 0.7,\n        \"begin_step_percent\": 0.0,\n        \"end_step_percent\": 1.0,\n        \"method\": \"composition\"\n      }\n    ]\n  }'\n</code></pre>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#4-complete-configuration-with-all-features","title":"4. Complete Configuration with All Features","text":"<pre><code>curl -X POST http://localhost:9090/api/v1/recall/default \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"positive_prompt\": \"masterpiece, detailed photo with specific style\",\n    \"negative_prompt\": \"blurry, low quality\",\n    \"model\": \"FLUX Schnell\",\n    \"steps\": 25,\n    \"cfg_scale\": 8.0,\n    \"width\": 1024,\n    \"height\": 768,\n    \"seed\": 42,\n    \"loras\": [\n      {\n        \"model_name\": \"add-detail-xl\",\n        \"weight\": 0.6,\n        \"is_enabled\": true\n      }\n    ],\n    \"control_layers\": [\n      {\n        \"model_name\": \"controlnet-depth-sdxl-1.0\",\n        \"image_name\": \"depth_map.png\",\n        \"weight\": 1.0,\n        \"begin_step_percent\": 0.0,\n        \"end_step_percent\": 0.7\n      }\n    ],\n    \"ip_adapters\": [\n      {\n        \"model_name\": \"ip-adapter-plus-face_sd15\",\n        \"image_name\": \"style_reference.png\",\n        \"weight\": 0.5,\n        \"begin_step_percent\": 0.0,\n        \"end_step_percent\": 1.0,\n        \"method\": \"style\"\n      }\n    ]\n  }'\n</code></pre>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#response-format","title":"Response Format","text":"<pre><code>{\n  \"status\": \"success\",\n  \"queue_id\": \"default\",\n  \"updated_count\": 15,\n  \"parameters\": {\n    \"positive_prompt\": \"...\",\n    \"steps\": 25,\n    \"loras\": [\n      {\n        \"model_key\": \"abc123...\",\n        \"weight\": 0.6,\n        \"is_enabled\": true\n      }\n    ],\n    \"control_layers\": [\n      {\n        \"model_key\": \"controlnet-xyz...\",\n        \"weight\": 1.0,\n        \"image\": {\n          \"image_name\": \"depth_map.png\"\n        }\n      }\n    ],\n    \"ip_adapters\": [\n      {\n        \"model_key\": \"ip-adapter-xyz...\",\n        \"weight\": 0.5,\n        \"image\": {\n          \"image_name\": \"style_reference.png\"\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#websocket-events","title":"WebSocket Events","text":"<p>When parameters are updated, a <code>recall_parameters_updated</code> event is emitted via WebSocket to the queue room. The frontend automatically:</p> <ol> <li>Applies standard parameters (prompts, steps, dimensions, etc.)</li> <li>Loads and adds LoRAs to the LoRA list</li> <li>Logs control layer and IP adapter configurations with image information</li> <li>Makes image references available for manual canvas/reference image creation</li> </ol>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#logging","title":"Logging","text":""},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#backend-logs","title":"Backend Logs","text":"<p>Backend logs show: - Model name \u2192 key resolution (success/failure) - Image file validation (found/not found) - Parameter storage confirmation - Event emission status</p> <p>Example log messages: <pre><code>INFO: Resolved ControlNet model name 'controlnet-canny-sdxl-1.0' to key 'controlnet-xyz...'\nINFO: Found image file: depth_map.png\nINFO: Updated 12 recall parameters for queue default\nINFO: Resolved 1 LoRA(s)\nINFO: Resolved 1 control layer(s)\nINFO: Resolved 1 IP adapter(s)\n</code></pre></p>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#frontend-logs","title":"Frontend Logs","text":"<p>Frontend logs (check browser console): - Set <code>localStorage.ROARR_FILTER = 'debug'</code> to see all debug messages - Look for messages from the <code>events</code> namespace - LoRA loading, model resolution, and parameter application are logged</p> <p>Example log messages: <pre><code>INFO: Applied 5 recall parameters to store\nINFO: Received 1 control layer(s) with image support\nINFO: Control layer 1: controlnet-xyz... (weight: 0.75, image: depth_map.png)\nDEBUG: Control layer 1 image available at: outputs/images/depth_map.png\nINFO: Received 1 IP adapter(s) with image support\nINFO: IP adapter 1: ip-adapter-xyz... (weight: 0.7, image: style_reference.png)\nDEBUG: IP adapter 1 image available at: outputs/images/style_reference.png\n</code></pre></p>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#limitations","title":"Limitations","text":"<ol> <li>Canvas Integration: Control layers and IP adapters with images are currently logged but not automatically added to canvas layers</li> <li>Users can view the configuration and manually create canvas layers with the provided images</li> <li> <p>Future enhancement: Auto-create canvas layers with stored images</p> </li> <li> <p>Model Availability: Models must be installed in InvokeAI before they can be recalled</p> </li> <li> <p>Image Availability: Images must exist in the outputs/images directory</p> </li> <li>Missing images are logged as warnings but don't fail the request</li> <li> <p>Other parameters are still applied even if images are missing</p> </li> <li> <p>Image URLs: Only local filenames from outputs/images are supported</p> </li> <li>Remote image URLs are not currently supported</li> </ol>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#testing","title":"Testing","text":"<p>Use the provided test script:</p> <pre><code>./test_recall_loras_controlnets.sh\n</code></pre> <p>This will test: - LoRA addition with multiple models - Control layer configuration with image references - IP adapter configuration with image references - Combined parameter updates with all features</p> <p>Note: Update the image names in the test script to match actual images in your outputs/images directory.</p>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#troubleshooting","title":"Troubleshooting","text":""},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#images-not-found","title":"Images Not Found","text":"<p>If you see \"Image file not found\" in the logs: 1. Verify the image filename matches exactly (case-sensitive) 2. Ensure the image is in <code>{INVOKEAI_ROOT}/outputs/images/</code> 3. Check that the filename doesn't include the <code>outputs/images/</code> prefix</p>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#models-not-found","title":"Models Not Found","text":"<p>If you see \"Could not find model\" messages: 1. Verify the model name matches exactly (case-sensitive) 2. Ensure the model is installed in InvokeAI 3. Check the model name using the models browser in the UI</p>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#event-not-received","title":"Event Not Received","text":"<p>If the frontend doesn't receive the event: 1. Check browser console for connection errors 2. Verify the queue_id matches the frontend's queue (usually \"default\") 3. Check backend logs for event emission errors</p>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_API_LORAS_CONTROLNETS_IMAGES/#future-enhancements","title":"Future Enhancements","text":"<p>Potential improvements: 1. Auto-create canvas layers with provided control layer images 2. Auto-create reference image layers with provided IP adapter images 3. Support for image URLs 4. Batch operations for multiple queue IDs 5. Image upload capability (accept base64 or file upload)</p>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_PARAMETERS_API/","title":"Recall Parameters API","text":""},{"location":"contributing/RECALL_PARAMETERS/RECALL_PARAMETERS_API/#overview","title":"Overview","text":"<p>A new REST API endpoint has been added to the InvokeAI backend that allows programmatic updates to recallable parameters from another process. This enables external applications or scripts to modify frontend parameters like prompts, models, and step counts via HTTP requests.</p> <p>When parameters are updated via the API, the backend automatically broadcasts a WebSocket event to all connected frontend clients subscribed to that queue, causing them to update immediately.</p>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_PARAMETERS_API/#how-it-works","title":"How It Works","text":"<ol> <li>API Request: External application sends a POST request with parameters to update</li> <li>Storage: Parameters are stored in client state persistence, associated with a queue ID</li> <li>Broadcast: A WebSocket event (<code>recall_parameters_updated</code>) is emitted to all frontend clients listening to that queue</li> <li>Frontend Update: Connected frontend clients receive the event and can process the updated parameters</li> <li>Immediate Display: The frontend UI updates automatically with the new values</li> </ol> <p>This means if you have the InvokeAI frontend open in a browser, updating parameters via the API will instantly reflect on the screen without any manual action needed.</p>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_PARAMETERS_API/#endpoint","title":"Endpoint","text":"<p>Base URL: <code>http://localhost:9090/api/v1/recall/{queue_id}</code></p>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_PARAMETERS_API/#post-update-recall-parameters","title":"POST - Update Recall Parameters","text":"<p>Updates recallable parameters for a given queue ID.</p>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_PARAMETERS_API/#request","title":"Request","text":"<pre><code>POST /api/v1/recall/{queue_id}\nContent-Type: application/json\n\n{\n  \"positive_prompt\": \"a beautiful landscape\",\n  \"negative_prompt\": \"blurry, low quality\",\n  \"model\": \"sd-1.5\",\n  \"steps\": 20,\n  \"cfg_scale\": 7.5,\n  \"width\": 512,\n  \"height\": 512,\n  \"seed\": 12345\n}\n</code></pre> <p>The queue id is usually \"default\".</p>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_PARAMETERS_API/#parameters","title":"Parameters","text":"<p>All parameters are optional. Only provide the parameters you want to update:</p> Parameter Type Description <code>positive_prompt</code> string Positive prompt text <code>negative_prompt</code> string Negative prompt text <code>model</code> string Main model name/identifier <code>refiner_model</code> string Refiner model name/identifier <code>vae_model</code> string VAE model name/identifier <code>scheduler</code> string Scheduler name <code>steps</code> integer Number of generation steps (\u22651) <code>refiner_steps</code> integer Number of refiner steps (\u22650) <code>cfg_scale</code> number CFG scale for guidance <code>cfg_rescale_multiplier</code> number CFG rescale multiplier <code>refiner_cfg_scale</code> number Refiner CFG scale <code>guidance</code> number Guidance scale <code>width</code> integer Image width in pixels (\u226564) <code>height</code> integer Image height in pixels (\u226564) <code>seed</code> integer Random seed (\u22650) <code>denoise_strength</code> number Denoising strength (0-1) <code>refiner_denoise_start</code> number Refiner denoising start (0-1) <code>clip_skip</code> integer CLIP skip layers (\u22650) <code>seamless_x</code> boolean Enable seamless X tiling <code>seamless_y</code> boolean Enable seamless Y tiling <code>refiner_positive_aesthetic_score</code> number Refiner positive aesthetic score <code>refiner_negative_aesthetic_score</code> number Refiner negative aesthetic score"},{"location":"contributing/RECALL_PARAMETERS/RECALL_PARAMETERS_API/#response","title":"Response","text":"<pre><code>{\n  \"status\": \"success\",\n  \"queue_id\": \"queue_123\",\n  \"updated_count\": 7,\n  \"parameters\": {\n    \"positive_prompt\": \"a beautiful landscape\",\n    \"negative_prompt\": \"blurry, low quality\",\n    \"model\": \"sd-1.5\",\n    \"steps\": 20,\n    \"cfg_scale\": 7.5,\n    \"width\": 512,\n    \"height\": 512,\n    \"seed\": 12345\n  }\n}\n</code></pre>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_PARAMETERS_API/#get-retrieve-recall-parameters","title":"GET - Retrieve Recall Parameters","text":"<p>Retrieves metadata about stored recall parameters.</p>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_PARAMETERS_API/#request_1","title":"Request","text":"<pre><code>GET /api/v1/recall/{queue_id}\n</code></pre>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_PARAMETERS_API/#response_1","title":"Response","text":"<pre><code>{\n  \"status\": \"success\",\n  \"queue_id\": \"queue_123\",\n  \"note\": \"Use the frontend to access stored recall parameters, or set specific parameters using POST\"\n}\n</code></pre>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_PARAMETERS_API/#usage-examples","title":"Usage Examples","text":""},{"location":"contributing/RECALL_PARAMETERS/RECALL_PARAMETERS_API/#using-curl","title":"Using cURL","text":"<pre><code># Update prompts and model\ncurl -X POST http://localhost:9090/api/v1/recall/default \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"positive_prompt\": \"a cyberpunk city at night\",\n    \"negative_prompt\": \"dark, unclear\",\n    \"model\": \"sd-1.5\",\n    \"steps\": 30\n  }'\n\n# Update just the seed\ncurl -X POST http://localhost:9090/api/v1/recall/default \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"seed\": 99999}'\n</code></pre>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_PARAMETERS_API/#using-python","title":"Using Python","text":"<pre><code>import requests\nimport json\n\n# Configuration\nAPI_URL = \"http://localhost:9090/api/v1/recall/default\"\n\n# Update multiple parameters\nparams = {\n    \"positive_prompt\": \"a serene forest\",\n    \"negative_prompt\": \"people, buildings\",\n    \"steps\": 25,\n    \"cfg_scale\": 7.0,\n    \"seed\": 42\n}\n\nresponse = requests.post(API_URL, json=params)\nresult = response.json()\n\nprint(f\"Status: {result['status']}\")\nprint(f\"Updated {result['updated_count']} parameters\")\nprint(json.dumps(result['parameters'], indent=2))\n</code></pre>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_PARAMETERS_API/#using-nodejsjavascript","title":"Using Node.js/JavaScript","text":"<pre><code>const API_URL = 'http://localhost:9090/api/v1/recall/default';\n\nconst params = {\n  positive_prompt: 'a beautiful sunset',\n  negative_prompt: 'blurry',\n  steps: 20,\n  width: 768,\n  height: 768,\n  seed: 12345\n};\n\nfetch(API_URL, {\n  method: 'POST',\n  headers: { 'Content-Type': 'application/json' },\n  body: JSON.stringify(params)\n})\n  .then(res =&gt; res.json())\n  .then(data =&gt; console.log(data));\n</code></pre>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_PARAMETERS_API/#implementation-details","title":"Implementation Details","text":"<ul> <li>Parameters are stored in the client state persistence service, using keys prefixed with <code>recall_</code></li> <li>The parameters are associated with a <code>queue_id</code>, allowing multiple concurrent sessions to maintain separate parameter sets</li> <li>Only non-null parameters are processed and stored</li> <li>The endpoint provides validation for numeric ranges (e.g., steps \u2265 1, dimensions \u2265 64)</li> <li>All parameter values are JSON-serialized for storage</li> <li>When parameter values are changed, the backend generates a web sockets event that the frontend listens to.</li> </ul>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_PARAMETERS_API/#integration-with-frontend","title":"Integration with Frontend","text":"<p>The stored parameters can be accessed by the frontend through the existing client state API or by implementing hooks that read from the recall parameter storage. This allows external applications to pre-populate generation parameters before the user initiates image generation.</p>"},{"location":"contributing/RECALL_PARAMETERS/RECALL_PARAMETERS_API/#error-handling","title":"Error Handling","text":"<ul> <li>400 Bad Request: Invalid parameters or parameter values</li> <li>500 Internal Server Error: Server-side error storing or retrieving parameters</li> </ul> <p>Errors include detailed messages explaining what went wrong.</p>"},{"location":"contributing/contribution_guides/development/","title":"Development","text":""},{"location":"contributing/contribution_guides/development/#what-do-i-need-to-know-to-help","title":"What do I need to know to help?","text":"<p>If you are looking to help with a code contribution, InvokeAI uses several different technologies under the hood: Python (Pydantic, FastAPI, diffusers) and Typescript (React, Redux Toolkit, ChakraUI, Mantine, Konva). Familiarity with StableDiffusion and image generation concepts is helpful, but not essential.</p>"},{"location":"contributing/contribution_guides/development/#get-started","title":"Get Started","text":"<p>To get started, take a look at our new contributors checklist</p> <p>Once you're setup, for more information, you can review the documentation specific to your area of interest:</p> <p>If you don't feel ready to make a code contribution yet, no problem! You can also help out in other ways, such as documentation, translation or helping support other users and triage issues as they're reported in GitHub.</p> <p>There are two paths to making a development contribution:</p> <ol> <li>Choosing an open issue to address. Open issues can be found in the Issues section of the InvokeAI repository. These are tagged by the issue type (bug, enhancement, etc.) along with the \u201cgood first issues\u201d tag denoting if they are suitable for first time contributors.<ol> <li>Additional items can be found on our roadmap. The roadmap is organized in terms of priority, and contains features of varying size and complexity. If there is an inflight item you\u2019d like to help with, reach out to the contributor assigned to the item to see how you can help.</li> </ol> </li> <li>Opening a new issue or feature to add. Please make sure you have searched through existing issues before creating new ones.</li> </ol> <p>Regardless of what you choose, please post in the  #dev-chat channel of the Discord before you start development in order to confirm that the issue or feature is aligned with the current direction of the project. We value our contributors time and effort and want to ensure that no one\u2019s time is being misspent.</p>"},{"location":"contributing/contribution_guides/development/#invokeai-architecure","title":"InvokeAI Architecure","text":""},{"location":"contributing/contribution_guides/development/#frontend-documentation","title":"Frontend Documentation","text":""},{"location":"contributing/contribution_guides/development/#node-documentation","title":"Node Documentation","text":""},{"location":"contributing/contribution_guides/development/#local-development","title":"Local Development","text":""},{"location":"contributing/contribution_guides/development/#best-practices","title":"Best Practices:","text":"<ul> <li>Keep your pull requests small. Smaller pull requests are more likely to be accepted and merged</li> <li>Comments! Commenting your code helps reviewers easily understand your contribution</li> <li>Use Python and Typescript\u2019s typing systems, and consider using an editor with LSP support to streamline development</li> <li>Make all communications public. This ensure knowledge is shared with the whole community</li> </ul>"},{"location":"contributing/contribution_guides/development/#where-can-i-go-for-help","title":"Where can I go for help?","text":"<p>If you need help, you can ask questions in the #dev-chat channel of the Discord.</p> <p>For frontend related work, @psychedelicious is the best person to reach out to.</p> <p>For backend related work, please reach out to @blessedcoolant, @lstein, @StAlKeR7779 or @psychedelicious.</p>"},{"location":"contributing/contribution_guides/development/#what-does-the-code-of-conduct-mean-for-me","title":"What does the Code of Conduct mean for me?","text":"<p>Our Code of Conduct  means that you are responsible for treating everyone on the project with respect and courtesy regardless of their identity. If you are the victim of any inappropriate behavior or comments as described in our Code of Conduct, we are here for you and will do the best to ensure that the abuser is reprimanded appropriately, per our code.</p>"},{"location":"contributing/contribution_guides/documentation/","title":"Documentation","text":"<p>Documentation is an important part of any open source project. It provides a clear and concise way to communicate how the software works, how to use it, and how to troubleshoot issues. Without proper documentation, it can be difficult for users to understand the purpose and functionality of the project.</p>"},{"location":"contributing/contribution_guides/documentation/#contributing","title":"Contributing","text":"<p>All documentation is maintained in our GitHub repository. If you come across documentation that is out of date or incorrect, please submit a pull request with the necessary changes.</p> <p>When updating or creating documentation, please keep in mind Invoke is a tool for everyone, not just those who have familiarity with generative art.</p>"},{"location":"contributing/contribution_guides/documentation/#help-questions","title":"Help &amp; Questions","text":"<p>Please ping @hipsterusername on Discord if you have any questions.</p>"},{"location":"contributing/contribution_guides/newContributorChecklist/","title":"New Contributor Guide","text":"<p>If you're a new contributor to InvokeAI or Open Source Projects, this is the guide for you.</p>"},{"location":"contributing/contribution_guides/newContributorChecklist/#new-contributor-checklist","title":"New Contributor Checklist","text":"<ul> <li> Set up your local development environment &amp; fork of InvokAI by following the steps outlined here</li> <li> Set up your local tooling with this guide. Feel free to skip this step if you already have tooling you're comfortable with.</li> <li> Familiarize yourself with Git &amp; our project structure by reading through the development documentation</li> <li> Join the #dev-chat channel of the Discord</li> <li> Choose an issue to work on! This can be achieved by asking in the #dev-chat channel, tackling a good first issue or finding an item on the roadmap. If nothing in any of those places catches your eye, feel free to work on something of interest to you!</li> <li> Make your first Pull Request with the guide below</li> <li> Happy development! Don't be afraid to ask for help - we're happy to help you contribute!</li> </ul>"},{"location":"contributing/contribution_guides/newContributorChecklist/#how-do-i-make-a-contribution","title":"How do I make a contribution?","text":"<p>Never made an open source contribution before? Wondering how contributions work in our project? Here's a quick rundown!</p> <p>Before starting these steps, ensure you have your local environment configured for development.</p> <ol> <li>Find a good first issue that you are interested in addressing or a feature that you would like to add. Then, reach out to our team in the #dev-chat channel of the Discord to ensure you are setup for success.</li> <li>Fork the InvokeAI repository to your GitHub profile. This means that you will have a copy of the repository under\u00a0your-GitHub-username/InvokeAI.</li> <li> <p>Clone the repository to your local machine using:</p> <pre><code>git clone https://github.com/your-GitHub-username/InvokeAI.git\n</code></pre> </li> </ol> <p>If you're unfamiliar with using Git through the commandline, GitHub Desktop is a easy-to-use alternative with a UI. You can do all the same steps listed here, but through the interface. 4. Create a new branch for your fix using:</p> <pre><code>git checkout -b branch-name-here\n</code></pre> <ol> <li>Make the appropriate changes for the issue you are trying to address or the feature that you want to add.</li> <li> <p>Add the file contents of the changed files to the \"snapshot\" git uses to manage the state of the project, also known as the index:</p> <pre><code>git add -A\n</code></pre> </li> <li> <p>Store the contents of the index with a descriptive message.</p> <pre><code>git commit -m \"Insert a short message of the changes made here\"\n</code></pre> </li> <li> <p>Push the changes to the remote repository using</p> <pre><code>git push origin branch-name-here\n</code></pre> </li> <li> <p>Submit a pull request to the main branch of the InvokeAI repository. If you're not sure how to, follow this guide</p> </li> <li>Title the pull request with a short description of the changes made and the issue or bug number associated with your change. For example, you can title an issue like so \"Added more log outputting to resolve #1234\".</li> <li>In the description of the pull request, explain the changes that you made, any issues you think exist with the pull request you made, and any questions you have for the maintainer. It's OK if your pull request is not perfect (no pull request is), the reviewer will be able to help you fix any problems and improve it!</li> <li>Wait for the pull request to be reviewed by other collaborators.</li> <li>Make changes to the pull request if the reviewer(s) recommend them.</li> <li>Celebrate your success after your pull request is merged!</li> </ol> <p>If you\u2019d like to learn more about contributing to Open Source projects, here is a\u00a0Getting Started Guide.</p>"},{"location":"contributing/contribution_guides/newContributorChecklist/#best-practices","title":"Best Practices","text":"<ul> <li> <p>Keep your pull requests small. Smaller pull requests are more likely to be accepted and merged</p> </li> <li> <p>Comments! Commenting your code helps reviewers easily understand your contribution</p> </li> <li>Use Python and Typescript\u2019s typing systems, and consider using an editor with LSP support to streamline development</li> <li>Make all communications public. This ensure knowledge is shared with the whole community</li> </ul>"},{"location":"contributing/contribution_guides/newContributorChecklist/#where-can-i-go-for-help","title":"Where can I go for help?","text":"<p>If you need help, you can ask questions in the #dev-chat channel of the Discord.</p> <p>For frontend related work, @pyschedelicious is the best person to reach out to.</p> <p>For backend related work, please reach out to @blessedcoolant, @lstein, @StAlKeR7779 or @pyschedelicious.</p>"},{"location":"contributing/contribution_guides/translation/","title":"Translation","text":"<p>InvokeAI uses\u00a0Weblate\u00a0for translation. Weblate is a FOSS project providing a scalable translation service. Weblate automates the tedious parts of managing translation of a growing project, and the service is generously provided at no cost to FOSS projects like InvokeAI.</p>"},{"location":"contributing/contribution_guides/translation/#contributing","title":"Contributing","text":"<p>If you'd like to contribute by adding or updating a translation, please visit our\u00a0Weblate project. You'll need to sign in with your GitHub account (a number of other accounts are supported, including Google).</p> <p>Once signed in, select a language and then the Web UI component. From here you can Browse and Translate strings from English to your chosen language. Zen mode offers a simpler translation experience.</p> <p>Your changes will be attributed to you in the automated PR process; you don't need to do anything else.</p>"},{"location":"contributing/contribution_guides/translation/#help-questions","title":"Help &amp; Questions","text":"<p>Please check Weblate's\u00a0documentation\u00a0or ping @Harvestor on Discord if you have any questions.</p>"},{"location":"contributing/contribution_guides/translation/#thanks","title":"Thanks","text":"<p>Thanks to the InvokeAI community for their efforts to translate the project!</p>"},{"location":"contributing/contribution_guides/tutorials/","title":"Tutorials","text":"<p>Tutorials help new &amp; existing users expand their ability to use InvokeAI to the full extent of our features and services.  </p> <p>Currently, we have a set of tutorials available on our YouTube channel, but as InvokeAI continues to evolve with new updates, we want to ensure that we are giving our users the resources they need to succeed. </p> <p>Tutorials can be in the form of videos or article walkthroughs on a subject of your choice. We recommend focusing tutorials on the key image generation methods, or on a specific component within one of the image generation methods.</p>"},{"location":"contributing/contribution_guides/tutorials/#contributing","title":"Contributing","text":"<p>Please reach out to @imic or @hipsterusername on Discord to help create tutorials for InvokeAI.</p>"},{"location":"contributing/frontend/","title":"Invoke UI","text":"<p>Invoke's UI is made possible by many contributors and open-source libraries. Thank you!</p>"},{"location":"contributing/frontend/#dev-environment","title":"Dev environment","text":"<p>Follow the dev environment guide to get set up. Run the UI using <code>pnpm dev</code>.</p>"},{"location":"contributing/frontend/#package-scripts","title":"Package scripts","text":"<ul> <li><code>dev</code>: run the frontend in dev mode, enabling hot reloading</li> <li><code>build</code>: run all checks (dpdm, eslint, prettier, tsc, knip) and then build the frontend</li> <li><code>lint:dpdm</code>: check circular dependencies</li> <li><code>lint:eslint</code>: check code quality</li> <li><code>lint:prettier</code>: check code formatting</li> <li><code>lint:tsc</code>: check type issues</li> <li><code>lint:knip</code>: check for unused exports or objects</li> <li><code>lint</code>: run all checks concurrently</li> <li><code>fix</code>: run <code>eslint</code> and <code>prettier</code>, fixing fixable issues</li> <li><code>test:ui</code>: run <code>vitest</code> with the fancy web UI</li> </ul>"},{"location":"contributing/frontend/#type-generation","title":"Type generation","text":"<p>We use openapi-typescript to generate types from the app's OpenAPI schema. The generated types are committed to the repo in schema.ts.</p> <p>If you make backend changes, it's important to regenerate the frontend types:</p> <pre><code>cd invokeai/frontend/web &amp;&amp; python ../../../scripts/generate_openapi_schema.py | pnpm typegen\n</code></pre> <p>On macOS and Linux, you can run <code>make frontend-typegen</code> as a shortcut for the above snippet.</p>"},{"location":"contributing/frontend/#localization","title":"Localization","text":"<p>We use i18next for localization, but translation to languages other than English happens on our Weblate project.</p> <p>Only the English source strings (i.e. <code>en.json</code>) should be changed on this repo.</p>"},{"location":"contributing/frontend/#vscode","title":"VSCode","text":""},{"location":"contributing/frontend/#example-debugger-config","title":"Example debugger config","text":"<pre><code>{\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      \"type\": \"chrome\",\n      \"request\": \"launch\",\n      \"name\": \"Invoke UI\",\n      \"url\": \"http://localhost:5173\",\n      \"webRoot\": \"${workspaceFolder}/invokeai/frontend/web\"\n    }\n  ]\n}\n</code></pre>"},{"location":"contributing/frontend/#remote-dev","title":"Remote dev","text":"<p>We've noticed an intermittent timeout issue with the VSCode remote dev port forwarding.</p> <p>We suggest disabling the editor's port forwarding feature and doing it manually via SSH:</p> <pre><code>ssh -L 9090:localhost:9090 -L 5173:localhost:5173 user@host\n</code></pre>"},{"location":"contributing/frontend/#contributing-guidelines","title":"Contributing Guidelines","text":"<p>Thanks for your interest in contributing to the Invoke Web UI!</p> <p>Please follow these guidelines when contributing.</p>"},{"location":"contributing/frontend/#check-in-before-investing-your-time","title":"Check in before investing your time","text":"<p>Please check in before you invest your time on anything besides a trivial fix, in case it conflicts with ongoing work or isn't aligned with the vision for the app.</p> <p>If a feature request or issue doesn't already exist for the thing you want to work on, please create one.</p> <p>Ping <code>@psychedelicious</code> on discord in the <code>#frontend-dev</code> channel or in the feature request / issue you want to work on - we're happy to chat.</p>"},{"location":"contributing/frontend/#code-conventions","title":"Code conventions","text":"<ul> <li>This is a fairly complex app with a deep component tree. Please use memoization (<code>useCallback</code>, <code>useMemo</code>, <code>memo</code>) with enthusiasm.</li> <li>If you need to add some global, ephemeral state, please use [nanostores] if possible.</li> <li>Be careful with your redux selectors. If they need to be parameterized, consider creating them inside a <code>useMemo</code>.</li> <li>Feel free to use <code>lodash</code> (via <code>lodash-es</code>) to make the intent of your code clear.</li> <li>Please add comments describing the \"why\", not the \"how\" (unless it is really arcane).</li> </ul>"},{"location":"contributing/frontend/#commit-format","title":"Commit format","text":"<p>Please use the conventional commits spec for the web UI, with a scope of \"ui\":</p> <ul> <li><code>chore(ui): bump deps</code></li> <li><code>chore(ui): lint</code></li> <li><code>feat(ui): add some cool new feature</code></li> <li><code>fix(ui): fix some bug</code></li> </ul>"},{"location":"contributing/frontend/#tests","title":"Tests","text":"<p>We don't do any UI testing at this time, but consider adding tests for sensitive logic.</p> <p>We use <code>vitest</code>, and tests should be next to the file they are testing. If the logic is in <code>something.ts</code>, the tests should be in <code>something.test.ts</code>.</p> <p>In some situations, we may want to test types. For example, if you use <code>zod</code> to create a schema that should match a generated type, it's best to add a test to confirm that the types match. Use <code>tsafe</code>'s assert for this.</p>"},{"location":"contributing/frontend/#submitting-a-pr","title":"Submitting a PR","text":"<ul> <li>Ensure your branch is tidy. Use an interactive rebase to clean up the commit history and reword the commit messages if they are not descriptive.</li> <li>Run <code>pnpm lint</code>. Some issues are auto-fixable with <code>pnpm fix</code>.</li> <li>Fill out the PR form when creating the PR.</li> <li>It doesn't need to be super detailed, but a screenshot or video is nice if you changed something visually.</li> <li>If a section isn't relevant, delete it.</li> </ul>"},{"location":"contributing/frontend/#other-docs","title":"Other docs","text":"<ul> <li>Workflows - Design and Implementation</li> <li>State Management</li> </ul>"},{"location":"contributing/frontend/canvas-text-tool/","title":"Canvas Text Tool","text":""},{"location":"contributing/frontend/canvas-text-tool/#overview","title":"Overview","text":"<p>The canvas text workflow is split between a Konva module that owns tool state and a React overlay that handles text entry.</p> <ul> <li><code>invokeai/frontend/web/src/features/controlLayers/konva/CanvasTool/CanvasTextToolModule.ts</code></li> <li>Owns the tool, cursor preview, and text session state (including the cursor \"T\" marker).</li> <li>Manages dynamic cursor contrast, starts sessions on pointer down, and commits sessions by rasterizing the active text block into a new raster layer.</li> <li><code>invokeai/frontend/web/src/features/controlLayers/components/Text/CanvasTextOverlay.tsx</code></li> <li>Renders the on-canvas editor as a <code>contentEditable</code> overlay positioned in canvas space.</li> <li>Syncs keyboard input, suppresses app hotkeys, and forwards commits/cancels to the Konva module.</li> <li><code>invokeai/frontend/web/src/features/controlLayers/components/Text/TextToolOptions.tsx</code></li> <li>Provides the font dropdown, size slider/input, formatting toggles, and alignment buttons that appear when the Text tool is active.</li> </ul>"},{"location":"contributing/frontend/canvas-text-tool/#rasterization-pipeline","title":"Rasterization pipeline","text":"<p><code>renderTextToCanvas()</code> (<code>invokeai/frontend/web/src/features/controlLayers/text/textRenderer.ts</code>) converts the editor contents into a transparent canvas. The Text tool module configures the renderer with the active font stack, weight, styling flags, alignment, and the active canvas color. The resulting canvas is encoded to a PNG data URL and stored in a new raster layer (<code>image</code> object) with a transparent background.</p> <p>Layer placement preserves the original click location:</p> <ul> <li>The session stores the anchor coordinate (where the user clicked) and current alignment.</li> <li><code>calculateLayerPosition()</code> calculates the top-left position for the raster layer after applying the configured padding and alignment offsets.</li> <li>New layers are inserted directly above the currently-selected raster layer (when present) and selected automatically.</li> </ul>"},{"location":"contributing/frontend/canvas-text-tool/#font-stacks","title":"Font stacks","text":"<p>Font definitions live in <code>invokeai/frontend/web/src/features/controlLayers/text/textConstants.ts</code> as ten deterministic stacks (sans, serif, mono, rounded, script, humanist, slab serif, display, narrow, UI serif). Each stack lists system-safe fallbacks so the editor can choose the first available font per platform.</p> <p>To add or adjust fonts:</p> <ol> <li>Update <code>TEXT_FONT_STACKS</code> with the new <code>id</code>, <code>label</code>, and CSS <code>font-family</code> stack.</li> <li>If you add a new stack, extend the <code>TEXT_FONT_IDS</code> tuple and update the <code>canvasTextSlice</code> schema default (<code>TEXT_DEFAULT_FONT_ID</code>).</li> <li>Provide translation strings for any new labels in <code>public/locales/*</code>.</li> <li>The editor and renderer will automatically pick up the new stack via <code>getFontStackById()</code>.</li> </ol>"},{"location":"contributing/frontend/state-management/","title":"State Management","text":"<p>The app makes heavy use of Redux Toolkit, its Query library, and <code>nanostores</code>.</p>"},{"location":"contributing/frontend/state-management/#redux","title":"Redux","text":"<p>We use RTK extensively - slices, entity adapters, queries, reselect, the whole 9 yards. Their docs are excellent.</p>"},{"location":"contributing/frontend/state-management/#nanostores","title":"<code>nanostores</code>","text":"<p>nanostores is a tiny state management library. It provides both imperative and declarative APIs.</p>"},{"location":"contributing/frontend/state-management/#example","title":"Example","text":"<pre><code>export const $myStringOption = atom&lt;string | null&gt;(null);\n\n// Outside a component, or within a callback for performance-critical logic\n$myStringOption.get();\n$myStringOption.set('new value');\n\n// Inside a component\nconst myStringOption = useStore($myStringOption);\n</code></pre>"},{"location":"contributing/frontend/state-management/#where-to-put-nanostores","title":"Where to put nanostores","text":"<ul> <li>For global application state, export your stores from <code>invokeai/frontend/web/src/app/store/nanostores/</code>.</li> <li>For feature state, create a file for the stores next to the redux slice definition (e.g. <code>invokeai/frontend/web/src/features/myFeature/myFeatureNanostores.ts</code>).</li> <li>For hooks with global state, export the store from the same file the hook is in, or put it next to the hook.</li> </ul>"},{"location":"contributing/frontend/state-management/#when-to-use-nanostores","title":"When to use nanostores","text":"<ul> <li>For non-serializable data that needs to be available throughout the app, use <code>nanostores</code> instead of a global.</li> <li>For ephemeral global state (i.e. state that does not need to be persisted), use <code>nanostores</code> instead of redux.</li> <li>For performance-critical code and in callbacks, redux selectors can be problematic due to the declarative reactivity system. Consider refactoring to use <code>nanostores</code> if there's a measurable performance issue.</li> </ul>"},{"location":"contributing/frontend/workflows/","title":"Workflows - Design and Implementation","text":"<p>This document describes, at a high level, the design and implementation of workflows in the InvokeAI frontend. There are a substantial number of implementation details not included, but which are hopefully clear from the code.</p> <p>InvokeAI's backend uses graphs, composed of nodes and edges, to process data and generate images.</p> <p>Nodes have any number of input fields and output fields. Edges connect nodes together via their inputs and outputs. Fields have data types which dictate how they may be connected.</p> <p>During execution, a nodes' outputs may be passed along to any number of other nodes' inputs.</p> <p>Workflows are an enriched abstraction over a graph.</p>"},{"location":"contributing/frontend/workflows/#design","title":"Design","text":"<p>InvokeAI provide two ways to build graphs in the frontend: the Linear UI and Workflow Editor.</p> <p>To better understand the use case and challenges related to workflows, we will review both of these modes.</p>"},{"location":"contributing/frontend/workflows/#linear-ui","title":"Linear UI","text":"<p>This includes the Text to Image, Image to Image and Unified Canvas tabs.</p> <p>The user-managed parameters on these tabs are stored as simple objects in the application state. When the user invokes, adding a generation to the queue, we internally build a graph from these parameters.</p> <p>This logic can be fairly complex due to the range of features available and their interactions. Depending on the parameters selected, the graph may be very different. Building graphs in code can be challenging - you are trying to construct a non-linear structure in a linear context.</p> <p>The simplest graph building logic is for Text to Image with a SD1.5 model: buildLinearTextToImageGraph.ts</p> <p>There are many other graph builders in the same directory for different tabs or base models (e.g. SDXL). Some are pretty hairy.</p> <p>In the Linear UI, we go straight from simple application state to graph via these builders.</p>"},{"location":"contributing/frontend/workflows/#workflow-editor","title":"Workflow Editor","text":"<p>The Workflow Editor is a visual graph editor, allowing users to draw edges from node to node to construct a graph. This far more approachable way to create complex graphs.</p> <p>InvokeAI uses the reactflow library to power the Workflow Editor. It provides both a graph editor UI and manages its own internal graph state.</p>"},{"location":"contributing/frontend/workflows/#workflows","title":"Workflows","text":"<p>A workflow is a representation of a graph plus additional metadata:</p> <ul> <li>Name</li> <li>Description</li> <li>Version</li> <li>Notes</li> <li>Exposed fields</li> <li>Author, tags, category, etc.</li> </ul> <p>Workflows should have other qualities:</p> <ul> <li>Portable: you should be able to load a workflow created by another person.</li> <li>Resilient: you should be able to \"upgrade\" a workflow as the application changes.</li> <li>Abstract: as much as is possible, workflows should not be married to the specific implementation details of the application.</li> </ul> <p>To support these qualities, workflows are serializable, have a versioned schemas, and represent graphs as minimally as possible. Fortunately, the reactflow state for nodes and edges works perfectly for this.</p>"},{"location":"contributing/frontend/workflows/#workflow-reactflow-state-invokeai-graph","title":"Workflow -&gt; reactflow state -&gt; InvokeAI graph","text":"<p>Given a workflow, we need to be able to derive reactflow state and/or an InvokeAI graph from it.</p> <p>The first step - workflow to reactflow state - is very simple. The logic is in nodesSlice.ts, in the <code>workflowLoaded</code> reducer.</p> <p>The reactflow state is, however, structurally incompatible with our backend's graph structure. When a user invokes on a Workflow, we need to convert the reactflow state into an InvokeAI graph. This is far simpler than the graph building logic from the Linear UI: buildNodesGraph.ts</p>"},{"location":"contributing/frontend/workflows/#nodes-vs-invocations","title":"Nodes vs Invocations","text":"<p>We often use the terms \"node\" and \"invocation\" interchangeably, but they may refer to different things in the frontend.</p> <p>reactflow has its own definitions of \"node\", \"edge\" and \"handle\" which are closely related to InvokeAI graph concepts.</p> <ul> <li>A reactflow node is related to an InvokeAI invocation. It has a \"data\" property, which holds the InvokeAI-specific invocation data.</li> <li>A reactflow edge is roughly equivalent to an InvokeAI edge.</li> <li>A reactflow handle is roughly equivalent to an InvokeAI input or output field.</li> </ul>"},{"location":"contributing/frontend/workflows/#workflow-linear-view","title":"Workflow Linear View","text":"<p>Graphs are very capable data structures, but not everyone wants to work with them all the time.</p> <p>To allow less technical users - or anyone who wants a less visually noisy workspace - to benefit from the power of nodes, InvokeAI has a workflow feature called the Linear View.</p> <p>A workflow input field can be added to this Linear View, and its input component can be presented similarly to the Linear UI tabs. Internally, we add the field to the workflow's list of exposed fields.</p>"},{"location":"contributing/frontend/workflows/#openapi-schema","title":"OpenAPI Schema","text":"<p>OpenAPI is a schema specification that can represent complex data structures and relationships. The backend is capable of generating an OpenAPI schema for all invocations.</p> <p>When the UI connects, it requests this schema and parses each invocation into an invocation template. Invocation templates have a number of properties, like title, description and type, but the most important ones are their input and output field templates.</p> <p>Invocation and field templates are the \"source of truth\" for graphs, because they indicate what the backend is able to process.</p> <p>When a user adds a new node to their workflow, these templates are used to instantiate a node with fields instantiated from the input and output field templates.</p>"},{"location":"contributing/frontend/workflows/#field-instances-and-templates","title":"Field Instances and Templates","text":"<p>Field templates consist of:</p> <ul> <li>Name: the identifier of the field, its variable name in python</li> <li>Type: derived from the field's type annotation in python (e.g. IntegerField, ImageField, MainModelField)</li> <li>Constraints: derived from the field's creation args in python (e.g. minimum value for an integer)</li> <li>Default value: optionally provided in the field's creation args (e.g. 42 for an integer)</li> </ul> <p>Field instances are created from the templates and have name, type and optionally a value.</p> <p>The type of the field determines the UI components that are rendered for it.</p> <p>A field instance's name associates it with its template.</p>"},{"location":"contributing/frontend/workflows/#stateful-vs-stateless-fields","title":"Stateful vs Stateless Fields","text":"<p>Stateful fields store their value in the frontend graph. Think primitives, model identifiers, images, etc. Fields are only stateful if the frontend allows the user to directly input a value for them.</p> <p>Many field types, however, are stateless. An example is a <code>UNetField</code>, which contains some data describing a UNet. Users cannot directly provide this data - it is created and consumed in the backend.</p> <p>Stateless fields do not store their value in the node, so their field instances do not have values.</p> <p>\"Custom\" fields will always be treated as stateless fields.</p>"},{"location":"contributing/frontend/workflows/#single-and-collection-fields","title":"Single and Collection Fields","text":"<p>Field types have a name and cardinality property which may identify it as a SINGLE, COLLECTION or SINGLE_OR_COLLECTION field.</p> <ul> <li>If a field is annotated in python as a singular value or class, its field type is parsed as a SINGLE type (e.g. <code>int</code>, <code>ImageField</code>, <code>str</code>).</li> <li>If a field is annotated in python as a list, its field type is parsed as a COLLECTION type (e.g. <code>list[int]</code>).</li> <li>If it is annotated as a union of a type and list, the type will be parsed as a SINGLE_OR_COLLECTION type (e.g. <code>Union[int, list[int]]</code>). Fields may not be unions of different types (e.g. <code>Union[int, list[str]]</code> and <code>Union[int, str]</code> are not allowed).</li> </ul>"},{"location":"contributing/frontend/workflows/#implementation","title":"Implementation","text":"<p>The majority of data structures in the backend are pydantic models. Pydantic provides OpenAPI schemas for all models and we then generate TypeScript types from those.</p> <p>The OpenAPI schema is parsed at runtime into our invocation templates.</p> <p>Workflows and all related data are modeled in the frontend using zod. Related types are inferred from the zod schemas.</p> <p>In python, invocations are pydantic models with fields. These fields become node inputs. The invocation's <code>invoke()</code> function returns a pydantic model - its output. Like the invocation itself, the output model has any number of fields, which become node outputs.</p>"},{"location":"contributing/frontend/workflows/#zod-schemas-and-types","title":"zod Schemas and Types","text":"<p>The zod schemas, inferred types, and type guards are in [types/].</p> <p>Roughly order from lowest-level to highest:</p> <ul> <li><code>common.ts</code>: stateful field data, and couple other misc types</li> <li><code>field.ts</code>: fields - types, values, instances, templates</li> <li><code>invocation.ts</code>: invocations and other node types</li> <li><code>workflow.ts</code>: workflows and constituents</li> </ul> <p>We customize the OpenAPI schema to include additional properties on invocation and field schemas. To facilitate parsing this schema into templates, we modify/wrap the types from openapi-types in <code>openapi.ts</code>.</p>"},{"location":"contributing/frontend/workflows/#openapi-schema-parsing","title":"OpenAPI Schema Parsing","text":"<p>The entrypoint for OpenAPI schema parsing is parseSchema.ts.</p> <p>General logic flow:</p> <ul> <li>Iterate over all invocation schema objects</li> <li>Extract relevant invocation-level attributes (e.g. title, type, version, etc)</li> <li>Iterate over the invocation's input fields<ul> <li>Parse each field's type</li> <li>Build a field input template from the type - either a stateful template or \"generic\" stateless template</li> </ul> </li> <li>Iterate over the invocation's output fields<ul> <li>Parse the field's type (same as inputs)</li> <li>Build a field output template</li> </ul> </li> <li>Assemble the attributes and fields into an invocation template</li> </ul> <p>Most of these involve very straightforward <code>reduce</code>s, but the less intuitive steps are detailed below.</p>"},{"location":"contributing/frontend/workflows/#parsing-field-types","title":"Parsing Field Types","text":"<p>Field types are represented as structured objects:</p> <pre><code>type FieldType = {\n  name: string;\n  cardinality: 'SINGLE' | 'COLLECTION' | 'SINGLE_OR_COLLECTION';\n};\n</code></pre> <p>The parsing logic is in <code>parseFieldType.ts</code>.</p> <p>There are 4 general cases for field type parsing.</p>"},{"location":"contributing/frontend/workflows/#primitive-types","title":"Primitive Types","text":"<p>When a field is annotated as a primitive values (e.g. <code>int</code>, <code>str</code>, <code>float</code>), the field type parsing is fairly straightforward. The field is represented by a simple OpenAPI schema object, which has a <code>type</code> property.</p> <p>We create a field type name from this <code>type</code> string (e.g. <code>string</code> -&gt; <code>StringField</code>). The cardinality is <code>\"SINGLE\"</code>.</p>"},{"location":"contributing/frontend/workflows/#complex-types","title":"Complex Types","text":"<p>When a field is annotated as a pydantic model (e.g. <code>ImageField</code>, <code>MainModelField</code>, <code>ControlField</code>), it is represented as a reference object. Reference objects are pointers to another schema or reference object within the schema.</p> <p>We need to dereference the schema to pull these out. Dereferencing may require recursion. We use the reference object's name directly for the field type name.</p> <p>Unfortunately, at this time, we've had limited success using external libraries to deference at runtime, so we do this ourselves.</p>"},{"location":"contributing/frontend/workflows/#collection-types","title":"Collection Types","text":"<p>When a field is annotated as a list of a single type, the schema object has an <code>items</code> property. They may be a schema object or reference object and must be parsed to determine the item type.</p> <p>We use the item type for field type name. The cardinality is <code>\"COLLECTION\"</code>.</p>"},{"location":"contributing/frontend/workflows/#single-or-collection-types","title":"Single or Collection Types","text":"<p>When a field is annotated as a union of a type and list of that type, the schema object has an <code>anyOf</code> property, which holds a list of valid types for the union.</p> <p>After verifying that the union has two members (a type and list of the same type), we use the type for field type name, with cardinality <code>\"SINGLE_OR_COLLECTION\"</code>.</p>"},{"location":"contributing/frontend/workflows/#optional-fields","title":"Optional Fields","text":"<p>In OpenAPI v3.1, when an object is optional, it is put into an <code>anyOf</code> along with a primitive schema object with <code>type: 'null'</code>.</p> <p>Handling this adds a fair bit of complexity, as we now must filter out the <code>'null'</code> types and work with the remaining types as described above.</p> <p>If there is a single remaining schema object, we must recursively call to <code>parseFieldType()</code> to get parse it.</p>"},{"location":"contributing/frontend/workflows/#building-field-input-templates","title":"Building Field Input Templates","text":"<p>Now that we have a field type, we can build an input template for the field.</p> <p>Stateful fields all get a function to build their template, while stateless fields are constructed directly. This is possible because stateless fields have no default value or constraints.</p> <p>See buildFieldInputTemplate.ts.</p>"},{"location":"contributing/frontend/workflows/#building-field-output-templates","title":"Building Field Output Templates","text":"<p>Field outputs are similar to stateless fields - they do not have any value in the frontend. When building their templates, we don't need a special function for each field type.</p> <p>See buildFieldOutputTemplate.ts.</p>"},{"location":"contributing/frontend/workflows/#managing-reactflow-state","title":"Managing reactflow State","text":"<p>As described above, the workflow editor state is the essentially the reactflow state, plus some extra metadata.</p> <p>We provide reactflow with an array of nodes and edges via redux, and a number of event handlers. These handlers dispatch redux actions, managing nodes and edges.</p> <p>The pieces of redux state relevant to workflows are:</p> <ul> <li><code>state.nodes.nodes</code>: the reactflow nodes state</li> <li><code>state.nodes.edges</code>: the reactflow edges state</li> <li><code>state.nodes.workflow</code>: the workflow metadata</li> </ul>"},{"location":"contributing/frontend/workflows/#building-nodes-and-edges","title":"Building Nodes and Edges","text":"<p>A reactflow node has a few important top-level properties:</p> <ul> <li><code>id</code>: unique identifier</li> <li><code>type</code>: a string that maps to a react component to render the node</li> <li><code>position</code>: XY coordinates</li> <li><code>data</code>: arbitrary data</li> </ul> <p>When the user adds a node, we build invocation node data, storing it in <code>data</code>. Invocation properties (e.g. type, version, label, etc.) are copied from the invocation template. Inputs and outputs are built from the invocation template's field templates.</p> <p>See buildInvocationNode.ts.</p> <p>Edges are managed by reactflow, but briefly, they consist of:</p> <ul> <li><code>source</code>: id of the source node</li> <li><code>sourceHandle</code>: id of the source node handle (output field)</li> <li><code>target</code>: id of the target node</li> <li><code>targetHandle</code>: id of the target node handle (input field)</li> </ul> <p>Edge creation is gated behind validation logic. This validation compares the input and output field types and overall graph state.</p>"},{"location":"contributing/frontend/workflows/#building-a-workflow","title":"Building a Workflow","text":"<p>Building a workflow entity is as simple as dropping the nodes, edges and metadata into an object.</p> <p>Each node and edge is parsed with a zod schema, which serves to strip out any unneeded data.</p> <p>See buildWorkflow.ts.</p>"},{"location":"contributing/frontend/workflows/#loading-a-workflow","title":"Loading a Workflow","text":"<p>Workflows may be loaded from external sources or the user's local instance. In all cases, the workflow needs to be handled with care, as an untrusted object.</p> <p>Loading has a few stages which may throw or warn if there are problems:</p> <ul> <li>Parsing the workflow data structure itself, migrating it if necessary (throws)</li> <li>Check for a template for each node (warns)</li> <li>Check each node's version against its template (warns)</li> <li>Validate the source and target of each edge (warns)</li> </ul> <p>This validation occurs in validateWorkflow.ts.</p> <p>If there are no fatal errors, the workflow is then stored in redux state.</p>"},{"location":"contributing/frontend/workflows/#workflow-migrations","title":"Workflow Migrations","text":"<p>When the workflow schema changes, we may need to perform some data migrations. This occurs as workflows are loaded. zod schemas for each workflow schema version is retained to facilitate migrations.</p> <p>Previous schemas are in folders in <code>invokeai/frontend/web/src/features/nodes/types/</code>, eg <code>v1/</code>.</p> <p>Migration logic is in migrations.ts.</p>"},{"location":"features/Text_tool/","title":"Text Tool","text":""},{"location":"features/Text_tool/#font-selection","title":"Font selection","text":"<p>The Text tool uses a set of predefined font stacks. When you choose a font, the app resolves the first available font on your system from that stack and uses it for both the editor overlay and the rasterized result. This provides consistent styling across platforms while still falling back to safe system fonts if a preferred font is missing.</p>"},{"location":"features/Text_tool/#size-and-spacing","title":"Size and spacing","text":"<ul> <li>Size controls the font size in pixels.</li> <li>Spacing controls the line height multiplier (Dense, Normal, Spacious). This affects the distance between lines while editing the text.</li> </ul>"},{"location":"features/Text_tool/#uncommitted-state","title":"Uncommitted state","text":"<p>While text is uncommitted, it remains editable on-canvas. Access to other tools is blocked. Switching to other tabs (Generate, Upascaling, Workflows etc.) discards the text. The uncommitted box can be moved and rotated:</p> <ul> <li>Move: Hold Ctrl (Windows/Linux) or Command (macOS) and drag to move the text box.</li> <li>Rotate: Drag the rotation handle above the box. Hold Shift while rotating to snap to 15 degree increments.</li> </ul> <p>The text is committed to a raster layer when you press Enter. Press Esc to discard the current text session.</p>"},{"location":"features/database/","title":"Database","text":"<p>Invoke uses a SQLite database to store image, workflow, model, and execution data.</p> <p>We take great care to ensure your data is safe, by utilizing transactions and a database migration system.</p> <p>Even so, when testing a prerelease version of the app, we strongly suggest either backing up your database or using an in-memory database. This ensures any prelease hiccups or databases schema changes will not cause problems for your data.</p>"},{"location":"features/database/#database-backup","title":"Database Backup","text":"<p>Backing up your database is very simple. Invoke's data is stored in an <code>$INVOKEAI_ROOT</code> directory - where your <code>invoke.sh</code>/<code>invoke.bat</code> and <code>invokeai.yaml</code> files live.</p> <p>To back up your database, copy the <code>invokeai.db</code> file from <code>$INVOKEAI_ROOT/databases/invokeai.db</code> to somewhere safe.</p> <p>If anything comes up during prelease testing, you can simply copy your backup back into <code>$INVOKEAI_ROOT/databases/</code>.</p>"},{"location":"features/database/#in-memory-database","title":"In-Memory Database","text":"<p>SQLite can run on an in-memory database. Your existing database is untouched when this mode is enabled, but your existing data won't be accessible.</p> <p>This is very useful for testing, as there is no chance of a database change modifying your \"physical\" database.</p> <p>To run Invoke with a memory database, edit your <code>invokeai.yaml</code> file and add <code>use_memory_db: true</code>:</p> <pre><code>use_memory_db: true\n</code></pre> <p>Delete this line (or set it to <code>false</code>) to use your main database.</p>"},{"location":"features/gallery/","title":"InvokeAI Gallery Panel","text":""},{"location":"features/gallery/#quick-guided-walkthrough-of-the-gallery-panels-features","title":"Quick guided walkthrough of the Gallery Panel's features","text":"<p>The Gallery Panel is a fast way to review, find, and make use of images you've generated and loaded. The Gallery is divided into Boards. The Uncategorized board is always  present but you can create your own for better organization.</p> <p></p>"},{"location":"features/gallery/#board-display-and-settings","title":"Board Display and Settings","text":"<p>At the very top of the Gallery Panel are the boards disclosure and settings buttons.</p> <p></p> <p>The disclosure button shows the name of the currently selected board and allows you to show and hide the board thumbnails (shown in the image below).</p> <p></p> <p>The settings button opens a list of options.</p> <p></p> <ul> <li>Image Size this slider lets you control the size of the image previews (images of three different sizes).</li> <li>Auto-Switch to New Images if you turn this on, whenever a new image is generated, it will automatically be loaded into the current image panel on the Text to Image tab and into the result panel on the Image to Image tab. This will happen invisibly if you are on any other tab when the image is generated.</li> <li>Auto-Assign Board on Click whenever an image is generated or saved, it always gets put in a board. The board it gets put into is marked with AUTO (image of board marked). Turning on Auto-Assign Board on Click will make whichever board you last selected be the destination when you click Invoke. That means you can click Invoke, select a different board, and then click Invoke again and the two images will be put in two different boards. (bold)It's the board selected when Invoke is clicked that's used, not the board that's selected when the image is finished generating.(bold) Turning this off, enables the Auto-Add Board drop down which lets you set one specific board to always put generated images into. This also enables and disables the Auto-add to this Board menu item described below.</li> <li>Always Show Image Size Badge this toggles whether to show image sizes for each image preview (show two images, one with sizes shown, one without)</li> </ul> <p>Below these two buttons, you'll see the Search Boards text entry area. You use this to search for specific boards by the name of the board. Next to it is the Add Board (+) button which lets you add new boards. Boards can be renamed by clicking on the name of the board under its thumbnail and typing in the new name.</p>"},{"location":"features/gallery/#board-thumbnail-menu","title":"Board Thumbnail Menu","text":"<p>Each board has a context menu (ctrl+click / right-click).</p> <p></p> <ul> <li>Auto-add to this Board if you've disabled Auto-Assign Board on Click in the board settings, you can use this option to set this board to be where new images are put.</li> <li>Download Board this will add all the images in the board into a zip file and provide a link to it in a notification (image of notification)</li> <li>Delete Board this will delete the board <p>[!CAUTION] This will delete all the images in the board and the board itself.</p> </li> </ul>"},{"location":"features/gallery/#board-contents","title":"Board Contents","text":"<p>Every board is organized by two tabs, Images and Assets.</p> <p></p> <p>Images are the Invoke-generated images that are placed into the board. Assets are images that you upload into Invoke to be used as an Image Prompt or in the Image to Image tab.</p>"},{"location":"features/gallery/#image-thumbnail-menu","title":"Image Thumbnail Menu","text":"<p>Every image generated by Invoke has its generation information stored as text inside the image file itself. This can be read directly by selecting the image and clicking on the Info button  in any of the image result panels. </p> <p>Each image also has a context menu (ctrl+click / right-click).</p> <p></p> <p>The options are (items marked with an * will not work with images that lack generation information): - Open in New Tab this will open the image alone in a new browser tab, separate from the Invoke interface. - Download Image this will trigger your browser to download the image. - Load Workflow **** this will load any workflow settings into the Workflow tab and automatically open it. - ***Remix Image **** this will load all of the image's generation information, (bold)excluding its Seed, into the left hand control panel - ***Use Prompt **** this will load only the image's text prompts into the left-hand control panel - ***Use Seed **** this will load only the image's Seed into the left-hand control panel - ***Use All **** this will load all of the image's generation information into the left-hand control panel - ***Send to Image to Image this will put the image into the left-hand panel in the Image to Image tab and automatically open it - Send to Unified Canvas This will (bold)replace whatever is already present(bold) in the Unified Canvas tab with the image and automatically open the tab - Change Board this will oipen a small window that will let you move the image to a different board. This is the same as dragging the image to that board's thumbnail. - Star Image this will add the image to the board's list of starred images that are always kept at the top of the gallery. This is the same as clicking on the star on the top right-hand side of the image that appears when you hover over the image with the mouse - Delete Image this will delete the image from the board</p> <p>[!CAUTION]  This will delete the image entirely from Invoke.</p>"},{"location":"features/gallery/#summary","title":"Summary","text":"<p>This walkthrough only covers the Gallery interface and Boards. Actually generating images is handled by Prompts, the Image to Image tab, and the Unified Canvas.</p>"},{"location":"features/gallery/#acknowledgements","title":"Acknowledgements","text":"<p>A huge shout-out to the core team working to make the Web GUI a reality, including psychedelicious, Kyle0654 and blessedcoolant. hipsterusername was the team's unofficial cheerleader and added tooltips/docs.</p>"},{"location":"features/hotkeys/","title":"Customizable Hotkeys","text":"<p>InvokeAI allows you to customize all keyboard shortcuts (hotkeys) to match your workflow preferences.</p>"},{"location":"features/hotkeys/#features","title":"Features","text":"<ul> <li>View All Hotkeys: See all available keyboard shortcuts in one place</li> <li>Customize Any Hotkey: Change any shortcut to your preference</li> <li>Multiple Bindings: Assign multiple key combinations to the same action</li> <li>Smart Validation: Built-in validation prevents invalid combinations</li> <li>Persistent Settings: Your custom hotkeys are saved and restored across sessions</li> <li>Easy Reset: Reset individual hotkeys or all hotkeys back to defaults</li> </ul>"},{"location":"features/hotkeys/#how-to-use","title":"How to Use","text":""},{"location":"features/hotkeys/#opening-the-hotkeys-modal","title":"Opening the Hotkeys Modal","text":"<p>Press <code>Shift+?</code> or click the keyboard icon in the application to open the Hotkeys Modal.</p>"},{"location":"features/hotkeys/#viewing-hotkeys","title":"Viewing Hotkeys","text":"<p>In View Mode (default), you can: - Browse all available hotkeys organized by category (App, Canvas, Gallery, Workflows, etc.) - Search for specific hotkeys using the search bar - See the current key combination for each action</p>"},{"location":"features/hotkeys/#customizing-hotkeys","title":"Customizing Hotkeys","text":"<ol> <li>Click the Edit Mode button at the bottom of the Hotkeys Modal</li> <li>Find the hotkey you want to change</li> <li>Click the pencil icon next to it</li> <li>The editor will appear with:</li> <li>Input field: Enter your new hotkey combination</li> <li>Modifier buttons: Quick-insert Mod, Ctrl, Shift, Alt keys</li> <li>Help icon (?): Shows syntax examples and valid keys</li> <li> <p>Live preview: See how your hotkey will look</p> </li> <li> <p>Enter your new hotkey using the format:</p> </li> <li><code>mod+a</code> - Mod key + A (Mod = Ctrl on Windows/Linux, Cmd on Mac)</li> <li><code>ctrl+shift+k</code> - Multiple modifiers</li> <li><code>f1</code> - Function keys</li> <li> <p><code>mod+enter, ctrl+enter</code> - Multiple alternatives (separated by comma)</p> </li> <li> <p>Click the checkmark or press Enter to save</p> </li> <li>Click the X or press Escape to cancel</li> </ol>"},{"location":"features/hotkeys/#resetting-hotkeys","title":"Resetting Hotkeys","text":"<p>Reset a single hotkey: - Click the counter-clockwise arrow icon that appears next to customized hotkeys</p> <p>Reset all hotkeys: - In Edit Mode, click the Reset All to Default button at the bottom</p>"},{"location":"features/hotkeys/#hotkey-format-reference","title":"Hotkey Format Reference","text":"<p>Valid Modifiers: - <code>mod</code> - Context-aware: Ctrl (Windows/Linux) or Cmd (Mac) - <code>ctrl</code> - Control key - <code>shift</code> - Shift key - <code>alt</code> - Alt key (Option on Mac)</p> <p>Valid Keys: - Letters: <code>a-z</code> - Numbers: <code>0-9</code> - Function keys: <code>f1-f12</code> - Special keys: <code>enter</code>, <code>space</code>, <code>tab</code>, <code>backspace</code>, <code>delete</code>, <code>escape</code> - Arrow keys: <code>up</code>, <code>down</code>, <code>left</code>, <code>right</code> - And more...</p> <p>Examples: - \u2705 <code>mod+s</code> - Save action - \u2705 <code>ctrl+shift+p</code> - Command palette - \u2705 <code>f5, mod+r</code> - Two alternatives for refresh - \u274c <code>mod+</code> - Invalid (no key after modifier) - \u274c <code>shift+ctrl+</code> - Invalid (ends with modifier)</p>"},{"location":"features/hotkeys/#for-developers","title":"For Developers","text":"<p>For technical implementation details, architecture, and how to add new hotkeys to the system, see the Hotkeys Developer Documentation.</p>"},{"location":"features/low-vram/","title":"Low-VRAM mode","text":"<p>As of v5.6.0, Invoke has a low-VRAM mode. It works on systems with dedicated GPUs (Nvidia GPUs on Windows/Linux and AMD GPUs on Linux).</p> <p>This allows you to generate even if your GPU doesn't have enough VRAM to hold full models. Most users should be able to run even the beefiest models - like the ~24GB unquantised FLUX dev model.</p>"},{"location":"features/low-vram/#enabling-low-vram-mode","title":"Enabling Low-VRAM mode","text":"<p>To enable Low-VRAM mode, add this line to your <code>invokeai.yaml</code> configuration file, then restart Invoke:</p> <pre><code>enable_partial_loading: true\n</code></pre> <p>Windows users should also disable the Nvidia sysmem fallback.</p> <p>It is possible to fine-tune the settings for best performance or if you still get out-of-memory errors (OOMs).</p> <p>How to find <code>invokeai.yaml</code></p> <p>The <code>invokeai.yaml</code> configuration file lives in your install directory. To access it, run the Invoke Community Edition launcher and click the install location. This will open your install directory in a file explorer window.</p> <p>You'll see <code>invokeai.yaml</code> there and can edit it with any text editor. After making changes, restart Invoke.</p> <p>If you don't see <code>invokeai.yaml</code>, launch Invoke once. It will create the file on its first startup.</p>"},{"location":"features/low-vram/#details-and-fine-tuning","title":"Details and fine-tuning","text":"<p>Low-VRAM mode involves 4 features, each of which can be configured or fine-tuned:</p> <ul> <li>Partial model loading (<code>enable_partial_loading</code>)</li> <li>PyTorch CUDA allocator config (<code>pytorch_cuda_alloc_conf</code>)</li> <li>Dynamic RAM and VRAM cache sizes (<code>max_cache_ram_gb</code>, <code>max_cache_vram_gb</code>)</li> <li>Working memory (<code>device_working_mem_gb</code>)</li> <li>Keeping a RAM weight copy (<code>keep_ram_copy_of_weights</code>)</li> </ul> <p>Read on to learn about these features and understand how to fine-tune them for your system and use-cases.</p>"},{"location":"features/low-vram/#partial-model-loading","title":"Partial model loading","text":"<p>Invoke's partial model loading works by streaming model \"layers\" between RAM and VRAM as they are needed.</p> <p>When an operation needs layers that are not in VRAM, but there isn't enough room to load them, inactive layers are offloaded to RAM to make room.</p>"},{"location":"features/low-vram/#enabling-partial-model-loading","title":"Enabling partial model loading","text":"<p>As described above, you can enable partial model loading by adding this line to <code>invokeai.yaml</code>:</p> <pre><code>enable_partial_loading: true\n</code></pre>"},{"location":"features/low-vram/#pytorch-cuda-allocator-config","title":"PyTorch CUDA allocator config","text":"<p>The PyTorch CUDA allocator's behavior can be configured using the <code>pytorch_cuda_alloc_conf</code> config. Tuning the allocator configuration can help to reduce the peak reserved VRAM. The optimal configuration is dependent on many factors (e.g. device type, VRAM, CUDA driver version, etc.), but switching from PyTorch's native allocator to using CUDA's built-in allocator works well on many systems. To try this, add the following line to your <code>invokeai.yaml</code> file:</p> <pre><code>pytorch_cuda_alloc_conf: \"backend:cudaMallocAsync\"\n</code></pre> <p>A more complete explanation of the available configuration options is here.</p>"},{"location":"features/low-vram/#dynamic-ram-and-vram-cache-sizes","title":"Dynamic RAM and VRAM cache sizes","text":"<p>Loading models from disk is slow and can be a major bottleneck for performance. Invoke uses two model caches - RAM and VRAM - to reduce loading from disk to a minimum.</p> <p>By default, Invoke manages these caches' sizes dynamically for best performance.</p>"},{"location":"features/low-vram/#fine-tuning-cache-sizes","title":"Fine-tuning cache sizes","text":"<p>Prior to v5.6.0, the cache sizes were static, and for best performance, many users needed to manually fine-tune the <code>ram</code> and <code>vram</code> settings in <code>invokeai.yaml</code>.</p> <p>As of v5.6.0, the caches are dynamically sized. The <code>ram</code> and <code>vram</code> settings are no longer used, and new settings are added to configure the cache.</p> <p>Most users will not need to fine-tune the cache sizes.</p> <p>But, if your GPU has enough VRAM to hold models fully, you might get a perf boost by manually setting the cache sizes in <code>invokeai.yaml</code>:</p> <pre><code># The default max cache RAM size is logged on InvokeAI startup. It is determined based on your system RAM / VRAM.\n# You can override the default value by setting `max_cache_ram_gb`.\n# Increasing `max_cache_ram_gb` will increase the amount of RAM used to cache inactive models, resulting in faster model\n# reloads for the cached models.\n# As an example, if your system has 32GB of RAM and no other heavy processes, setting the `max_cache_ram_gb` to 28GB\n# might be a good value to achieve aggressive model caching.\nmax_cache_ram_gb: 28\n\n# The default max cache VRAM size is adjusted dynamically based on the amount of available VRAM (taking into\n# consideration the VRAM used by other processes).\n# You can override the default value by setting `max_cache_vram_gb`.\n# CAUTION: Most users should not manually set this value. See warning below.\nmax_cache_vram_gb: 16\n</code></pre> <p>Max safe value for <code>max_cache_vram_gb</code></p> <p>Most users should not manually configure the <code>max_cache_vram_gb</code>. This configuration value takes precedence over the <code>device_working_mem_gb</code> and any operations that explicitly reserve additional working memory (e.g. VAE decode). As such, manually configuring it increases the likelihood of encountering out-of-memory errors.</p> <p>For users who wish to configure <code>max_cache_vram_gb</code>, the max safe value can be determined by subtracting <code>device_working_mem_gb</code> from your GPU's VRAM. As described below, the default for <code>device_working_mem_gb</code> is 3GB.</p> <p>For example, if you have a 12GB GPU, the max safe value for <code>max_cache_vram_gb</code> is <code>12GB - 3GB = 9GB</code>.</p> <p>If you had increased <code>device_working_mem_gb</code> to 4GB, then the max safe value for <code>max_cache_vram_gb</code> is <code>12GB - 4GB = 8GB</code>.</p> <p>Most users who override <code>max_cache_vram_gb</code> are doing so because they wish to use significantly less VRAM, and should be setting <code>max_cache_vram_gb</code> to a value significantly less than the 'max safe value'.</p>"},{"location":"features/low-vram/#working-memory","title":"Working memory","text":"<p>Invoke cannot use all of your VRAM for model caching and loading. It requires some VRAM to use as working memory for various operations.</p> <p>Invoke reserves 3GB VRAM as working memory by default, which is enough for most use-cases. However, it is possible to fine-tune this setting if you still get OOMs.</p>"},{"location":"features/low-vram/#fine-tuning-working-memory","title":"Fine-tuning working memory","text":"<p>You can increase the working memory size in <code>invokeai.yaml</code> to prevent OOMs:</p> <pre><code># The default is 3GB - bump it up to 4GB to prevent OOMs.\ndevice_working_mem_gb: 4\n</code></pre> <p>Operations may request more working memory</p> <p>For some operations, we can determine VRAM requirements in advance and allocate additional working memory to prevent OOMs.</p> <p>VAE decoding is one such operation. This operation converts the generation process's output into an image. For large image outputs, this might use more than the default working memory size of 3GB.</p> <p>During this decoding step, Invoke calculates how much VRAM will be required to decode and requests that much VRAM from the model manager. If the amount exceeds the working memory size, the model manager will offload cached model layers from VRAM until there's enough VRAM to decode.</p> <p>Once decoding completes, the model manager \"reclaims\" the extra VRAM allocated as working memory for future model loading operations.</p>"},{"location":"features/low-vram/#keeping-a-ram-weight-copy","title":"Keeping a RAM weight copy","text":"<p>Invoke has the option of keeping a RAM copy of all model weights, even when they are loaded onto the GPU. This optimization is on by default, and enables faster model switching and LoRA patching. Disabling this feature will reduce the average RAM load while running Invoke (peak RAM likely won't change), at the cost of slower model switching and LoRA patching. If you have limited RAM, you can disable this optimization:</p> <pre><code># Set to false to reduce the average RAM usage at the cost of slower model switching and LoRA patching.\nkeep_ram_copy_of_weights: false\n</code></pre>"},{"location":"features/low-vram/#disabling-nvidia-sysmem-fallback-windows-only","title":"Disabling Nvidia sysmem fallback (Windows only)","text":"<p>On Windows, Nvidia GPUs are able to use system RAM when their VRAM fills up via sysmem fallback. While it sounds like a good idea on the surface, in practice it causes massive slowdowns during generation.</p> <p>It is strongly suggested to disable this feature:</p> <ul> <li>Open the NVIDIA Control Panel app.</li> <li>Expand 3D Settings on the left panel.</li> <li>Click Manage 3D Settings in the left panel.</li> <li>Find CUDA - Sysmem Fallback Policy in the right panel and set it to Prefer No Sysmem Fallback.</li> </ul> <p></p> <p>Invoke does the same thing, but better</p> <p>If the sysmem fallback feature sounds familiar, that's because Invoke's partial model loading strategy is conceptually very similar - use VRAM when there's room, else fall back to RAM.</p> <p>Unfortunately, the Nvidia implementation is not optimized for applications like Invoke and does more harm than good.</p>"},{"location":"features/low-vram/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/low-vram/#windows-page-file","title":"Windows page file","text":"<p>Invoke has high virtual memory (a.k.a. 'committed memory') requirements. This can cause issues on Windows if the page file size limits are hit. (See this issue for the technical details on why this happens: https://github.com/invoke-ai/InvokeAI/issues/7563).</p> <p>If you run out of page file space, InvokeAI may crash. Often, these crashes will happen with one of the following errors:</p> <ul> <li>InvokeAI exits with Windows error code <code>3221225477</code></li> <li>InvokeAI crashes without an error, but <code>eventvwr.msc</code> reveals an error with code <code>0xc0000005</code> (the hex equivalent of <code>3221225477</code>)</li> </ul> <p>If you are running out of page file space, try the following solutions:</p> <ul> <li>Make sure that you have sufficient disk space for the page file to grow. Watch your disk usage as Invoke runs. If it climbs near 100% leading up to the crash, then this is very likely the source of the issue. Clear out some disk space to resolve the issue.</li> <li>Make sure that your page file is set to \"System managed size\" (this is the default) rather than a custom size. Under the \"System managed size\" policy, the page file will grow dynamically as needed. </li> </ul>"},{"location":"features/orphaned_model_removal/","title":"Orphaned Models Synchronization Feature","text":""},{"location":"features/orphaned_model_removal/#overview","title":"Overview","text":"<p>This feature adds a UI for synchronizing the models directory by finding and removing orphaned model files. Orphaned models are directories that contain model files but are not referenced in the InvokeAI database.</p>"},{"location":"features/orphaned_model_removal/#implementation-summary","title":"Implementation Summary","text":""},{"location":"features/orphaned_model_removal/#backend-python","title":"Backend (Python)","text":""},{"location":"features/orphaned_model_removal/#new-service-orphanedmodelsservice","title":"New Service: <code>OrphanedModelsService</code>","text":"<ul> <li>Location: <code>invokeai/app/services/orphaned_models/</code></li> <li>Implements the core logic from the CLI script</li> <li>Methods:</li> <li><code>find_orphaned_models()</code>: Scans the models directory and database to find orphaned models</li> <li><code>delete_orphaned_models(paths)</code>: Safely deletes specified orphaned model directories</li> </ul>"},{"location":"features/orphaned_model_removal/#api-routes","title":"API Routes","text":"<p>Added to <code>invokeai/app/api/routers/model_manager.py</code>: - <code>GET /api/v2/models/sync/orphaned</code>: Returns list of orphaned models with metadata - <code>DELETE /api/v2/models/sync/orphaned</code>: Deletes selected orphaned models</p>"},{"location":"features/orphaned_model_removal/#data-models","title":"Data Models","text":"<ul> <li><code>OrphanedModelInfo</code>: Contains path, absolute_path, files list, and size_bytes</li> <li><code>DeleteOrphanedModelsRequest</code>: Contains list of paths to delete</li> <li><code>DeleteOrphanedModelsResponse</code>: Contains deleted paths and errors</li> </ul>"},{"location":"features/orphaned_model_removal/#frontend-typescriptreact","title":"Frontend (TypeScript/React)","text":""},{"location":"features/orphaned_model_removal/#new-components","title":"New Components","text":"<ol> <li>SyncModelsButton.tsx</li> <li>Red button styled with <code>colorScheme=\"error\"</code> for visual prominence</li> <li>Labeled \"Sync Models\" </li> <li>Opens the SyncModelsDialog when clicked</li> <li> <p>Located next to the \"+ Add Models\" button</p> </li> <li> <p>SyncModelsDialog.tsx</p> </li> <li>Modal dialog that displays orphaned models</li> <li>Features:<ul> <li>List of orphaned models with checkboxes (default: all checked)</li> <li>\"Select All\" / \"Deselect All\" toggle</li> <li>Shows file count and total size for each model</li> <li>\"Delete\" and \"Cancel\" buttons</li> <li>Loading spinner while fetching data</li> <li>Error handling with user-friendly messages</li> </ul> </li> <li>Automatically shows toast if no orphaned models found</li> <li>Shows success/error toasts after deletion</li> </ol>"},{"location":"features/orphaned_model_removal/#api-integration","title":"API Integration","text":"<ul> <li>Added <code>useGetOrphanedModelsQuery</code> and <code>useDeleteOrphanedModelsMutation</code> hooks to <code>services/api/endpoints/models.ts</code></li> <li>Integrated with RTK Query for efficient data fetching and caching</li> </ul>"},{"location":"features/orphaned_model_removal/#translation-strings","title":"Translation Strings","text":"<p>Added to <code>public/locales/en.json</code>: - syncModels, noOrphanedModels, orphanedModelsFound - orphanedModelsDescription, foundOrphanedModels (with pluralization) - filesCount, deleteSelected, deselectAll - Success/error messages for deletion operations</p>"},{"location":"features/orphaned_model_removal/#user-experience-flow","title":"User Experience Flow","text":"<ol> <li>User clicks the red \"Sync Models\" button in the Model Manager</li> <li>System queries the backend for orphaned models</li> <li>If no orphaned models:</li> <li>Toast message: \"The models directory is synchronized. No orphaned files found.\"</li> <li>Dialog closes automatically</li> <li>If orphaned models found:</li> <li>Dialog shows list with checkboxes (all selected by default)</li> <li>User can toggle individual models or use \"Select All\" / \"Deselect All\"</li> <li>Each model shows:<ul> <li>Directory path</li> <li>File count</li> <li>Total size (formatted: B, KB, MB, GB)</li> </ul> </li> <li>User clicks \"Delete {{count}} selected\"</li> <li>System deletes selected models</li> <li>Success/error toasts appear</li> <li>Dialog closes</li> </ol>"},{"location":"features/orphaned_model_removal/#safety-features","title":"Safety Features","text":"<ol> <li>Database Backup: The service creates a backup before any deletion</li> <li>Selective Deletion: Users choose which models to delete</li> <li>Path Validation: Ensures paths are within the models directory</li> <li>Error Handling: Reports which models failed to delete and why</li> <li>Default Selected: All models are selected by default for convenience</li> <li>Confirmation Required: User must explicitly click Delete</li> </ol>"},{"location":"features/orphaned_model_removal/#technical-details","title":"Technical Details","text":""},{"location":"features/orphaned_model_removal/#directory-based-detection","title":"Directory-Based Detection","text":"<p>The system treats model paths as directories: - If database has <code>model-id/file.safetensors</code>, the entire <code>model-id/</code> directory belongs to that model - All files and subdirectories within a registered model directory are protected - Only directories with NO registered models are flagged as orphaned</p>"},{"location":"features/orphaned_model_removal/#supported-file-extensions","title":"Supported File Extensions","text":"<ul> <li>.safetensors</li> <li>.ckpt</li> <li>.pt</li> <li>.pth</li> <li>.bin</li> <li>.onnx</li> </ul>"},{"location":"features/orphaned_model_removal/#skipped-directories","title":"Skipped Directories","text":"<ul> <li>.download_cache</li> <li>.convert_cache</li> <li>__pycache__</li> <li>.git</li> </ul>"},{"location":"features/orphaned_model_removal/#testing-recommendations","title":"Testing Recommendations","text":"<ol> <li>Test with orphaned models: </li> <li>Manually copy a model directory to models folder</li> <li>Verify it appears in the dialog</li> <li> <p>Delete it and verify removal</p> </li> <li> <p>Test with no orphaned models:</p> </li> <li>Clean install</li> <li> <p>Verify toast message appears</p> </li> <li> <p>Test partial selection:</p> </li> <li>Select only some models</li> <li> <p>Verify only selected ones are deleted</p> </li> <li> <p>Test error scenarios:</p> </li> <li>Invalid paths</li> <li>Permission issues</li> <li>Verify error messages are clear</li> </ol>"},{"location":"features/orphaned_model_removal/#files-changed","title":"Files Changed","text":""},{"location":"features/orphaned_model_removal/#backend","title":"Backend","text":"<ul> <li><code>invokeai/app/services/orphaned_models/__init__.py</code> (new)</li> <li><code>invokeai/app/services/orphaned_models/orphaned_models_service.py</code> (new)</li> <li><code>invokeai/app/api/routers/model_manager.py</code> (modified)</li> </ul>"},{"location":"features/orphaned_model_removal/#frontend","title":"Frontend","text":"<ul> <li><code>invokeai/frontend/web/src/services/api/endpoints/models.ts</code> (modified)</li> <li><code>invokeai/frontend/web/src/features/modelManagerV2/subpanels/ModelManager.tsx</code> (modified)</li> <li><code>invokeai/frontend/web/src/features/modelManagerV2/subpanels/ModelManagerPanel/SyncModelsButton.tsx</code> (new)</li> <li><code>invokeai/frontend/web/src/features/modelManagerV2/subpanels/ModelManagerPanel/SyncModelsDialog.tsx</code> (new)</li> <li><code>invokeai/frontend/web/public/locales/en.json</code> (modified)</li> </ul>"},{"location":"features/orphaned_model_removal/#future-enhancements","title":"Future Enhancements","text":"<p>Potential improvements for future versions: 1. Show preview of what will be deleted before deletion 2. Add option to move orphaned models to archive instead of deleting 3. Show disk space that will be freed 4. Add filter/search in orphaned models list 5. Support for undo operation 6. Scheduled automatic cleanup</p>"},{"location":"help/SAMPLER_CONVERGENCE/","title":"Sampler Convergence","text":"<p>As features keep increasing, making the right choices for your needs can become increasingly difficult. What sampler to use? And for how many steps? Do you change the CFG value? Do you use prompt weighting? Do you allow variations?</p> <p>Even once you have a result, do you blend it with other images? Pass it through <code>img2img</code>? With what strength? Do you use inpainting to correct small details? Outpainting to extend cropped sections?</p> <p>The purpose of this series of documents is to help you better understand these tools, so you can make the best out of them. Feel free to contribute with your own findings!</p> <p>In this document, we will talk about sampler convergence.</p> <p>Looking for a short version? Here's a TL;DR in 3 tables.</p> <p>Remember</p> <ul> <li>Results converge as steps (<code>-s</code>) are increased (except for <code>K_DPM_2_A</code> and <code>K_EULER_A</code>). Often at \u2265 <code>-s100</code>, but may require \u2265 <code>-s700</code>).</li> <li>Producing a batch of candidate images at low (<code>-s8</code> to <code>-s30</code>) step counts can save you hours of computation.</li> <li><code>K_HEUN</code> and <code>K_DPM_2</code>  converge in less steps (but are slower).</li> <li><code>K_DPM_2_A</code> and <code>K_EULER_A</code> incorporate a lot of creativity/variability.</li> </ul> Sampler (3 sample avg) it/s (M1 Max 64GB, 512x512) <code>DDIM</code> 1.89 <code>PLMS</code> 1.86 <code>K_EULER</code> 1.86 <code>K_LMS</code> 1.91 <code>K_HEUN</code> 0.95 (slower) <code>K_DPM_2</code> 0.95 (slower) <code>K_DPM_2_A</code> 0.95 (slower) <code>K_EULER_A</code> 1.86 <p>suggestions</p> <p>For most use cases, <code>K_LMS</code>, <code>K_HEUN</code> and <code>K_DPM_2</code> are the best choices (the latter 2 run 0.5x as quick, but tend to converge 2x as quick as <code>K_LMS</code>). At very low steps (\u2264 <code>-s8</code>), <code>K_HEUN</code> and <code>K_DPM_2</code> are not recommended. Use <code>K_LMS</code> instead.</p> <p>For variability, use <code>K_EULER_A</code> (runs 2x as quick as <code>K_DPM_2_A</code>).</p>"},{"location":"help/SAMPLER_CONVERGENCE/#sampler-results","title":"Sampler results","text":"<p>Let's start by choosing a prompt and using it with each of our 8 samplers, running it for 10, 20, 30, 40, 50 and 100 steps.</p> <p>Anime. <code>\"an anime girl\" -W512 -H512 -C7.5 -S3031912972</code></p> <p></p>"},{"location":"help/SAMPLER_CONVERGENCE/#sampler-convergence_1","title":"Sampler convergence","text":"<p>Immediately, you can notice results tend to converge -that is, as <code>-s</code> (step) values increase, images look more and more similar until there comes a point where the image no longer changes.</p> <p>You can also notice how <code>DDIM</code> and <code>PLMS</code> eventually tend to converge to K-sampler results as steps are increased. Among K-samplers, <code>K_HEUN</code> and <code>K_DPM_2</code> seem to require the fewest steps to converge, and even at low step counts they are good indicators of the final result. And finally, <code>K_DPM_2_A</code> and <code>K_EULER_A</code> seem to do a bit of their own thing and don't keep much similarity with the rest of the samplers.</p>"},{"location":"help/SAMPLER_CONVERGENCE/#batch-generation-speedup","title":"Batch generation speedup","text":"<p>This realization is very useful because it means you don't need to create a batch of 100 images (<code>-n100</code>) at <code>-s100</code> to choose your favorite 2 or 3 images. You can produce the same 100 images at <code>-s10</code> to <code>-s30</code> using a K-sampler (since they converge faster), get a rough idea of the final result, choose your 2 or 3 favorite ones, and then run <code>-s100</code> on those images to polish some details. The latter technique is 3-8x as quick.</p> <p>Example</p> <p>At 60s per 100 steps.</p> <p>A) 60s * 100 images = 6000s (100 images at <code>-s100</code>, manually picking 3 favorites)</p> <p>B) 6s 100 images + 60s 3 images = 780s (100 images at <code>-s10</code>, manually picking 3 favorites, and running those 3 at <code>-s100</code> to polish details)</p> <p>The result is 1 hour and 40 minutes for Variant A, vs 13 minutes for Variant B.</p>"},{"location":"help/SAMPLER_CONVERGENCE/#topic-convergance","title":"Topic convergance","text":"<p>Now, these results seem interesting, but do they hold for other topics? How about nature? Food? People? Animals? Let's try!</p> <p>Nature. <code>\"valley landscape wallpaper, d&amp;d art, fantasy, painted, 4k, high detail, sharp focus, washed colors, elaborate excellent painted illustration\" -W512 -H512 -C7.5 -S1458228930</code></p> <p></p> <p>With nature, you can see how initial results are even more indicative of final result -more so than with characters/people. <code>K_HEUN</code> and <code>K_DPM_2</code> are again the quickest indicators, almost right from the start. Results also converge faster (e.g. <code>K_HEUN</code> converged at <code>-s21</code>).</p> <p>Food. <code>\"a hamburger with a bowl of french fries\" -W512 -H512 -C7.5 -S4053222918</code></p> <p></p> <p>Again, <code>K_HEUN</code> and <code>K_DPM_2</code> take the fewest number of steps to be good indicators of the final result. <code>K_DPM_2_A</code> and <code>K_EULER_A</code> seem to incorporate a lot of creativity/variability, capable of producing rotten hamburgers, but also of adding lettuce to the mix. And they're the only samplers that produced an actual 'bowl of fries'!</p> <p>Animals. <code>\"grown tiger, full body\" -W512 -H512 -C7.5 -S3721629802</code></p> <p></p> <p><code>K_HEUN</code> and <code>K_DPM_2</code> once again require the least number of steps to be indicative of the final result (around <code>-s30</code>), while other samplers are still struggling with several tails or malformed back legs.</p> <p>It also takes longer to converge (for comparison, <code>K_HEUN</code> required around 150 steps to converge). This is normal, as producing human/animal faces/bodies is one of the things the model struggles the most with. For these topics, running for more steps will often increase coherence within the composition.</p> <p>People. <code>\"Ultra realistic photo, (Miranda Bloom-Kerr), young, stunning model, blue eyes, blond hair, beautiful face, intricate, highly detailed, smooth, art by artgerm and greg rutkowski and alphonse mucha, stained glass\" -W512 -H512 -C7.5 -S2131956332</code>. This time, we will go up to 300 steps.</p> <p></p> <p>Observing the results, it again takes longer for all samplers to converge (<code>K_HEUN</code> took around 150 steps), but we can observe good indicative results much earlier (see: <code>K_HEUN</code>). Conversely, <code>DDIM</code> and <code>PLMS</code> are still undergoing moderate changes (see: lace around her neck), even at <code>-s300</code>.</p> <p>In fact, as we can see in this other experiment, some samplers can take 700+ steps to converge when generating people.</p> <p></p> <p>Note also the point of convergence may not be the most desirable state (e.g. I prefer an earlier version of the face, more rounded), but it will probably be the most coherent arms/hands/face attributes-wise. You can always merge different images with a photo editing tool and pass it through <code>img2img</code> to smoothen the composition.</p>"},{"location":"help/SAMPLER_CONVERGENCE/#sampler-generation-times","title":"Sampler generation times","text":"<p>Once we understand the concept of sampler convergence, we must look into the performance of each sampler in terms of steps (iterations) per second, as not all samplers run at the same speed.</p> <p>On my M1 Max with 64GB of RAM, for a 512x512 image</p> Sampler (3 sample average) it/s <code>DDIM</code> 1.89 <code>PLMS</code> 1.86 <code>K_EULER</code> 1.86 <code>K_LMS</code> 1.91 <code>K_HEUN</code> 0.95 (slower) <code>K_DPM_2</code> 0.95 (slower) <code>K_DPM_2_A</code> 0.95 (slower) <code>K_EULER_A</code> 1.86 <p>Combining our results with the steps per second of each sampler, three choices come out on top: <code>K_LMS</code>, <code>K_HEUN</code> and <code>K_DPM_2</code> (where the latter two run 0.5x as quick but tend to converge 2x as quick as <code>K_LMS</code>). For creativity and a lot of variation between iterations, <code>K_EULER_A</code> can be a good choice (which runs 2x as quick as <code>K_DPM_2_A</code>).</p> <p>Additionally, image generation at very low steps (\u2264 <code>-s8</code>) is not recommended for <code>K_HEUN</code> and <code>K_DPM_2</code>. Use <code>K_LMS</code> instead.</p> <p></p>"},{"location":"help/SAMPLER_CONVERGENCE/#three-key-points","title":"Three key points","text":"<p>Finally, it is relevant to mention that, in general, there are 3 important moments in the process of image formation as steps increase:</p> <ul> <li> <p>The (earliest) point at which an image becomes a good indicator of the final result (useful for batch generation at low step values, to then improve the quality/coherence of the chosen images via running the same prompt and seed for more steps).</p> </li> <li> <p>The (earliest) point at which an image becomes coherent, even if different from the result if steps are increased (useful for batch generation at low step values, where quality/coherence is improved via techniques other than increasing the steps -e.g. via inpainting).</p> </li> <li> <p>The point at which an image fully converges.</p> </li> </ul> <p>Hence, remember that your workflow/strategy should define your optimal number of steps, even for the same prompt and seed (for example, if you seek full convergence, you may run <code>K_LMS</code> for <code>-s200</code> in the case of the red-haired girl, but <code>K_LMS</code> and <code>-s20</code>-taking one tenth the time- may do as well if your workflow includes adding small details, such as the missing shoulder strap, via <code>img2img</code>).</p>"},{"location":"help/diffusion/","title":"Diffusion Overview","text":"<p>Taking the time to understand the diffusion process will help you to understand how to more effectively use InvokeAI.</p> <p>There are two main ways Stable Diffusion works - with images, and latents.</p> <p>Image space represents images in pixel form that you look at. Latent space represents compressed inputs. It\u2019s in latent space that Stable Diffusion processes images. A VAE (Variational Auto Encoder) is responsible for compressing and encoding inputs into latent space, as well as decoding outputs back into image space.</p> <p>To fully understand the diffusion process, we need to understand a few more terms: UNet, CLIP, and conditioning.</p> <p>A U-Net is a model trained on a large number of latent images with with known amounts of random noise added.  This means that the U-Net can be given a slightly noisy image and it will predict the pattern of noise needed to subtract from the image in order to recover the original. </p> <p>CLIP is a model that tokenizes and encodes text into conditioning. This conditioning guides the model during the denoising steps to produce a new image. </p> <p>The U-Net and CLIP work together during the image generation process at each denoising step, with the U-Net removing noise in such a way that the result is similar to images in the U-Net\u2019s training set, while CLIP guides the U-Net towards creating images that are most similar to the prompt.</p> <p>When you generate an image using text-to-image, multiple steps occur in latent space: 1. Random noise is generated at the chosen height and width. The noise\u2019s characteristics are dictated by  seed. This noise tensor is passed into latent space. We\u2019ll call this noise A. 2. Using a model\u2019s U-Net, a noise predictor examines noise A, and the words tokenized by CLIP from your prompt (conditioning). It generates its own noise tensor to predict what the final image might look like in latent space. We\u2019ll call this noise B. 3. Noise B is subtracted from noise A in an attempt to create a latent image consistent with the prompt. This step is repeated for the number of sampler steps chosen. 4. The VAE decodes the final latent image from latent space into image space.</p> <p>Image-to-image is a similar process, with only step 1 being different: 1. The input image is encoded from image space into latent space by the VAE. Noise is then added to the input latent image. Denoising Strength dictates how many noise steps are added, and the amount of noise added at each step. A Denoising Strength of 0 means there are 0 steps and no noise added, resulting in an unchanged image, while a Denoising Strength of 1 results in the image being completely replaced with noise and a full set of denoising steps are performance. The process is then the same as steps 2-4 in the text-to-image process. </p> <p>Furthermore, a model provides the CLIP prompt tokenizer, the VAE, and a U-Net (where noise prediction occurs given a prompt and initial noise tensor).</p> <p>A noise scheduler (eg. DPM++ 2M Karras) schedules the subtraction of noise from the latent image across the sampler steps chosen (step 3 above). Less noise is usually subtracted at higher sampler steps. </p>"},{"location":"help/gettingStartedWithAI/","title":"Getting Started with AI Image Generation","text":"<p>New to image generation with AI? You\u2019re in the right place! </p> <p>This is a high level walkthrough of some of the concepts and terms you\u2019ll see as you start using InvokeAI. Please note, this is not an exhaustive guide and may be out of date due to the rapidly changing nature of the space. </p>"},{"location":"help/gettingStartedWithAI/#using-invokeai","title":"Using InvokeAI","text":""},{"location":"help/gettingStartedWithAI/#prompt-crafting","title":"Prompt Crafting","text":"<ul> <li>Prompts are the basis of using InvokeAI, providing the models directions on what to generate. As a general rule of thumb, the more detailed your prompt is, the better your result will be.</li> </ul> <p>To get started, here\u2019s an easy template to use for structuring your prompts:</p> <ul> <li>Subject, Style, Quality, Aesthetic<ul> <li>Subject: What your image will be about. E.g. \u201ca futuristic city with trains\u201d,  \u201cpenguins floating on icebergs\u201d, \u201cfriends sharing beers\u201d</li> <li>Style: The style or medium in which your image will be in. E.g. \u201cphotograph\u201d, \u201cpencil sketch\u201d, \u201coil paints\u201d, or \u201cpop art\u201d, \u201ccubism\u201d, \u201cabstract\u201d</li> <li>Quality: A particular aspect or trait that you would like to see emphasized in your image. E.g. \"award-winning\", \"featured in {relevant set of high quality works}\", \"professionally acclaimed\". Many people often use \"masterpiece\".</li> <li>Aesthetics: The visual impact and design of the artwork. This can be colors, mood, lighting, setting, etc.</li> </ul> </li> <li>There are two prompt boxes: Positive Prompt &amp; Negative Prompt.<ul> <li>A Positive Prompt includes words you want the model to reference when creating an image.</li> <li>Negative Prompt is for anything you want the model to eliminate when creating an image. It doesn\u2019t always interpret things exactly the way you would, but helps control the generation process. Always try to include a few terms - you can typically use lower quality image terms like \u201cblurry\u201d or \u201cdistorted\u201d with good success.</li> </ul> </li> <li>Some examples prompts you can try on your own:<ul> <li>A detailed oil painting of a tranquil forest at sunset with vibrant+ colors and soft, golden light filtering through the trees</li> <li>friends sharing beers in a busy city, realistic colored pencil sketch, twilight, masterpiece, bright, lively</li> </ul> </li> </ul>"},{"location":"help/gettingStartedWithAI/#generation-workflows","title":"Generation Workflows","text":"<ul> <li>Invoke offers a number of different workflows for interacting with models to produce images. Each is extremely powerful on its own, but together provide you an unparalleled way of producing high quality creative outputs that align with your vision.<ul> <li>Text to Image: The text to image tab focuses on the key workflow of using a prompt to generate a new image. It includes other features that help control the generation process as well.</li> <li>Image to Image: With image to image, you provide an image as a reference (called the \u201cinitial image\u201d), which provides more guidance around color and structure to the AI as it generates a new image. This is provided alongside the same features as Text to Image.</li> <li>Unified Canvas: The Unified Canvas is an advanced AI-first image editing tool that is easy to use, but hard to master. Drag an image onto the canvas from your gallery in order to regenerate certain elements, edit content or colors (known as inpainting), or extend the image with an exceptional degree of consistency and clarity (called outpainting).</li> </ul> </li> </ul>"},{"location":"help/gettingStartedWithAI/#improving-image-quality","title":"Improving Image Quality","text":"<ul> <li>Fine tuning your prompt - the more specific you are, the closer the image will turn out to what is in your head!  Adding more details in the Positive Prompt or Negative Prompt can help add / remove pieces of your image to improve it - You can also use advanced techniques like upweighting and downweighting to control the influence of certain words. Learn more here.<ul> <li>Tip: If you\u2019re seeing poor results, try adding the things you don\u2019t like about the image to your negative prompt may help. E.g. distorted, low quality, unrealistic, etc.</li> </ul> </li> <li>Explore different models - Other models can produce different results due to the data they\u2019ve been trained on. Each model has specific language and settings it works best with; a model\u2019s documentation is your friend here.  Play around with some and see what works best for you!</li> <li>Increasing Steps - The number of steps used controls how much time the model is given to produce an image, and depends on the \u201cScheduler\u201d used. The schedule controls how each step is processed by the model. More steps tends to mean better results, but will take longer - We recommend at least 30 steps for most</li> <li>Tweak and Iterate - Remember, it\u2019s best to change one thing at a time so you know what is working and what isn't. Sometimes you just need to try a new image, and other times using a new prompt might be the ticket. For testing, consider turning off the \u201crandom\u201d Seed - Using the same seed with the same settings will produce the same image, which makes it the perfect way to learn exactly what your changes are doing.</li> <li>Explore Advanced Settings - InvokeAI has a full suite of tools available to allow you complete control over your image creation process - Check out our docs if you want to learn more.</li> </ul>"},{"location":"help/gettingStartedWithAI/#terms-concepts","title":"Terms &amp; Concepts","text":"<p>If you're interested in learning more, check out this presentation from one of our maintainers (@lstein). </p>"},{"location":"help/gettingStartedWithAI/#stable-diffusion","title":"Stable Diffusion","text":"<p>Stable Diffusion is deep learning, text-to-image model that is the foundation of the capabilities found in InvokeAI. Since the release of Stable Diffusion, there have been many subsequent models created based on Stable Diffusion that are designed to generate specific types of images. </p>"},{"location":"help/gettingStartedWithAI/#prompts","title":"Prompts","text":"<p>Prompts provide the models directions on what to generate. As a general rule of thumb, the more detailed your prompt is, the better your result will be.</p>"},{"location":"help/gettingStartedWithAI/#models","title":"Models","text":"<p>Models are the magic that power InvokeAI. These files represent the output of training a machine on understanding massive amounts of images - providing them with the capability to generate new images using just a text description of what you\u2019d like to see. (Like Stable Diffusion!)</p> <p>Invoke offers a simple way to download several different models upon installation, but many more can be discovered online, including at https://models.invoke.ai </p> <p>Each model can produce a unique style of output, based on the images it was trained on - Try out different models to see which best fits your creative vision!</p> <ul> <li>Models that contain \u201cinpainting\u201d in the name are designed for use with the inpainting feature of the Unified Canvas</li> </ul>"},{"location":"help/gettingStartedWithAI/#scheduler","title":"Scheduler","text":"<p>Schedulers guide the process of removing noise (de-noising) from data. They determine:</p> <ol> <li>The number of steps to take to remove the noise.</li> <li>Whether the steps are random (stochastic) or predictable (deterministic).</li> <li>The specific method (algorithm) used for de-noising.</li> </ol> <p>Experimenting with different schedulers is recommended as each will produce different outputs!</p>"},{"location":"help/gettingStartedWithAI/#steps","title":"Steps","text":"<p>The number of de-noising steps each generation through. </p> <p>Schedulers can be intricate and there's often a balance to strike between how quickly they can de-noise data and how well they can do it. It's typically advised to experiment with different schedulers to see which one gives the best results. There has been a lot written on the internet about different schedulers, as well as exploring what the right level of \"steps\" are for each. You can save generation time by reducing the number of steps used, but you'll want to make sure that you are satisfied with the quality of images produced!</p>"},{"location":"help/gettingStartedWithAI/#low-rank-adaptations-loras","title":"Low-Rank Adaptations / LoRAs","text":"<p>Low-Rank Adaptations (LoRAs) are like a smaller, more focused version of models, intended to focus on training a better understanding of how a specific character, style, or concept looks.</p>"},{"location":"help/gettingStartedWithAI/#textual-inversion-embeddings","title":"Textual Inversion Embeddings","text":"<p>Textual Inversion Embeddings, like LoRAs, assist with more easily prompting for certain characters, styles, or concepts. However, embeddings are trained to update the relationship between a specific word (known as the \u201ctrigger\u201d) and the intended output. </p>"},{"location":"help/gettingStartedWithAI/#controlnet","title":"ControlNet","text":"<p>ControlNets are neural network models that are able to extract key features from an existing image and use these features to guide the output of the image generation model. </p>"},{"location":"help/gettingStartedWithAI/#vae","title":"VAE","text":"<p>Variational auto-encoder (VAE) is a encode/decode model that translates the \"latents\" image produced during the image generation procees to the large pixel images that we see. </p>"},{"location":"installation/docker/","title":"Docker","text":"<p>macOS users</p> <p>Docker can not access the GPU on macOS, so your generation speeds will be slow. Use the launcher instead.</p> <p>Linux and Windows Users</p> <p>Configure Docker to access your machine's GPU. Docker Desktop on Windows includes GPU support. Linux users should follow the NVIDIA or AMD documentation.</p>"},{"location":"installation/docker/#tldr","title":"TL;DR","text":"<p>Ensure your Docker setup is able to use your GPU. Then:</p> <pre><code>```bash\ndocker run --runtime=nvidia --gpus=all --publish 9090:9090 ghcr.io/invoke-ai/invokeai\n```\n</code></pre> <p>Once the container starts up, open http://localhost:9090 in your browser, install some models, and start generating.</p>"},{"location":"installation/docker/#build-it-yourself","title":"Build-It-Yourself","text":"<p>All the docker materials are located inside the docker directory in the Git repo.</p> <pre><code>```bash\ncd docker\ncp .env.sample .env\ndocker compose up\n```\n</code></pre> <p>We also ship the <code>run.sh</code> convenience script. See the <code>docker/README.md</code> file for detailed instructions on how to customize the docker setup to your needs.</p>"},{"location":"installation/docker/#prerequisites","title":"Prerequisites","text":""},{"location":"installation/docker/#install-docker","title":"Install Docker","text":"<p>On the Docker Desktop app, go to Preferences, Resources, Advanced. Increase the CPUs and Memory to avoid this Issue. You may need to increase Swap and Disk image size too.</p>"},{"location":"installation/docker/#setup","title":"Setup","text":"<p>Set up your environment variables. In the <code>docker</code> directory, make a copy of <code>.env.sample</code> and name it <code>.env</code>. Make changes as necessary.</p> <p>Any environment variables supported by InvokeAI can be set here - please see the CONFIGURATION for further detail.</p> <p>At a minimum, you might want to set the <code>INVOKEAI_ROOT</code> environment variable to point to the location where you wish to store your InvokeAI models, configuration, and outputs.</p> Environment-Variable  Default value  Description <code>INVOKEAI_ROOT</code> <code>~/invokeai</code> Required - the location of your InvokeAI root directory. It will be created if it does not exist. <code>HUGGING_FACE_HUB_TOKEN</code> InvokeAI will work without it, but some of the integrations with HuggingFace (like downloading from models from private repositories) may not work <code>GPU_DRIVER</code> <code>cuda</code> Optionally change this to <code>rocm</code> to build the image for AMD GPUs. NOTE: Use the <code>build.sh</code> script to build the image for this to take effect."},{"location":"installation/docker/#build-the-image","title":"Build the Image","text":"<p>Use the standard <code>docker compose build</code> command from within the <code>docker</code> directory.</p> <p>If using an AMD GPU: a: set the <code>GPU_DRIVER=rocm</code> environment variable in <code>docker-compose.yml</code> and continue using <code>docker compose build</code> as usual, or b: set <code>GPU_DRIVER=rocm</code> in the <code>.env</code> file and use the <code>build.sh</code> script, provided for convenience</p>"},{"location":"installation/docker/#run-the-container","title":"Run the Container","text":"<p>Use the standard <code>docker compose up</code> command, and generally the <code>docker compose</code> CLI as usual.</p> <p>Once the container starts up (and configures the InvokeAI root directory if this is a new installation), you can access InvokeAI at http://localhost:9090</p>"},{"location":"installation/docker/#troubleshooting-faq","title":"Troubleshooting / FAQ","text":"<ul> <li>Q: I am running on Windows under WSL2, and am seeing a \"no such file or directory\" error.</li> <li>A: Your <code>docker-entrypoint.sh</code> might have has Windows (CRLF) line endings, depending how you cloned the repository.   To solve this, change the line endings in the <code>docker-entrypoint.sh</code> file to <code>LF</code>. You can do this in VSCode   (<code>Ctrl+P</code> and search for \"line endings\"), or by using the <code>dos2unix</code> utility in WSL.   Finally, you may delete <code>docker-entrypoint.sh</code> followed by <code>git pull; git checkout docker/docker-entrypoint.sh</code>   to reset the file to its most recent version.   For more information on this issue, see Docker Desktop documentation</li> </ul>"},{"location":"installation/manual/","title":"Manual Install","text":"<p>Warning</p> <p>Python experience is mandatory.</p> <p>If you want to use Invoke locally, you should probably use the launcher.</p> <p>If you want to contribute to Invoke or run the app on the latest dev branch, instead follow the dev environment guide.</p> <p>InvokeAI is distributed as a python package on PyPI, installable with <code>pip</code>. There are a few things that are handled by the launcher that you'll need to manage manually, described in this guide.</p>"},{"location":"installation/manual/#requirements","title":"Requirements","text":"<p>Before you start, go through the installation requirements.</p>"},{"location":"installation/manual/#walkthrough","title":"Walkthrough","text":"<p>We'll use <code>uv</code> to install python and create a virtual environment, then install the <code>invokeai</code> package. <code>uv</code> is a modern, very fast alternative to <code>pip</code>.</p> <p>The following commands vary depending on the version of Invoke being installed and the system onto which it is being installed.</p> <ol> <li> <p>Install <code>uv</code> as described in its docs. We suggest using the standalone installer method.</p> <p>Run <code>uv --version</code> to confirm that <code>uv</code> is installed and working. After installation, you may need to restart your terminal to get access to <code>uv</code>.</p> </li> <li> <p>Create a directory for your installation, typically in your home directory (e.g. <code>~/invokeai</code> or <code>$Home/invokeai</code>):</p> Linux/macOSWindows (PowerShell) <pre><code>mkdir ~/invokeai\ncd ~/invokeai\n</code></pre> <pre><code>mkdir $Home/invokeai\ncd $Home/invokeai\n</code></pre> </li> <li> <p>Create a virtual environment in that directory:</p> <pre><code>uv venv --relocatable --prompt invoke --python 3.12 --python-preference only-managed .venv\n</code></pre> <p>This command creates a portable virtual environment at <code>.venv</code> complete with a portable python 3.12. It doesn't matter if your system has no python installed, or has a different version - <code>uv</code> will handle everything.</p> </li> <li> <p>Activate the virtual environment:</p> Linux/macOSWindows (PowerShell) <pre><code>source .venv/bin/activate\n</code></pre> <pre><code>.venv\\Scripts\\activate\n</code></pre> </li> <li> <p>Choose a version to install. Review the GitHub releases page.</p> </li> <li> <p>Determine the package specifier to use when installing. This is a performance optimization.</p> <ul> <li>If you have an Nvidia 20xx series GPU or older, use <code>invokeai[xformers]</code>.</li> <li>If you have an Nvidia 30xx series GPU or newer, or do not have an Nvidia GPU, use <code>invokeai</code>.</li> </ul> </li> <li> <p>Determine the torch backend to use for installation, if any. This is necessary to get the right version of torch installed. This is acheived by using UV's built in torch support.</p> Invoke v5.12 and laterInvoke v5.10.0 to v5.11.0Invoke v5.0.0 to v5.9.1Invoke v4 <ul> <li>If you are on Windows or Linux with an Nvidia GPU, use <code>--torch-backend=cu128</code>.</li> <li>If you are on Linux with no GPU, use <code>--torch-backend=cpu</code>.</li> <li>If you are on Linux with an AMD GPU, use <code>--torch-backend=rocm6.3</code>.</li> <li>In all other cases, do not use a torch backend.</li> </ul> <ul> <li>If you are on Windows or Linux with an Nvidia GPU, use <code>--torch-backend=cu126</code>.</li> <li>If you are on Linux with no GPU, use <code>--torch-backend=cpu</code>.</li> <li>If you are on Linux with an AMD GPU, use <code>--torch-backend=rocm6.2.4</code>.</li> <li>In all other cases, do not use an index.</li> </ul> <ul> <li>If you are on Windows with an Nvidia GPU, use <code>--torch-backend=cu124</code>.</li> <li>If you are on Linux with no GPU, use <code>--torch-backend=cpu</code>.</li> <li>If you are on Linux with an AMD GPU, use <code>--torch-backend=rocm6.1</code>.</li> <li>In all other cases, do not use an index.</li> </ul> <ul> <li>If you are on Windows with an Nvidia GPU, use <code>--torch-backend=cu124</code>.</li> <li>If you are on Linux with no GPU, use <code>--torch-backend=cpu</code>.</li> <li>If you are on Linux with an AMD GPU, use <code>--torch-backend=rocm5.2</code>.</li> <li>In all other cases, do not use an index.</li> </ul> </li> <li> <p>Install the <code>invokeai</code> package. Substitute the package specifier and version.</p> <pre><code>uv pip install &lt;PACKAGE_SPECIFIER&gt;==&lt;VERSION&gt; --python 3.12 --python-preference only-managed --force-reinstall\n</code></pre> <p>If you determined you needed to use a torch backend in the previous step, you'll need to set the backend like this:</p> <pre><code>uv pip install &lt;PACKAGE_SPECIFIER&gt;==&lt;VERSION&gt; --python 3.12 --python-preference only-managed --torch-backend=&lt;VERSION&gt; --force-reinstall\n</code></pre> </li> <li> <p>Deactivate and reactivate your venv so that the invokeai-specific commands become available in the environment:</p> Linux/macOSWindows (PowerShell) <pre><code>deactivate &amp;&amp; source .venv/bin/activate\n</code></pre> <pre><code>deactivate\n.venv\\Scripts\\activate\n</code></pre> </li> <li> <p>Run the application, specifying the directory you created earlier as the root directory:</p> Linux/macOSWindows (PowerShell) <pre><code>invokeai-web --root ~/invokeai\n</code></pre> <pre><code>invokeai-web --root $Home/invokeai\n</code></pre> </li> </ol>"},{"location":"installation/manual/#headless-install-and-launch-scripts","title":"Headless Install and Launch Scripts","text":"<p>If you run Invoke on a headless server, you might want to install and run Invoke on the command line.</p> <p>We do not plan to maintain scripts to do this moving forward, instead focusing our dev resources on the GUI launcher.</p> <p>You can create your own scripts for this by copying the handful of commands in this guide. <code>uv</code>'s <code>pip</code> interface docs may be useful.</p>"},{"location":"installation/models/","title":"Models","text":""},{"location":"installation/models/#checkpoint-and-diffusers-models","title":"Checkpoint and Diffusers Models","text":"<p>The model checkpoint files (<code>*.ckpt</code>) are the Stable Diffusion \"secret sauce\". They are the product of training the AI on millions of captioned images gathered from multiple sources.</p> <p>Originally there was only a single Stable Diffusion weights file, which many people named <code>model.ckpt</code>.</p> <p>Today, there are thousands of models, fine tuned to excel at specific styles, genres, or themes.</p> <p>Model Formats</p> <p>We also have two more popular model formats, both created HuggingFace:</p> <ul> <li><code>safetensors</code>: Single file, like <code>.ckpt</code> files. Prevents malware from lurking in a model.</li> <li><code>diffusers</code>: Splits the model components into separate files, allowing very fast loading.</li> </ul> <p>InvokeAI supports all three formats.</p>"},{"location":"installation/models/#starter-models","title":"Starter Models","text":"<p>When you first start InvokeAI, you'll see a popup prompting you to install some starter models from the Model Manager. Click the <code>Starter Models</code> tab to see the list.</p> <p>You'll find a collection of popular and high-quality models available for easy download.</p> <p>Some models carry license terms that limit their use in commercial applications or on public servers. It's your responsibility to adhere to the license terms.</p>"},{"location":"installation/models/#other-models","title":"Other Models","text":"<p>There are a few ways to install other models:</p> <ul> <li>URL or Local Path: Provide the path to a model on your computer, or a direct link to the model. Some sites require you to use an API token to download models, which you can set up in the config file.</li> <li>HuggingFace: Paste a HF Repo ID to install it. If there are multiple models in the repo, you'll get a list to choose from. Repo IDs look like this: <code>XpucT/Deliberate</code>. There is a copy button on each repo to copy the ID.</li> <li>Scan Folder: Scan a local folder for models. You can install all of the detected models in one click.</li> </ul> <p>Autoimport</p> <p>The dedicated autoimport folder is removed as of v4.0.0. You can do the same thing on the Scan Folder tab - paste the folder you'd like to import from and then click <code>Install All</code>.</p>"},{"location":"installation/models/#diffusers-models-in-hf-repo-subfolders","title":"Diffusers models in HF repo subfolders","text":"<p>HuggingFace repos can be structured in any way. Some model authors include multiple models within the same folder.</p> <p>In this situation, you may need to provide some additional information to identify the model you want, by adding <code>:subfolder_name</code> to the repo ID.</p> <p>Example</p> <p>Say you have a repo ID <code>monster-labs/control_v1p_sd15_qrcode_monster</code>, and the model you want is inside the <code>v2</code> subfolder.</p> <p>Add <code>:v2</code> to the repo ID and use that when installing the model: <code>monster-labs/control_v1p_sd15_qrcode_monster:v2</code></p>"},{"location":"installation/patchmatch/","title":"Installing PyPatchMatch","text":"<p>PatchMatch is an algorithm used to infill images. It can greatly improve outpainting results. PyPatchMatch is a python wrapper around a C++ implementation of the algorithm.</p> <p>It uses the image data around the target area as a reference to generate new image data of a similar character and quality.</p>"},{"location":"installation/patchmatch/#why-use-patchmatch","title":"Why Use PatchMatch","text":"<p>In the context of image generation, \"outpainting\" refers to filling in a transparent area using AI-generated image data. But the AI can't generate without some initial data. We need to first fill in the transparent area with something.</p> <p>The first step in \"outpainting\" then, is to fill in the transparent area with something. Generally, you get better results when that initial infill resembles the rest of the image.</p> <p>Because PatchMatch generates image data so similar to the rest of the image, it works very well as the first step in outpainting, typically producing better results than other infill methods supported by Invoke (e.g. LaMA, cv2 infill, random tiles).</p>"},{"location":"installation/patchmatch/#performance-caveat","title":"Performance Caveat","text":"<p>PatchMatch is CPU-bound, and the amount of time it takes increases proportionally as the infill area increases. While the numbers certainly vary depending on system specs, you can expect a noticeable slowdown once you start infilling areas around 512x512 pixels. 1024x1024 pixels can take several seconds to infill.</p>"},{"location":"installation/patchmatch/#installation","title":"Installation","text":"<p>Unfortunately, installation can be somewhat challenging, as it requires some things that <code>pip</code> cannot install for you.</p>"},{"location":"installation/patchmatch/#windows","title":"Windows","text":"<p>You're in luck! On Windows platforms PyPatchMatch will install automatically on Windows systems with no extra intervention.</p>"},{"location":"installation/patchmatch/#macintosh","title":"Macintosh","text":"<p>You need to have opencv installed so that pypatchmatch can be built:</p> <pre><code>brew install opencv\n</code></pre> <p>The next time you start <code>invoke</code>, after successfully installing opencv, pypatchmatch will be built.</p>"},{"location":"installation/patchmatch/#linux","title":"Linux","text":"<p>Prior to installing PyPatchMatch, you need to take the following steps:</p>"},{"location":"installation/patchmatch/#debian-based-distros","title":"Debian Based Distros","text":"<ol> <li>Install the <code>build-essential</code> tools:</li> </ol> <pre><code>sudo apt update\nsudo apt install build-essential\n</code></pre> <ol> <li>Install <code>opencv</code>:</li> </ol> <pre><code>sudo apt install python3-opencv libopencv-dev\n</code></pre> <ol> <li> <p>Activate the environment you use for invokeai, either with <code>conda</code> or with a    virtual environment.</p> </li> <li> <p>Install pypatchmatch:</p> </li> </ol> <pre><code>pip install pypatchmatch\n</code></pre> <ol> <li>Confirm that pypatchmatch is installed. At the command-line prompt enter    <code>python</code>, and then at the <code>&gt;&gt;&gt;</code> line type    <code>from patchmatch import patch_match</code>: It should look like the following:</li> </ol> <pre><code>Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; from patchmatch import patch_match\nCompiling and loading c extensions from \"/home/lstein/Projects/InvokeAI/.invokeai-env/src/pypatchmatch/patchmatch\".\nrm -rf build/obj libpatchmatch.so\nmkdir: created directory 'build/obj'\nmkdir: created directory 'build/obj/csrc/'\n[dep] csrc/masked_image.cpp ...\n[dep] csrc/nnf.cpp ...\n[dep] csrc/inpaint.cpp ...\n[dep] csrc/pyinterface.cpp ...\n[CC] csrc/pyinterface.cpp ...\n[CC] csrc/inpaint.cpp ...\n[CC] csrc/nnf.cpp ...\n[CC] csrc/masked_image.cpp ...\n[link] libpatchmatch.so ...\n</code></pre>"},{"location":"installation/patchmatch/#arch-based-distros","title":"Arch Based Distros","text":"<ol> <li>Install the <code>base-devel</code> package:</li> </ol> <pre><code>sudo pacman -Syu\nsudo pacman -S --needed base-devel\n</code></pre> <ol> <li>Install <code>opencv</code>, <code>blas</code>, and required dependencies:</li> </ol> <pre><code>sudo pacman -S opencv blas fmt glew vtk hdf5\n</code></pre> <p>or for CUDA support</p> <pre><code>sudo pacman -S opencv-cuda blas fmt glew vtk hdf5\n</code></pre> <ol> <li>Fix the naming of the <code>opencv</code> package configuration file:</li> </ol> <pre><code>cd /usr/lib/pkgconfig/\nln -sf opencv4.pc opencv.pc\n</code></pre> <p>Next, Follow Steps 4-6 from the Debian Section above</p> <p>If you see no errors you're ready to go!</p>"},{"location":"installation/quick_start/","title":"Invoke Community Edition Quick Start","text":"<p>Welcome to Invoke! Follow these steps to install, update, and get started creating.</p>"},{"location":"installation/quick_start/#step-1-system-requirements","title":"Step 1: System Requirements","text":"<p>Invoke runs on Windows 10+, macOS 14+ and Linux (Ubuntu 20.04+ is well-tested).</p> <p>Hardware requirements vary significantly depending on model and image output size. The requirements below are rough guidelines.</p> <ul> <li>All Apple Silicon (M1, M2, etc) Macs work, but 16GB+ memory is recommended.</li> <li>AMD GPUs are supported on Linux only. The VRAM requirements are the same as Nvidia GPUs.</li> </ul> <p>Hardware Requirements (Windows/Linux)</p> SD1.5 - 512\u00d7512SDXL - 1024\u00d71024FLUX.1 - 1024\u00d71024FLUX.2 Klein - 1024\u00d71024Z-Image Turbo - 1024x1024 <ul> <li>GPU: Nvidia 10xx series or later, 4GB+ VRAM.</li> <li>Memory: At least 8GB RAM.</li> <li>Disk: 10GB for base installation plus 30GB for models.</li> </ul> <ul> <li>GPU: Nvidia 20xx series or later, 8GB+ VRAM.</li> <li>Memory: At least 16GB RAM.</li> <li>Disk: 10GB for base installation plus 100GB for models.</li> </ul> <ul> <li>GPU: Nvidia 20xx series or later, 10GB+ VRAM.</li> <li>Memory: At least 32GB RAM.</li> <li>Disk: 10GB for base installation plus 200GB for models.</li> </ul> <ul> <li>GPU: Nvidia 20xx series or later, 6GB+ VRAM for GGUF Q4 quantized models, 12GB+ for full precision.</li> <li>Memory: At least 16GB RAM.</li> <li>Disk: 10GB for base installation plus 20GB for models.</li> </ul> <ul> <li>GPU: Nvidia 20xx series or later, 8GB+ VRAM for the Q4_K quantized model. 16GB+ needed for the Q8 or BF16 models.</li> <li>Memory: At least 16GB RAM.</li> <li>Disk: 10GB for base installation plus 35GB for models.</li> </ul> <p>More detail on system requirements can be found here.</p>"},{"location":"installation/quick_start/#step-2-download-and-set-up-the-launcher","title":"Step 2: Download and Set Up the Launcher","text":"<p>The Launcher manages your Invoke install. Follow these instructions to download and set up the Launcher.</p> <p>Instructions for each OS</p> WindowsmacOSLinux <ul> <li>Download for Windows</li> <li>Run the <code>EXE</code> to install the Launcher and start it.</li> <li>A desktop shortcut will be created; use this to run the Launcher in the future.</li> <li>You can delete the <code>EXE</code> file you downloaded.</li> </ul> <ul> <li>Download for macOS</li> <li>Open the <code>DMG</code> and drag the app into <code>Applications</code>.</li> <li>Run the Launcher using its entry in <code>Applications</code>.</li> <li>You can delete the <code>DMG</code> file you downloaded.</li> </ul> <ul> <li>Download for Linux</li> <li>You may need to edit the <code>AppImage</code> file properties and make it executable.</li> <li>Optionally move the file to a location that does not require admin privileges and add a desktop shortcut for it.</li> <li>Run the Launcher by double-clicking the <code>AppImage</code> or the shortcut you made.</li> </ul>"},{"location":"installation/quick_start/#step-3-install-invoke","title":"Step 3: Install Invoke","text":"<p>Run the Launcher you just set up if you haven't already. Click Install and follow the instructions to install (or update) Invoke.</p> <p>If you have an existing Invoke installation, you can select it and let the launcher manage the install. You'll be able to update or launch the installation.</p> <p>Updating</p> <p>The Launcher will check for updates for itself and Invoke.</p> <ul> <li>When the Launcher detects an update is available for itself, you'll get a small popup window. Click through this and the Launcher will update itself.</li> <li>When the Launcher detects an update for Invoke, you'll see a small green alert in the Launcher. Click that and follow the instructions to update Invoke.</li> </ul>"},{"location":"installation/quick_start/#step-4-launch","title":"Step 4: Launch","text":"<p>Once installed, click Finish, then Launch to start Invoke.</p> <p>The very first run after an installation or update will take a few extra moments to get ready.</p> <p>Server Mode</p> <p>The launcher runs Invoke as a desktop application. You can enable Server Mode in the launcher's settings to disable this and instead access the UI through your web browser.</p>"},{"location":"installation/quick_start/#step-5-install-models","title":"Step 5: Install Models","text":"<p>With Invoke started up, you'll need to install some models.</p> <p>The quickest way to get started is to install a Starter Model bundle. If you already have a model collection, Invoke can use it.</p> <p>Install Models</p> Install a Starter Model bundleUse my model collection <ol> <li>Go to the Models tab.</li> <li>Click Starter Models on the right.</li> <li>Click one of the bundles to install its models. Refer to the system requirements if you're unsure which model architecture will work for your system.</li> </ol> <ol> <li>Go to the Models tab.</li> <li>Click Scan Folder on the right.</li> <li>Paste the path to your models collection and click Scan Folder.</li> <li>With In-place install enabled, Invoke will leave the model files where they are. If you disable this, Invoke will move the models into its own folders.</li> </ol> <p>You\u2019re now ready to start creating!</p>"},{"location":"installation/quick_start/#step-6-learn-the-basics","title":"Step 6: Learn the Basics","text":"<p>We recommend watching our Getting Started Playlist. It covers essential features and workflows, including:</p> <ul> <li>Generating your first image.</li> <li>Using control layers and reference guides.</li> <li>Refining images with advanced workflows.</li> </ul>"},{"location":"installation/quick_start/#troubleshooting","title":"Troubleshooting","text":"<p>If installation fails, retrying the install in Repair Mode may fix it. There's a checkbox to enable this on the Review step of the install flow.</p> <p>If that doesn't fix it, clearing the <code>uv</code> cache might do the trick:</p> <ul> <li>Open and start the dev console (button at the bottom-left of the launcher).</li> <li>Run <code>uv cache clean</code>.</li> <li>Retry the installation. Enable Repair Mode for good measure.</li> </ul> <p>If you are still unable to install, try installing to a different location and see if that works.</p> <p>If you still have problems, ask for help on the Invoke discord.</p>"},{"location":"installation/quick_start/#other-installation-methods","title":"Other Installation Methods","text":"<ul> <li>You can install the Invoke application as a python package. See our manual install docs.</li> <li>You can run Invoke with docker. See our docker install docs.</li> </ul>"},{"location":"installation/quick_start/#need-help","title":"Need Help?","text":"<ul> <li>Visit our Support Portal.</li> <li>Watch the Getting Started Playlist.</li> <li>Join the conversation on Discord.</li> </ul>"},{"location":"installation/requirements/","title":"Requirements","text":"<p>Invoke runs on Windows 10+, macOS 14+ and Linux (Ubuntu 20.04+ is well-tested).</p>"},{"location":"installation/requirements/#hardware","title":"Hardware","text":"<p>Hardware requirements vary significantly depending on model and image output size.</p> <p>The requirements below are rough guidelines for best performance. GPUs with less VRAM typically still work, if a bit slower. Follow the Low-VRAM mode guide to optimize performance.</p> <ul> <li>All Apple Silicon (M1, M2, etc) Macs work, but 16GB+ memory is recommended.</li> <li>AMD GPUs are supported on Linux only. The VRAM requirements are the same as Nvidia GPUs.</li> </ul> <p>Hardware Requirements (Windows/Linux)</p> SD1.5 - 512\u00d7512SDXL - 1024\u00d71024FLUX.1 - 1024\u00d71024FLUX.2 Klein 4B - 1024\u00d71024FLUX.2 Klein 9B - 1024\u00d71024Z-Image Turbo - 1024x1024 <ul> <li>GPU: Nvidia 10xx series or later, 4GB+ VRAM.</li> <li>Memory: At least 8GB RAM.</li> <li>Disk: 10GB for base installation plus 30GB for models.</li> </ul> <ul> <li>GPU: Nvidia 20xx series or later, 8GB+ VRAM.</li> <li>Memory: At least 16GB RAM.</li> <li>Disk: 10GB for base installation plus 100GB for models.</li> </ul> <ul> <li>GPU: Nvidia 20xx series or later, 10GB+ VRAM.</li> <li>Memory: At least 32GB RAM.</li> <li>Disk: 10GB for base installation plus 200GB for models.</li> </ul> <ul> <li>GPU: Nvidia 30xx series or later, 12GB+ VRAM (e.g. RTX 3090, RTX 4070). FP8 version works with 8GB+ VRAM.</li> <li>Memory: At least 16GB RAM.</li> <li>Disk: 10GB for base installation plus 20GB for models (Diffusers format with encoder).</li> </ul> <ul> <li>GPU: Nvidia 40xx series, 24GB+ VRAM (e.g. RTX 4090). FP8 version works with 12GB+ VRAM.</li> <li>Memory: At least 32GB RAM.</li> <li>Disk: 10GB for base installation plus 40GB for models (Diffusers format with encoder).</li> </ul> <ul> <li>GPU: Nvidia 20xx series or later, 8GB+ VRAM for the Q4_K quantized model. 16GB+ needed for the Q8 or BF16 models.</li> <li>Memory: At least 16GB RAM.</li> <li>Disk: 10GB for base installation plus 35GB for models.</li> </ul> <p><code>tmpfs</code> on Linux</p> <p>If your temporary directory is mounted as a <code>tmpfs</code>, ensure it has sufficient space.</p>"},{"location":"installation/requirements/#python","title":"Python","text":"<p>The launcher installs python for you</p> <p>You don't need to do this if you are installing with the Invoke Launcher.</p> <p>Invoke requires python 3.11 through 3.12. If you don't already have one of these versions installed, we suggest installing 3.12, as it will be supported for longer.</p> <p>Check that your system has an up-to-date Python installed by running <code>python3 --version</code> in the terminal (Linux, macOS) or cmd/powershell (Windows).</p> <p>Installing Python</p> WindowsmacOSLinux <ul> <li>Install python with [an official installer].</li> <li>The installer includes an option to add python to your PATH. Be sure to enable this. If you missed it, re-run the installer, choose to modify an existing installation, and tick that checkbox.</li> <li>You may need to install [Microsoft Visual C++ Redistributable].</li> </ul> <ul> <li>Install python with [an official installer].</li> <li>If model installs fail with a certificate error, you may need to run this command (changing the python version to match what you have installed): <code>/Applications/Python\\ 3.11/Install\\ Certificates.command</code></li> <li>If you haven't already, you will need to install the XCode CLI Tools by running <code>xcode-select --install</code> in a terminal.</li> </ul> <ul> <li>Installing python varies depending on your system. We recommend using <code>uv</code> to manage your python installation.</li> <li>You'll need to install <code>libglib2.0-0</code> and <code>libgl1-mesa-glx</code> for OpenCV to work. For example, on a Debian system: <code>sudo apt update &amp;&amp; sudo apt install -y libglib2.0-0 libgl1-mesa-glx</code></li> </ul>"},{"location":"installation/requirements/#drivers","title":"Drivers","text":"<p>If you have an Nvidia or AMD GPU, you may need to manually install drivers or other support packages for things to work well or at all.</p>"},{"location":"installation/requirements/#nvidia","title":"Nvidia","text":"<p>Run <code>nvidia-smi</code> on your system's command line to verify that drivers and CUDA are installed. If this command fails, or doesn't report versions, you will need to install drivers.</p> <p>Go to the CUDA Toolkit Downloads and carefully follow the instructions for your system to get everything installed.</p> <p>Confirm that <code>nvidia-smi</code> displays driver and CUDA versions after installation.</p>"},{"location":"installation/requirements/#linux-via-nvidia-container-runtime","title":"Linux - via Nvidia Container Runtime","text":"<p>An alternative to installing CUDA locally is to use the Nvidia Container Runtime to run the application in a container.</p>"},{"location":"installation/requirements/#windows-nvidia-cudnn-dlls","title":"Windows - Nvidia cuDNN DLLs","text":"<p>An out-of-date cuDNN library can greatly hamper performance on 30-series and 40-series cards. Check with the community on discord to compare your <code>it/s</code> if you think you may need this fix.</p> <p>First, locate the destination for the DLL files and make a quick back up:</p> <ol> <li>Find your InvokeAI installation folder, e.g. <code>C:\\Users\\Username\\InvokeAI\\</code>.</li> <li>Open the <code>.venv</code> folder, e.g. <code>C:\\Users\\Username\\InvokeAI\\.venv</code> (you may need to show hidden files to see it).</li> <li>Navigate deeper to the <code>torch</code> package, e.g. <code>C:\\Users\\Username\\InvokeAI\\.venv\\Lib\\site-packages\\torch</code>.</li> <li>Copy the <code>lib</code> folder inside <code>torch</code> and back it up somewhere.</li> </ol> <p>Next, download and copy the updated cuDNN DLLs:</p> <ol> <li>Go to https://developer.nvidia.com/cudnn.</li> <li>Create an account if needed and log in.</li> <li>Choose the newest version of cuDNN that works with your GPU architecture. Consult the cuDNN support matrix to determine the correct version for your GPU.</li> <li>Download the latest version and extract it.</li> <li>Find the <code>bin</code> folder, e.g. <code>cudnn-windows-x86_64-SOME_VERSION\\bin</code>.</li> <li>Copy and paste the <code>.dll</code> files into the <code>lib</code> folder you located earlier. Replace files when prompted.</li> </ol> <p>If, after restarting the app, this doesn't improve your performance, either restore your back up or re-run the installer to reset <code>torch</code> back to its original state.</p>"},{"location":"installation/requirements/#amd","title":"AMD","text":"<p>Linux Only</p> <p>AMD GPUs are supported on Linux only, due to ROCm (the AMD equivalent of CUDA) support being Linux only.</p> <p>Bumps Ahead</p> <p>While the application does run on AMD GPUs, there are occasional bumps related to spotty torch support.</p> <p>Run <code>rocm-smi</code> on your system's command line verify that drivers and ROCm are installed. If this command fails, or doesn't report versions, you will need to install them.</p> <p>Go to the ROCm Documentation and carefully follow the instructions for your system to get everything installed.</p> <p>Confirm that <code>rocm-smi</code> displays driver and CUDA versions after installation.</p>"},{"location":"installation/requirements/#linux-via-docker-container","title":"Linux - via Docker Container","text":"<p>An alternative to installing ROCm locally is to use a ROCm docker container to run the application in a container.</p>"},{"location":"nodes/NODES/","title":"Using the Workflow Editor","text":"<p>The workflow editor is a blank canvas allowing for the use of individual functions and image transformations to control the image generation workflow. Nodes take in inputs on the left side of the node, and return an output on the right side of the node. A node graph is composed of multiple nodes that are connected together to create a workflow. Nodes' inputs and outputs are connected by dragging connectors from node to node. Inputs and outputs are color coded for ease of use.</p> <p>If you're not familiar with Diffusion, take a look at our Diffusion Overview. Understanding how diffusion works will enable you to more easily use the Workflow Editor and build workflows to suit your needs.</p>"},{"location":"nodes/NODES/#features","title":"Features","text":""},{"location":"nodes/NODES/#workflow-library","title":"Workflow Library","text":"<p>The Workflow Library enables you to save workflows to the Invoke database, allowing you to easily creating, modify and share workflows as needed. </p> <p>A curated set of workflows are provided by default - these are designed to help explain important nodes' usage in the Workflow Editor.</p> <p></p>"},{"location":"nodes/NODES/#linear-view","title":"Linear View","text":"<p>The Workflow Editor allows you to create a UI for your workflow, to make it easier to iterate on your generations. </p> <p>To add an input to the Linear UI, right click on the input label and select \"Add to Linear View\".</p> <p>The Linear UI View will also be part of the saved workflow, allowing you share workflows and enable other to use them, regardless of complexity. </p> <p></p>"},{"location":"nodes/NODES/#renaming-fields-and-nodes","title":"Renaming Fields and Nodes","text":"<p>Any node or input field can be renamed in the workflow editor. If the input field you have renamed has been added to the Linear View, the changed name will be reflected in the Linear View and the node. </p>"},{"location":"nodes/NODES/#managing-nodes","title":"Managing Nodes","text":"<ul> <li>Ctrl+C to copy a node</li> <li>Ctrl+V to paste a node</li> <li>Backspace/Delete to delete a node</li> <li>Shift+Click to drag and select multiple nodes </li> </ul>"},{"location":"nodes/NODES/#node-caching","title":"Node Caching","text":"<p>Nodes have a \"Use Cache\" option in their footer. This allows for performance improvements by using the previously cached values during the workflow processing. </p>"},{"location":"nodes/NODES/#important-nodes-concepts","title":"Important Nodes &amp; Concepts","text":"<p>There are several node grouping concepts that can be examined with a narrow focus. These (and other) groupings can be pieced together to make up functional graph setups, and are important to understanding how groups of nodes work together as part of a whole. Note that the screenshots below aren't examples of complete functioning node graphs (see Examples).</p>"},{"location":"nodes/NODES/#create-latent-noise","title":"Create Latent Noise","text":"<p>An initial noise tensor is necessary for the latent diffusion process. As a result, the Denoising node requires a noise node input.  </p> <p></p>"},{"location":"nodes/NODES/#text-prompt-conditioning","title":"Text Prompt Conditioning","text":"<p>Conditioning is necessary for the latent diffusion process, whether empty or not. As a result, the Denoising node requires positive and negative conditioning inputs. Conditioning is reliant on a CLIP text encoder provided by the Model Loader node.</p> <p></p>"},{"location":"nodes/NODES/#image-to-latents-vae","title":"Image to Latents &amp; VAE","text":"<p>The ImageToLatents node takes in a pixel image and a VAE and outputs a latents. The LatentsToImage node does the opposite, taking in a latents and a VAE and outpus a pixel image. </p> <p></p>"},{"location":"nodes/NODES/#defined-random-seeds","title":"Defined &amp; Random Seeds","text":"<p>It is common to want to use both the same seed (for continuity) and random seeds (for variety). To define a seed, simply enter it into the 'Seed' field on a noise node. Conversely, the RandomInt node generates a random integer between 'Low' and 'High', and can be used as input to the 'Seed' edge point on a noise node to randomize your seed.</p> <p></p>"},{"location":"nodes/NODES/#controlnet","title":"ControlNet","text":"<p>The ControlNet node outputs a Control, which can be provided as input to a Denoise Latents node. Depending on the type of ControlNet desired, ControlNet nodes usually require an image processor node, such as a Canny Processor or Depth Processor, which prepares an input image for use with ControlNet.</p> <p></p>"},{"location":"nodes/NODES/#lora","title":"LoRA","text":"<p>The Lora Loader node lets you load a LoRA and pass it as output.A LoRA provides fine-tunes to the UNet and text encoder weights that augment the base model\u2019s image and text vocabularies.</p> <p></p>"},{"location":"nodes/NODES/#scaling","title":"Scaling","text":"<p>Use the ImageScale, ScaleLatents, and Upscale nodes to upscale images and/or latent images. Upscaling is the process of enlarging an image and adding more detail. The chosen method differs across contexts. However, be aware that latents are already noisy and compressed at their original resolution; scaling an image could produce more detailed results.</p> <p></p>"},{"location":"nodes/NODES/#iteration-multiple-images-as-input","title":"Iteration + Multiple Images as Input","text":"<p>Iteration is a common concept in any processing, and means to repeat a process with given input. In nodes, you're able to use the Iterate node to iterate through collections usually gathered by the Collect node. The Iterate node has many potential uses, from processing a collection of images one after another, to varying seeds across multiple image generations and more. This screenshot demonstrates how to collect several images and use them in an image generation workflow.</p> <p></p>"},{"location":"nodes/NODES/#batch-multiple-image-generation-random-seeds","title":"Batch / Multiple Image Generation + Random Seeds","text":"<p>Batch or multiple image generation in the workflow editor is done using the RandomRange node. In this case, the 'Size' field represents the number of images to generate, meaning this example will generate 4 images. As RandomRange produces a collection of integers, we need to add the Iterate node to iterate through the collection. This noise can then be fed to the Denoise Latents node for it to iterate through the denoising process with the different seeds provided.</p> <p></p>"},{"location":"nodes/NODES_MIGRATION_V3_V4/","title":"Invoke v4.0.0 Nodes API Migration guide","text":"<p>Invoke v4.0.0 is versioned as such due to breaking changes to the API utilized by nodes, both core and custom.</p>"},{"location":"nodes/NODES_MIGRATION_V3_V4/#motivation","title":"Motivation","text":"<p>Prior to v4.0.0, the <code>invokeai</code> python package has not be set up to be utilized as a library. That is to say, it didn't have any explicitly public API, and node authors had to work with the unstable internal application API.</p> <p>v4.0.0 introduces a stable public API for nodes.</p>"},{"location":"nodes/NODES_MIGRATION_V3_V4/#changes","title":"Changes","text":"<p>There are two node-author-facing changes:</p> <ol> <li>Import Paths</li> <li>Invocation Context API</li> </ol>"},{"location":"nodes/NODES_MIGRATION_V3_V4/#import-paths","title":"Import Paths","text":"<p>All public objects are now exported from <code>invokeai.invocation_api</code>:</p> <pre><code># Old\nfrom invokeai.app.invocations.baseinvocation import (\n    BaseInvocation,\n    InputField,\n    InvocationContext,\n    invocation,\n)\nfrom invokeai.app.invocations.primitives import ImageField\n\n# New\nfrom invokeai.invocation_api import (\n    BaseInvocation,\n    ImageField,\n    InputField,\n    InvocationContext,\n    invocation,\n)\n</code></pre> <p>It's possible that we've missed some classes you need in your node. Please let us know if that's the case.</p>"},{"location":"nodes/NODES_MIGRATION_V3_V4/#invocation-context-api","title":"Invocation Context API","text":"<p>Most nodes utilize the Invocation Context, an object that is passed to the <code>invoke</code> that provides access to data and services a node may need.</p> <p>Until now, that object and the services it exposed were internal. Exposing them to nodes means that changes to our internal implementation could break nodes. The methods on the services are also often fairly complicated and allowed nodes to footgun.</p> <p>In v4.0.0, this object has been refactored to be much simpler.</p> <p>See the invocation API docs for full details of the API.</p> <p>This API may shift slightly until the release of v4.0.0 as we work through a few final updates to the Model Manager.</p>"},{"location":"nodes/NODES_MIGRATION_V3_V4/#improved-service-methods","title":"Improved Service Methods","text":"<p>The biggest offender was the image save method:</p> <pre><code># Old\nimage_dto = context.services.images.create(\n    image=image,\n    image_origin=ResourceOrigin.INTERNAL,\n    image_category=ImageCategory.GENERAL,\n    node_id=self.id,\n    session_id=context.graph_execution_state_id,\n    is_intermediate=self.is_intermediate,\n    metadata=self.metadata,\n    workflow=context.workflow,\n)\n\n# New\nimage_dto = context.images.save(image=image)\n</code></pre> <p>Other methods are simplified, or enhanced with additional functionality:</p> <pre><code># Old\nimage = context.services.images.get_pil_image(image_name)\n\n# New\nimage = context.images.get_pil(image_name)\nimage_cmyk = context.images.get_pil(image_name, \"CMYK\")\n</code></pre> <p>We also had some typing issues around tensors:</p> <pre><code># Old\n# `latents` typed as `torch.Tensor`, but could be `ConditioningFieldData`\nlatents = context.services.latents.get(self.latents.latents_name)\n# `data` typed as `torch.Tenssor,` but could be `ConditioningFieldData`\ncontext.services.latents.save(latents_name, data)\n\n# New - separate methods for tensors and conditioning data w/ correct typing\n# Also, the service generates the names\ntensor_name = context.tensors.save(tensor)\ntensor = context.tensors.load(tensor_name)\n# For conditioning\ncond_name = context.conditioning.save(cond_data)\ncond_data = context.conditioning.load(cond_name)\n</code></pre>"},{"location":"nodes/NODES_MIGRATION_V3_V4/#output-construction","title":"Output Construction","text":"<p>Core Outputs have builder functions right on them - no need to manually construct these objects, or use an extra utility:</p> <pre><code># Old\nimage_output = ImageOutput(\n    image=ImageField(image_name=image_dto.image_name),\n    width=image_dto.width,\n    height=image_dto.height,\n)\nlatents_output = build_latents_output(latents_name=name, latents=latents, seed=None)\nnoise_output = NoiseOutput(\n    noise=LatentsField(latents_name=latents_name, seed=seed),\n    width=latents.size()[3] * 8,\n    height=latents.size()[2] * 8,\n)\ncond_output = ConditioningOutput(\n    conditioning=ConditioningField(\n        conditioning_name=conditioning_name,\n    ),\n)\n\n# New\nimage_output = ImageOutput.build(image_dto)\nlatents_output = LatentsOutput.build(latents_name=name, latents=noise, seed=self.seed)\nnoise_output = NoiseOutput.build(latents_name=name, latents=noise, seed=self.seed)\ncond_output = ConditioningOutput.build(conditioning_name)\n</code></pre> <p>You can still create the objects using constructors if you want, but we suggest using the builder methods.</p>"},{"location":"nodes/comfyToInvoke/","title":"ComfyUI to InvokeAI","text":"<p>If you're coming to InvokeAI from ComfyUI, welcome! You'll find things are similar but different - the good news is that you already know how things should work, and it's just a matter of wiring them up! </p> <p>Some things to note: </p> <ul> <li>InvokeAI's nodes tend to be more granular than default nodes in Comfy. This means each node in Invoke will do a specific task and you might need to use multiple nodes to achieve the same result. The added granularity improves the control you have have over your workflows. </li> <li>InvokeAI's backend and ComfyUI's backend are very different which means Comfy workflows are not able to be imported into InvokeAI. However, we have created a list of popular workflows for you to get started with Nodes in InvokeAI!</li> </ul>"},{"location":"nodes/comfyToInvoke/#node-equivalents","title":"Node Equivalents:","text":"Comfy UI Category ComfyUI Node Invoke Equivalent Sampling KSampler Denoise Latents Sampling Ksampler Advanced Denoise Latents Loaders Load Checkpoint Main Model Loader or SDXL Main Model Loader Loaders Load VAE VAE Loader Loaders Load Lora LoRA Loader or SDXL Lora Loader Loaders Load ControlNet Model ControlNet Loaders Load ControlNet Model (diff) ControlNet Loaders Load Style Model Reference Only ControlNet will be coming in a future version of InvokeAI Loaders unCLIPCheckpointLoader N/A Loaders GLIGENLoader N/A Loaders Hypernetwork Loader N/A Loaders Load Upscale Model Occurs within \"Upscale (RealESRGAN)\" Conditioning CLIP Text Encode (Prompt) Compel (Prompt) or SDXL Compel (Prompt) Conditioning CLIP Set Last Layer CLIP Skip Conditioning Conditioning (Average) Use the .blend() feature of prompts Conditioning Conditioning (Combine) N/A Conditioning Conditioning (Concat) See the Prompt Tools Community Node Conditioning Conditioning (Set Area) N/A Conditioning Conditioning (Set Mask) Mask Edge Conditioning CLIP Vision Encode N/A Conditioning unCLIPConditioning N/A Conditioning Apply ControlNet ControlNet Conditioning Apply ControlNet (Advanced) ControlNet Latent VAE Decode Latents to Image Latent VAE Encode Image to Latents Latent Empty Latent Image Noise Latent Upscale Latent Resize Latents Latent Upscale Latent By Scale Latents Latent Latent Composite Blend Latents Latent LatentCompositeMasked N/A Image Save Image Image Image Preview Image Current Image Load Image Image Image Empty Image Blank Image Image Invert Image Invert Lerp Image Image Batch Images Link \"Image\" nodes into an \"Image Collection\" node Image Pad Image for Outpainting Outpainting is easily accomplished in the Unified Canvas Image ImageCompositeMasked Paste Image Image Upscale Image Resize Image Image Upscale Image By Upscale Image Image Upscale Image (using Model) Upscale Image Image ImageBlur Blur Image Image ImageQuantize N/A Image ImageSharpen N/A Image Canny Canny Processor Mask Load Image (as Mask) Image Mask Convert Mask to Image Image Mask Convert Image to Mask Image Mask SolidMask N/A Mask InvertMask Invert Lerp Image Mask CropMask Crop Image Mask MaskComposite Combine Mask Mask FeatherMask Blur Image Advanced Load CLIP Main Model Loader or SDXL Main Model Loader Advanced UNETLoader Main Model Loader or SDXL Main Model Loader Advanced DualCLIPLoader Main Model Loader or SDXL Main Model Loader Advanced Load Checkpoint Main Model Loader or SDXL Main Model Loader Advanced ConditioningZeroOut N/A Advanced ConditioningSetTimestepRange N/A Advanced CLIPTextEncodeSDXLRefiner Compel (Prompt) or SDXL Compel (Prompt) Advanced CLIPTextEncodeSDXL Compel (Prompt) or SDXL Compel (Prompt) Advanced ModelMergeSimple Model Merging is available in the Model Manager Advanced ModelMergeBlocks Model Merging is available in the Model Manager Advanced CheckpointSave Model saving is available in the Model Manager Advanced CLIPMergeSimple N/A"},{"location":"nodes/communityNodes/","title":"Community Nodes","text":"<p>These are nodes that have been developed by the community, for the community. If you're not sure what a node is, you can learn more about nodes here.</p> <p>If you'd like to submit a node for the community, please refer to the node creation overview.</p> <p>To use a node, add the node to the <code>nodes</code> folder found in your InvokeAI install location.</p> <p>The suggested method is to use <code>git clone</code> to clone the repository the node is found in. This allows for easy updates of the node in the future.</p> <p>If you'd prefer, you can also just download the whole node folder from the linked repository and add it to the <code>nodes</code> folder.</p> <p>To use a community workflow, download the <code>.json</code> node graph file and load it into Invoke AI via the Load Workflow button in the Workflow Editor.</p> <ul> <li>Community Nodes<ul> <li>Anamorphic Tools</li> <li>Adapters-Linked</li> <li>Autostereogram</li> <li>Average Images</li> <li>BiRefNet Background Removal</li> <li>Clean Image Artifacts After Cut</li> <li>Close Color Mask</li> <li>Clothing Mask</li> <li>Contrast Limited Adaptive Histogram Equalization</li> <li>Curves</li> <li>Depth Map from Wavefront OBJ</li> <li>Enhance Detail</li> <li>Film Grain</li> <li>Flip Pose</li> <li>Flux Ideal Size</li> <li>Generative Grammar-Based Prompt Nodes</li> <li>GPT2RandomPromptMaker</li> <li>Grid to Gif</li> <li>Halftone</li> <li>Hand Refiner with MeshGraphormer</li> <li>Image and Mask Composition Pack</li> <li>Image Dominant Color</li> <li>Image Export</li> <li>Image to Character Art Image Nodes</li> <li>Image Picker</li> <li>Image Resize Plus</li> <li>Latent Upscale</li> <li>Load Video Frame</li> <li>Make 3D</li> <li>Mask Operations</li> <li>Match Histogram</li> <li>Metadata-Linked</li> <li>Negative Image</li> <li>Nightmare Promptgen</li> <li>Ollama</li> <li>One Button Prompt</li> <li>Oobabooga</li> <li>Prompt Tools</li> <li>Remote Image</li> <li>BriaAI Background Remove</li> <li>Remove Background</li> <li>Retroize</li> <li>Stereogram</li> <li>Size Stepper Nodes</li> <li>Simple Skin Detection</li> <li>Text font to Image</li> <li>Thresholding</li> <li>Unsharp Mask</li> <li>XY Image to Grid and Images to Grids nodes</li> </ul> </li> <li>Example Node Template</li> <li>Disclaimer</li> <li>Help</li> </ul>"},{"location":"nodes/communityNodes/#anamorphic-tools","title":"Anamorphic Tools","text":"<p>Description: A set of nodes to perform anamorphic modifications to images, like lens blur, streaks, spherical distortion, and vignetting.</p> <p>Node Link: https://github.com/JPPhoto/anamorphic-tools</p>"},{"location":"nodes/communityNodes/#adapters-linked-nodes","title":"Adapters Linked Nodes","text":"<p>Description: A set of nodes for linked adapters (ControlNet, IP-Adaptor &amp; T2I-Adapter). This allows multiple adapters to be chained together without using a <code>collect</code> node which means it can be used inside an <code>iterate</code> node without any collecting on every iteration issues.</p> <ul> <li><code>ControlNet-Linked</code> - Collects ControlNet info to pass to other nodes.</li> <li><code>IP-Adapter-Linked</code> - Collects IP-Adapter info to pass to other nodes.</li> <li><code>T2I-Adapter-Linked</code> - Collects T2I-Adapter info to pass to other nodes.</li> </ul> <p>Note: These are inherited from the core nodes so any update to the core nodes should be reflected in these.</p> <p>Node Link: https://github.com/skunkworxdark/adapters-linked-nodes</p>"},{"location":"nodes/communityNodes/#autostereogram-nodes","title":"Autostereogram Nodes","text":"<p>Description: Generate autostereogram images from a depth map. This is not a very practically useful node but more a 90s nostalgic indulgence as I used to love these images as a kid.</p> <p>Node Link: https://github.com/skunkworxdark/autostereogram_nodes</p> <p>Example Usage:  -&gt;  -&gt;  </p>"},{"location":"nodes/communityNodes/#average-images","title":"Average Images","text":"<p>Description: This node takes in a collection of images of the same size and averages them as output. It converts everything to RGB mode first.</p> <p>Node Link: https://github.com/JPPhoto/average-images-node</p>"},{"location":"nodes/communityNodes/#birefnet-background-removal","title":"BiRefNet Background Removal","text":"<p>Description: Remove image backgrounds using BiRefNet (Bilateral Reference Network), a high-quality segmentation model. Supports multiple model variants including standard, high-resolution, matting, portrait, and specialized models for different use cases.</p> <p>Node Link: https://github.com/veeliks/invoke_birefnet</p> <p>Output Examples</p>"},{"location":"nodes/communityNodes/#clean-image-artifacts-after-cut","title":"Clean Image Artifacts After Cut","text":"<p>Description: Removes residual artifacts after an image is separated from its background.</p> <p>Node Link: https://github.com/VeyDlin/clean-artifact-after-cut-node</p> <p>View: </p>"},{"location":"nodes/communityNodes/#close-color-mask","title":"Close Color Mask","text":"<p>Description: Generates a mask for images based on a closely matching color, useful for color-based selections.</p> <p>Node Link: https://github.com/VeyDlin/close-color-mask-node</p> <p>View: </p>"},{"location":"nodes/communityNodes/#clothing-mask","title":"Clothing Mask","text":"<p>Description: Employs a U2NET neural network trained for the segmentation of clothing items in images.</p> <p>Node Link: https://github.com/VeyDlin/clothing-mask-node</p> <p>View: </p>"},{"location":"nodes/communityNodes/#contrast-limited-adaptive-histogram-equalization","title":"Contrast Limited Adaptive Histogram Equalization","text":"<p>Description: Enhances local image contrast using adaptive histogram equalization with contrast limiting.</p> <p>Node Link: https://github.com/VeyDlin/clahe-node</p> <p>View: </p>"},{"location":"nodes/communityNodes/#curves","title":"Curves","text":"<p>Description: Adjust an image's curve based on a user-defined string.</p> <p>Node Link: https://github.com/JPPhoto/curves-node</p>"},{"location":"nodes/communityNodes/#depth-map-from-wavefront-obj","title":"Depth Map from Wavefront OBJ","text":"<p>Description: Render depth maps from Wavefront .obj files (triangulated) using this simple 3D renderer utilizing numpy and matplotlib to compute and color the scene. There are simple parameters to change the FOV, camera position, and model orientation.</p> <p>To be imported, an .obj must use triangulated meshes, so make sure to enable that option if exporting from a 3D modeling program. This renderer makes each triangle a solid color based on its average depth, so it will cause anomalies if your .obj has large triangles. In Blender, the Remesh modifier can be helpful to subdivide a mesh into small pieces that work well given these limitations.</p> <p>Node Link: https://github.com/dwringer/depth-from-obj-node</p> <p>Example Usage: </p>"},{"location":"nodes/communityNodes/#enhance-detail","title":"Enhance Detail","text":"<p>Description: A single node that can enhance the detail in an image. Increase or decrease details in an image using a guided filter (as opposed to the typical Gaussian blur used by most sharpening filters.) Based on the <code>Enhance Detail</code> ComfyUI node from  https://github.com/spacepxl/ComfyUI-Image-Filters</p> <p>Node Link: https://github.com/skunkworxdark/enhance-detail-node</p> <p>Example Usage: </p>"},{"location":"nodes/communityNodes/#film-grain","title":"Film Grain","text":"<p>Description: This node adds a film grain effect to the input image based on the weights, seeds, and blur radii parameters. It works with RGB input images only.</p> <p>Node Link: https://github.com/JPPhoto/film-grain-node</p>"},{"location":"nodes/communityNodes/#flip-pose","title":"Flip Pose","text":"<p>Description: This node will flip an openpose image horizontally, recoloring it to make sure that it isn't facing the wrong direction. Note that it does not work with openpose hands.</p> <p>Node Link: https://github.com/JPPhoto/flip-pose-node</p>"},{"location":"nodes/communityNodes/#flux-ideal-size","title":"Flux Ideal Size","text":"<p>Description: This node returns an ideal size to use for the first stage of a Flux image generation pipeline. Generating at the right size helps limit duplication and odd subject placement.</p> <p>Node Link: https://github.com/JPPhoto/flux-ideal-size</p>"},{"location":"nodes/communityNodes/#generative-grammar-based-prompt-nodes","title":"Generative Grammar-Based Prompt Nodes","text":"<p>Description: This set of 3 nodes generates prompts from simple user-defined grammar rules (loaded from custom files - examples provided below). The prompts are made by recursively expanding a special template string, replacing nonterminal \"parts-of-speech\" until no nonterminal terms remain in the string.</p> <p>This includes 3 Nodes: - Lookup Table from File - loads a YAML file \"prompt\" section (or of a whole folder of YAML's) into a JSON-ified dictionary (Lookups output) - Lookups Entry from Prompt - places a single entry in a new Lookups output under the specified heading - Prompt from Lookup Table - uses a Collection of Lookups as grammar rules from which to randomly generate prompts.</p> <p>Node Link: https://github.com/dwringer/generative-grammar-prompt-nodes</p> <p>Example Usage: </p>"},{"location":"nodes/communityNodes/#gpt2randompromptmaker","title":"GPT2RandomPromptMaker","text":"<p>Description: A node for InvokeAI utilizes the GPT-2 language model to generate random prompts based on a provided seed and context.</p> <p>Node Link: https://github.com/mickr777/GPT2RandomPromptMaker</p> <p>Output Examples</p> <p>Generated Prompt: An enchanted weapon will be usable by any character regardless of their alignment.</p> <p></p>"},{"location":"nodes/communityNodes/#grid-to-gif","title":"Grid to Gif","text":"<p>Description: One node that turns a grid image into an image collection, one node that turns an image collection into a gif.</p> <p>Node Link: https://github.com/mildmisery/invokeai-GridToGifNode/blob/main/GridToGif.py</p> <p>Example Node Graph: https://github.com/mildmisery/invokeai-GridToGifNode/blob/main/Grid%20to%20Gif%20Example%20Workflow.json</p> <p>Output Examples</p> <p> </p>"},{"location":"nodes/communityNodes/#halftone","title":"Halftone","text":"<p>Description: Halftone converts the source image to grayscale and then performs halftoning. CMYK Halftone converts the image to CMYK and applies a per-channel halftoning to make the source image look like a magazine or newspaper. For both nodes, you can specify angles and halftone dot spacing.</p> <p>Node Link: https://github.com/JPPhoto/halftone-node</p> <p>Example</p> <p>Input:</p> <p></p> <p>Halftone Output:</p> <p></p> <p>CMYK Halftone Output:</p> <p></p>"},{"location":"nodes/communityNodes/#hand-refiner-with-meshgraphormer","title":"Hand Refiner with MeshGraphormer","text":"<p>Description: Hand Refiner takes in your image and automatically generates a fixed depth map for the hands along with a mask of the hands region that will conveniently allow you to use them along with ControlNet to fix the wonky hands generated by Stable Diffusion</p> <p>Node Link: https://github.com/blessedcoolant/invoke_meshgraphormer</p> <p>View </p>"},{"location":"nodes/communityNodes/#image-and-mask-composition-pack","title":"Image and Mask Composition Pack","text":"<p>Description: This is a pack of nodes for composing masks and images, including a simple text mask creator and both image and latent offset nodes. The offsets wrap around, so these can be used in conjunction with the Seamless node to progressively generate centered on different parts of the seamless tiling.</p> <p>This includes 15 Nodes:</p> <ul> <li>Adjust Image Hue Plus - Rotate the hue of an image in one of several different color spaces.</li> <li>Blend Latents/Noise (Masked) - Use a mask to blend part of one latents tensor [including Noise outputs] into another. Can be used to \"renoise\" sections during a multi-stage [masked] denoising process.</li> <li>Enhance Image - Boost or reduce color saturation, contrast, brightness, sharpness, or invert colors of any image at any stage with this simple wrapper for pillow [PIL]'s ImageEnhance module.</li> <li>Equivalent Achromatic Lightness - Calculates image lightness accounting for Helmholtz-Kohlrausch effect based on a method described by High, Green, and Nussbaum (2023).</li> <li>Text to Mask (Clipseg) - Input a prompt and an image to generate a mask representing areas of the image matched by the prompt.</li> <li>Text to Mask Advanced (Clipseg) - Output up to four prompt masks combined with logical \"and\", logical \"or\", or as separate channels of an RGBA image.</li> <li>Image Layer Blend - Perform a layered blend of two images using alpha compositing. Opacity of top layer is selectable, with optional mask and several different blend modes/color spaces.</li> <li>Image Compositor - Take a subject from an image with a flat backdrop and layer it on another image using a chroma key or flood select background removal.</li> <li>Image Dilate or Erode - Dilate or expand a mask (or any image!). This is equivalent to an expand/contract operation.</li> <li>Image Value Thresholds - Clip an image to pure black/white beyond specified thresholds.</li> <li>Offset Latents - Offset a latents tensor in the vertical and/or horizontal dimensions, wrapping it around.</li> <li>Offset Image - Offset an image in the vertical and/or horizontal dimensions, wrapping it around.</li> <li>Rotate/Flip Image - Rotate an image in degrees clockwise/counterclockwise about its center, optionally resizing the image boundaries to fit, or flipping it about the vertical and/or horizontal axes.</li> <li>Shadows/Highlights/Midtones - Extract three masks (with adjustable hard or soft thresholds) representing shadows, midtones, and highlights regions of an image.</li> <li>Text Mask (simple 2D) - create and position a white on black (or black on white) line of text using any font locally available to Invoke.</li> </ul> <p>Node Link: https://github.com/dwringer/composition-nodes</p> <p></p>"},{"location":"nodes/communityNodes/#image-dominant-color","title":"Image Dominant Color","text":"<p>Description: Identifies and extracts the dominant color from an image using k-means clustering.</p> <p>Node Link: https://github.com/VeyDlin/image-dominant-color-node</p> <p>View: </p>"},{"location":"nodes/communityNodes/#image-export","title":"Image Export","text":"<p>Description: Export images in multiple formats (AVIF, JPEG, PNG, TIFF, WebP) with format-specific compression and quality options.</p> <p>Node Link: https://github.com/veeliks/invoke_image_export</p> <p>Nodes:</p>"},{"location":"nodes/communityNodes/#image-to-character-art-image-nodes","title":"Image to Character Art Image Nodes","text":"<p>Description: Group of nodes to convert an input image into ascii/unicode art Image</p> <p>Node Link: https://github.com/mickr777/imagetoasciiimage</p> <p>Output Examples</p> <p> </p>"},{"location":"nodes/communityNodes/#image-picker","title":"Image Picker","text":"<p>Description: This InvokeAI node takes in a collection of images and randomly chooses one. This can be useful when you have a number of poses to choose from for a ControlNet node, or a number of input images for another purpose.</p> <p>Node Link: https://github.com/JPPhoto/image-picker-node</p>"},{"location":"nodes/communityNodes/#image-resize-plus","title":"Image Resize Plus","text":"<p>Description: Provides various image resizing options such as fill, stretch, fit, center, and crop.</p> <p>Node Link: https://github.com/VeyDlin/image-resize-plus-node</p> <p>View: </p>"},{"location":"nodes/communityNodes/#latent-upscale","title":"Latent Upscale","text":"<p>Description: This node uses a small (~2.4mb) model to upscale the latents used in a Stable Diffusion 1.5 or Stable Diffusion XL image generation, rather than the typical interpolation method, avoiding the traditional downsides of the latent upscale technique.</p> <p>Node Link: https://github.com/gogurtenjoyer/latent-upscale</p>"},{"location":"nodes/communityNodes/#load-video-frame","title":"Load Video Frame","text":"<p>Description: This is a video frame image provider + indexer/video creation nodes for hooking up to iterators and ranges and ControlNets and such for invokeAI node experimentation. Think animation + ControlNet outputs.</p> <p>Node Link: https://github.com/helix4u/load_video_frame</p> <p>Output Example: </p>"},{"location":"nodes/communityNodes/#make-3d","title":"Make 3D","text":"<p>Description: Create compelling 3D stereo images from 2D originals.</p> <p>Node Link: https://gitlab.com/srcrr/shift3d/-/raw/main/make3d.py</p> <p>Example Node Graph: https://gitlab.com/srcrr/shift3d/-/raw/main/example-workflow.json?ref_type=heads&amp;inline=false</p> <p>Output Examples</p> <p> </p>"},{"location":"nodes/communityNodes/#mask-operations","title":"Mask Operations","text":"<p>Description: Offers logical operations (OR, SUB, AND) for combining and manipulating image masks.</p> <p>Node Link: https://github.com/VeyDlin/mask-operations-node</p> <p>View: </p>"},{"location":"nodes/communityNodes/#match-histogram","title":"Match Histogram","text":"<p>Description: An InvokeAI node to match a histogram from one image to another.  This is a bit like the <code>color correct</code> node in the main InvokeAI but this works in the YCbCr colourspace and can handle images of different sizes. Also does not require a mask input. - Option to only transfer luminance channel. - Option to save output as grayscale</p> <p>A good use case for this node is to normalize the colors of an image that has been through the tiled scaling workflow of my XYGrid Nodes.</p> <p>See full docs here: https://github.com/skunkworxdark/Prompt-tools-nodes/edit/main/README.md</p> <p>Node Link: https://github.com/skunkworxdark/match_histogram</p> <p>Output Examples</p> <p></p>"},{"location":"nodes/communityNodes/#metadata-linked-nodes","title":"Metadata Linked Nodes","text":"<p>Description: A set of nodes for Metadata. Collect Metadata from within an <code>iterate</code> node &amp; extract metadata from an image.</p> <ul> <li><code>Metadata Item Linked</code> - Allows collecting of metadata while within an iterate node with no need for a collect node or conversion to metadata node</li> <li><code>Metadata From Image</code> - Provides Metadata from an image</li> <li><code>Metadata To String</code> - Extracts a String value of a label from metadata</li> <li><code>Metadata To Integer</code> - Extracts an Integer value of a label from metadata</li> <li><code>Metadata To Float</code> - Extracts a Float value of a label from metadata</li> <li><code>Metadata To Scheduler</code> - Extracts a Scheduler value of a label from metadata</li> <li><code>Metadata To Bool</code> - Extracts Bool types from metadata</li> <li><code>Metadata To Model</code> - Extracts model types from metadata</li> <li><code>Metadata To SDXL Model</code> - Extracts SDXL model types from metadata</li> <li><code>Metadata To LoRAs</code> - Extracts Loras from metadata.</li> <li><code>Metadata To SDXL LoRAs</code> - Extracts SDXL Loras from metadata</li> <li><code>Metadata To ControlNets</code> - Extracts ControNets from metadata</li> <li><code>Metadata To IP-Adapters</code> - Extracts IP-Adapters from metadata</li> <li><code>Metadata To T2I-Adapters</code> - Extracts T2I-Adapters from metadata</li> <li><code>Denoise Latents + Metadata</code> - This is an inherited version of the existing <code>Denoise Latents</code> node but with a metadata input and output.</li> </ul> <p>Node Link: https://github.com/skunkworxdark/metadata-linked-nodes</p>"},{"location":"nodes/communityNodes/#negative-image","title":"Negative Image","text":"<p>Description: Creates a negative version of an image, effective for visual effects and mask inversion.</p> <p>Node Link: https://github.com/VeyDlin/negative-image-node</p> <p>View: </p>"},{"location":"nodes/communityNodes/#nightmare-promptgen","title":"Nightmare Promptgen","text":"<p>Description: Nightmare Prompt Generator - Uses a local text generation model to create unique imaginative (but usually nightmarish) prompts for InvokeAI. By default, it allows you to choose from some gpt-neo models I finetuned on over 2500 of my own InvokeAI prompts in Compel format, but you're able to add your own, as well. Offers support for replacing any troublesome words with a random choice from list you can also define.</p> <p>Node Link: https://github.com/gogurtenjoyer/nightmare-promptgen</p>"},{"location":"nodes/communityNodes/#ollama-node","title":"Ollama Node","text":"<p>Description: Uses Ollama API to expand text prompts for text-to-image generation using local LLMs. Works great for expanding basic prompts into detailed natural language prompts for Flux. Also provides a toggle to unload the LLM model immediately after expanding, to free up VRAM for Invoke to continue the image generation workflow.</p> <p>Node Link: https://github.com/Jonseed/Ollama-Node</p> <p>Example Node Graph: https://github.com/Jonseed/Ollama-Node/blob/main/Ollama-Node-Flux-example.json</p> <p>View:</p> <p></p>"},{"location":"nodes/communityNodes/#one-button-prompt","title":"One Button Prompt","text":"<p>Description: an extensive suite of auto prompt generation and prompt helper nodes based on extensive logic. Get creative with the best prompt generator in the world.</p> <p>The main node generates interesting prompts based on a set of parameters. There are also some additional nodes such as Auto Negative Prompt, One Button Artify, Create Prompt Variant and other cool prompt toys to play around with.</p> <p>Node Link: https://github.com/AIrjen/OneButtonPrompt_X_InvokeAI</p> <p>Nodes:</p> <p></p>"},{"location":"nodes/communityNodes/#oobabooga","title":"Oobabooga","text":"<p>Description: asks a local LLM running in Oobabooga's Text-Generation-Webui to write a prompt based on the user input.</p> <p>Link: https://github.com/sammyf/oobabooga-node</p> <p>Example:</p> <p>\"describe a new mystical  creature in its natural environment\"</p> <p>can return</p> <p>\"The mystical creature I am describing to you is called the \"Glimmerwing\". It is a majestic, iridescent being that inhabits the depths of the most enchanted forests and glimmering lakes. Its body is covered in shimmering scales that reflect every color of the rainbow, and it has delicate, translucent wings that sparkle like diamonds in the sunlight. The Glimmerwing's home is a crystal-clear lake, surrounded by towering trees with leaves that shimmer like jewels. In this serene environment, the Glimmerwing spends its days swimming gracefully through the water, chasing schools of glittering fish and playing with the gentle ripples of the lake's surface. As the sun sets, the Glimmerwing perches on a branch of one of the trees, spreading its wings to catch the last rays of light. The creature's scales glow softly, casting a rainbow of colors across the forest floor. The Glimmerwing sings a haunting melody, its voice echoing through the stillness of the night air. Its song is said to have the power to heal the sick and bring peace to troubled souls. Those who are lucky enough to hear the Glimmerwing's song are forever changed by its beauty and grace.\"</p> <p></p> <p>Requirement</p> <p>a Text-Generation-Webui instance (might work remotely too, but I never tried it) and obviously InvokeAI 3.x</p> <p>Note</p> <p>This node works best with SDXL models, especially as the style can be described independently of the LLM's output.</p>"},{"location":"nodes/communityNodes/#prompt-tools","title":"Prompt Tools","text":"<p>Description: A set of InvokeAI nodes that add general prompt (string) manipulation tools.  Designed to accompany the <code>Prompts From File</code> node and other prompt generation nodes.</p> <ol> <li><code>Prompt To File</code> - saves a prompt or collection of prompts to a file. one per line. There is an append/overwrite option.</li> <li><code>PTFields Collect</code> - Converts image generation fields into a Json format string that can be passed to Prompt to file.</li> <li><code>PTFields Expand</code> - Takes Json string and converts it to individual generation parameters. This can be fed from the Prompts to file node.</li> <li><code>Prompt Strength</code> - Formats prompt with strength like the weighted format of compel</li> <li><code>Prompt Strength Combine</code> - Combines weighted prompts for .and()/.blend()</li> <li><code>CSV To Index String</code> - Gets a string from a CSV by index. Includes a Random index option</li> </ol> <p>The following Nodes are now included in v3.2 of Invoke and are no longer in this set of tools. - <code>Prompt Join</code> -&gt; <code>String Join</code> - <code>Prompt Join Three</code> -&gt; <code>String Join Three</code> - <code>Prompt Replace</code> -&gt; <code>String Replace</code> - <code>Prompt Split Neg</code> -&gt; <code>String Split Neg</code></p> <p>See full docs here: https://github.com/skunkworxdark/Prompt-tools-nodes/edit/main/README.md</p> <p>Node Link: https://github.com/skunkworxdark/Prompt-tools-nodes</p> <p>Workflow Examples</p> <p></p>"},{"location":"nodes/communityNodes/#remote-image","title":"Remote Image","text":"<p>Description: This is a pack of nodes to interoperate with other services, be they public websites or bespoke local servers. The pack consists of these nodes:</p> <ul> <li>Load Remote Image - Lets you load remote images such as a realtime webcam image, an image of the day, or dynamically created images.</li> <li>Post Image to Remote Server - Lets you upload an image to a remote server using an HTTP POST request, eg for storage, display or further processing.</li> </ul> <p>Node Link: https://github.com/fieldOfView/InvokeAI-remote_image</p>"},{"location":"nodes/communityNodes/#briaai-remove-background","title":"BriaAI Remove Background","text":"<p>Description: Implements one click background removal with BriaAI's new version 1.4 model which seems to be producing better results than any other previous background removal tool.</p> <p>Node Link: https://github.com/blessedcoolant/invoke_bria_rmbg</p> <p>View </p>"},{"location":"nodes/communityNodes/#remove-background","title":"Remove Background","text":"<p>Description: An integration of the rembg package to remove backgrounds from images using multiple U2NET models.</p> <p>Node Link: https://github.com/VeyDlin/remove-background-node</p> <p>View: </p>"},{"location":"nodes/communityNodes/#retroize","title":"Retroize","text":"<p>Description: Retroize is a collection of nodes for InvokeAI to \"Retroize\" images. Any image can be given a fresh coat of retro paint with these nodes, either from your gallery or from within the graph itself. It includes nodes to pixelize, quantize, palettize, and ditherize images; as well as to retrieve palettes from existing images.</p> <p>Node Link: https://github.com/Ar7ific1al/invokeai-retroizeinode/</p> <p>Retroize Output Examples</p> <p></p>"},{"location":"nodes/communityNodes/#stereogram-nodes","title":"Stereogram Nodes","text":"<p>Description: A set of custom nodes for InvokeAI to create cross-view or parallel-view stereograms. Stereograms are 2D images that, when viewed properly, reveal a 3D scene. Check out r/crossview for tutorials.</p> <p>Node Link: https://github.com/simonfuhrmann/invokeai-stereo</p> <p>Example Workflow and Output </p>"},{"location":"nodes/communityNodes/#simple-skin-detection","title":"Simple Skin Detection","text":"<p>Description: Detects skin in images based on predefined color thresholds.</p> <p>Node Link: https://github.com/VeyDlin/simple-skin-detection-node</p> <p>View: </p>"},{"location":"nodes/communityNodes/#size-stepper-nodes","title":"Size Stepper Nodes","text":"<p>Description: This is a set of nodes for calculating the necessary size increments for doing upscaling workflows. Use the Final Size &amp; Orientation node to enter your full size dimensions and orientation (portrait/landscape/random), then plug that and your initial generation dimensions into the Ideal Size Stepper and get 1, 2, or 3 intermediate pairs of dimensions for upscaling. Note this does not output the initial size or full size dimensions: the 1, 2, or 3 outputs of this node are only the intermediate sizes.</p> <p>A third node is included, Random Switch (Integers), which is just a generic version of Final Size with no orientation selection.</p> <p>Node Link: https://github.com/dwringer/size-stepper-nodes</p> <p>Example Usage: </p>"},{"location":"nodes/communityNodes/#text-font-to-image","title":"Text font to Image","text":"<p>Description: text font to text image node for InvokeAI, download a font to use (or if in font cache uses it from there), the text is always resized to the image size, but can control that with padding, optional 2<sup>nd</sup> line</p> <p>Node Link: https://github.com/mickr777/textfontimage</p> <p>Output Examples</p> <p></p> <p>Results after using the depth controlnet</p> <p> </p>"},{"location":"nodes/communityNodes/#thresholding","title":"Thresholding","text":"<p>Description: This node generates masks for highlights, midtones, and shadows given an input image. You can optionally specify a blur for the lookup table used in making those masks from the source image.</p> <p>Node Link: https://github.com/JPPhoto/thresholding-node</p> <p>Examples</p> <p>Input:</p> <p></p> <p>Highlights/Midtones/Shadows:</p> <p> </p> <p>Highlights/Midtones/Shadows (with LUT blur enabled):</p> <p> </p>"},{"location":"nodes/communityNodes/#unsharp-mask","title":"Unsharp Mask","text":"<p>Description: Applies an unsharp mask filter to an image, preserving its alpha channel in the process.</p> <p>Node Link: https://github.com/JPPhoto/unsharp-mask-node</p>"},{"location":"nodes/communityNodes/#xy-image-to-grid-and-images-to-grids-nodes","title":"XY Image to Grid and Images to Grids nodes","text":"<p>Description: These nodes add the following to InvokeAI: - Generate grids of images from multiple input images - Create XY grid images with labels from parameters - Split images into overlapping tiles for processing (for super-resolution workflows) - Recombine image tiles into a single output image blending the seams</p> <p>The nodes include: 1. <code>Images To Grids</code> - Combine multiple images into a grid of images 2. <code>XYImage To Grid</code> - Take X &amp; Y params and creates a labeled image grid. 3. <code>XYImage Tiles</code> - Super-resolution (embiggen) style tiled resizing 4. <code>Image Tot XYImages</code> - Takes an image and cuts it up into a number of columns and rows. 5. Multiple supporting nodes - Helper nodes for data wrangling and building <code>XYImage</code> collections</p> <p>See full docs here: https://github.com/skunkworxdark/XYGrid_nodes/edit/main/README.md</p> <p>Node Link: https://github.com/skunkworxdark/XYGrid_nodes</p> <p>Output Examples</p> <p></p>"},{"location":"nodes/communityNodes/#example-node-template","title":"Example Node Template","text":"<p>Description: This node allows you to do super cool things with InvokeAI.</p> <p>Node Link: https://github.com/invoke-ai/InvokeAI/blob/main/invokeai/app/invocations/prompt.py</p> <p>Example Workflow: https://github.com/invoke-ai/InvokeAI/blob/docs/main/docs/workflows/Prompt_from_File.json</p> <p>Output Examples</p> <p></p>"},{"location":"nodes/communityNodes/#disclaimer","title":"Disclaimer","text":"<p>The nodes linked have been developed and contributed by members of the Invoke AI community. While we strive to ensure the quality and safety of these contributions, we do not guarantee the reliability or security of the nodes. If you have issues or concerns with any of the nodes below, please raise it on GitHub or in the Discord.</p>"},{"location":"nodes/communityNodes/#help","title":"Help","text":"<p>If you run into any issues with a node, please post in the InvokeAI Discord.</p>"},{"location":"nodes/contributingNodes/","title":"Contributing Nodes","text":"<p>To learn about the specifics of creating a new node, please visit our Node creation documentation. </p> <p>Once you\u2019ve created a node and confirmed that it behaves as expected locally, follow these steps: </p> <ul> <li>Make sure the node is contained in a new Python (.py) file. Preferably, the node is in a repo with a README detailing the nodes usage &amp; examples to help others more easily use your node. Including the tag \"invokeai-node\" in your repository's README can also help other users find it more easily. </li> <li>Submit a pull request with a link to your node(s) repo in GitHub against the <code>main</code> branch to add the node to the Community Nodes list<ul> <li>Make sure you are following the template below and have provided all relevant details about the node and what it does. Example output images and workflows are very helpful for other users looking to use your node.</li> </ul> </li> <li>A maintainer will review the pull request and node. If the node is aligned with the direction of the project, you may be asked for permission to include it in the core project.</li> </ul>"},{"location":"nodes/contributingNodes/#community-node-template","title":"Community Node Template","text":"<pre><code>--------------------------------\n### Super Cool Node Template\n\n**Description:** This node allows you to do super cool things with InvokeAI.\n\n**Node Link:** https://github.com/invoke-ai/InvokeAI/fake_node.py\n\n**Example Node Graph:**  https://github.com/invoke-ai/InvokeAI/fake_node_graph.json\n\n**Output Examples** \n\n![InvokeAI](https://invoke-ai.github.io/InvokeAI/assets/invoke_ai_banner.png)\n</code></pre>"},{"location":"nodes/defaultNodes/","title":"List of Default Nodes","text":"<p>The table below contains a list of the default nodes shipped with InvokeAI and their descriptions.</p> Node  Function Add Integers Adds two numbers Boolean Primitive Collection A collection of boolean primitive values Boolean Primitive A boolean primitive value Canny Processor Canny edge detection for ControlNet CenterPadCrop Pad or crop an image's sides from the center by specified pixels. Positive values are outside of the image. CLIP Skip Skip layers in clip text_encoder model. Collect Collects values into a collection Color Correct Shifts the colors of a target image to match the reference image, optionally using a mask to only color-correct certain regions of the target image. Color Primitive A color primitive value Compel Prompt Parse prompt using compel package to conditioning. Conditioning Primitive Collection A collection of conditioning tensor primitive values Conditioning Primitive A conditioning tensor primitive value Content Shuffle Processor Applies content shuffle processing to image ControlNet Collects ControlNet info to pass to other nodes Create Denoise Mask Converts a greyscale or transparency image into a mask for denoising. Create Gradient Mask Creates a mask for Gradient (\"soft\", \"differential\") inpainting that gradually expands during denoising. Improves edge coherence. Denoise Latents Denoises noisy latents to decodable images Divide Integers Divides two numbers Dynamic Prompt Parses a prompt using adieyal/dynamicprompts' random or combinatorial generator FaceMask Generates masks for faces in an image to use with Inpainting FaceIdentifier Identifies and labels faces in an image FaceOff Creates a new image that is a scaled bounding box with a mask on the face for Inpainting Float Math Perform basic math operations on two floats Float Primitive Collection A collection of float primitive values Float Primitive A float primitive value Float Range Creates a range HED (softedge) Processor Applies HED edge detection to image Blur Image Blurs an image Extract Image Channel Gets a channel from an image. Image Primitive Collection A collection of image primitive values Integer Math Perform basic math operations on two integers Convert Image Mode Converts an image to a different mode. Crop Image Crops an image to a specified box. The box can be outside of the image. Ideal Size Calculates an ideal image size for latents for a first pass of a multi-pass upscaling to avoid duplication and other artifacts Image Hue Adjustment Adjusts the Hue of an image. Inverse Lerp Image Inverse linear interpolation of all pixels of an image Image Primitive An image primitive value Lerp Image Linear interpolation of all pixels of an image Offset Image Channel Add to or subtract from an image color channel by a uniform value. Multiply Image Channel Multiply or Invert an image color channel by a scalar value. Multiply Images Multiplies two images together using <code>PIL.ImageChops.multiply()</code>. Blur NSFW Image Add blur to NSFW-flagged images Paste Image Pastes an image into another image. ImageProcessor Base class for invocations that preprocess images for ControlNet Resize Image Resizes an image to specific dimensions Round Float Rounds a float to a specified number of decimal places Float to Integer Converts a float to an integer. Optionally rounds to an even multiple of a input number. Scale Image Scales an image by a factor Image to Latents Encodes an image into latents. Add Invisible Watermark Add an invisible watermark to an image Solid Color Infill Infills transparent areas of an image with a solid color PatchMatch Infill Infills transparent areas of an image using the PatchMatch algorithm Tile Infill Infills transparent areas of an image with tiles of the image Integer Primitive Collection A collection of integer primitive values Integer Primitive An integer primitive value Iterate Iterates over a list of items Latents Primitive Collection A collection of latents tensor primitive values Latents Primitive A latents tensor primitive value Latents to Image Generates an image from latents. Leres (Depth) Processor Applies leres processing to image Lineart Anime Processor Applies line art anime processing to image Lineart Processor Applies line art processing to image LoRA Loader Apply selected lora to unet and text_encoder. Main Model Loader Loads a main model, outputting its submodels. Combine Mask Combine two masks together by multiplying them using <code>PIL.ImageChops.multiply()</code>. Mask Edge Applies an edge mask to an image Mask from Alpha Extracts the alpha channel of an image as a mask. Mediapipe Face Processor Applies mediapipe face processing to image Midas (Depth) Processor Applies Midas depth processing to image MLSD Processor Applies MLSD processing to image Multiply Integers Multiplies two numbers Noise Generates latent noise. Normal BAE Processor Applies NormalBae processing to image ONNX Latents to Image Generates an image from latents. ONNX Prompt (Raw) A node to process inputs and produce outputs. May use dependency injection in init to receive providers. ONNX Text to Latents Generates latents from conditionings. ONNX Model Loader Loads a main model, outputting its submodels. OpenCV Inpaint Simple inpaint using opencv. DW Openpose Processor Applies Openpose processing to image PIDI Processor Applies PIDI processing to image Prompts from File Loads prompts from a text file Random Integer Outputs a single random integer. Random Range Creates a collection of random numbers Integer Range Creates a range of numbers from start to stop with step Integer Range of Size Creates a range from start to start + size with step Resize Latents Resizes latents to explicit width/height (in pixels). Provided dimensions are floor-divided by 8. SDXL Compel Prompt Parse prompt using compel package to conditioning. SDXL LoRA Loader Apply selected lora to unet and text_encoder. SDXL Main Model Loader Loads an sdxl base model, outputting its submodels. SDXL Refiner Compel Prompt Parse prompt using compel package to conditioning. SDXL Refiner Model Loader Loads an sdxl refiner model, outputting its submodels. Scale Latents Scales latents by a given factor. Segment Anything Processor Applies segment anything processing to image Show Image Displays a provided image, and passes it forward in the pipeline. String Primitive Collection A collection of string primitive values String Primitive A string primitive value Subtract Integers Subtracts two numbers Tile Resample Processor Tile resampler processor Upscale (RealESRGAN) Upscales an image using RealESRGAN. VAE Loader Loads a VAE model, outputting a VaeLoaderOutput Zoe (Depth) Processor Applies Zoe depth processing to image"},{"location":"nodes/invocation-api/","title":"Invocation API","text":"<p>Each invocation's <code>invoke</code> method is provided a single arg - the Invocation Context.</p> <p>This object provides an API the invocation can use to interact with application services, for example:</p> <ul> <li>Saving images</li> <li>Logging messages</li> <li>Loading models</li> </ul> <pre><code>class MyInvocation(BaseInvocation):\n  ...\n  def invoke(self, context: InvocationContext) -&gt; ImageOutput:\n      # Load an image\n      image_pil = context.images.get_pil(self.image.image_name)\n      # Do something to the image\n      output_image = do_something_cool(image_pil)\n      # Save the image\n      image_dto = context.images.save(output_image)\n      # Log a message\n      context.logger.info(f\"Did something cool, image saved!\")\n      # Return the output\n      return ImageOutput.build(image_dto)\n      ...\n</code></pre> <p>The full API is documented below.</p>"},{"location":"nodes/invocation-api/#mixins","title":"Mixins","text":"<p>Two important mixins are provided to facilitate working with metadata and gallery boards.</p>"},{"location":"nodes/invocation-api/#withmetadata","title":"<code>WithMetadata</code>","text":"<p>Inherit from this class (in addition to <code>BaseInvocation</code>) to add a <code>metadata</code> input to your node. When you do this, you can access the metadata dict from <code>self.metadata</code> in the <code>invoke()</code> function.</p> <p>The dict will be populated via the node's input, and you can add any metadata you'd like to it. When you call <code>context.images.save()</code>, if the metadata dict has any data, it be automatically embedded in the image.</p>"},{"location":"nodes/invocation-api/#withboard","title":"<code>WithBoard</code>","text":"<p>Inherit from this class (in addition to <code>BaseInvocation</code>) to add a <code>board</code> input to your node. This renders as a drop-down to select a board. The user's selection will be accessible from <code>self.board</code> in the <code>invoke()</code> function.</p> <p>When you call <code>context.images.save()</code>, if a board was selected, the image will added to that board as it is saved.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.InvocationContext","title":"InvocationContext","text":"<p>Provides access to various services and data for the current invocation.</p> <p>Attributes:</p> Name Type Description <code>images</code> <code>ImagesInterface</code> <p>Methods to save, get and update images and their metadata.</p> <code>tensors</code> <code>TensorsInterface</code> <p>Methods to save and get tensors, including image, noise, masks, and masked images.</p> <code>conditioning</code> <code>ConditioningInterface</code> <p>Methods to save and get conditioning data.</p> <code>models</code> <code>ModelsInterface</code> <p>Methods to check if a model exists, get a model, and get a model's info.</p> <code>logger</code> <code>LoggerInterface</code> <p>The app logger.</p> <code>config</code> <code>ConfigInterface</code> <p>The app config.</p> <code>util</code> <code>UtilInterface</code> <p>Utility methods, including a method to check if an invocation was canceled and step callbacks.</p> <code>boards</code> <code>BoardsInterface</code> <p>Methods to interact with boards.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ImagesInterface","title":"ImagesInterface","text":"<p>Methods:</p> Name Description <code>get_dto</code> <p>Gets an image as an ImageDTO object.</p> <code>get_metadata</code> <p>Gets an image's metadata, if it has any.</p> <code>get_path</code> <p>Gets the internal path to an image or thumbnail.</p> <code>get_pil</code> <p>Gets an image as a PIL Image object. This method returns a copy of the image.</p> <code>save</code> <p>Saves an image, returning its DTO.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ImagesInterface.get_dto","title":"get_dto","text":"<pre><code>get_dto(image_name: str) -&gt; ImageDTO\n</code></pre> <p>Gets an image as an ImageDTO object.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>The name of the image to get.</p> required <p>Returns:</p> Type Description <code>ImageDTO</code> <p>The image as an ImageDTO object.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ImagesInterface.get_metadata","title":"get_metadata","text":"<pre><code>get_metadata(image_name: str) -&gt; Optional[MetadataField]\n</code></pre> <p>Gets an image's metadata, if it has any.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>The name of the image to get the metadata for.</p> required <p>Returns:</p> Type Description <code>Optional[MetadataField]</code> <p>The image's metadata, if it has any.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ImagesInterface.get_path","title":"get_path","text":"<pre><code>get_path(image_name: str, thumbnail: bool = False) -&gt; Path\n</code></pre> <p>Gets the internal path to an image or thumbnail.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>The name of the image to get the path of.</p> required <code>thumbnail</code> <code>bool</code> <p>Get the path of the thumbnail instead of the full image</p> <code>False</code> <p>Returns:</p> Type Description <code>Path</code> <p>The local path of the image or thumbnail.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ImagesInterface.get_pil","title":"get_pil","text":"<pre><code>get_pil(image_name: str, mode: IMAGE_MODES | None = None) -&gt; Image\n</code></pre> <p>Gets an image as a PIL Image object. This method returns a copy of the image.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>The name of the image to get.</p> required <code>mode</code> <code>IMAGE_MODES | None</code> <p>The color mode to convert the image to. If None, the original mode is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Image</code> <p>The image as a PIL Image object.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ImagesInterface.save","title":"save","text":"<pre><code>save(image: Image, board_id: Optional[str] = None, image_category: ImageCategory = GENERAL, metadata: Optional[MetadataField] = None) -&gt; ImageDTO\n</code></pre> <p>Saves an image, returning its DTO.</p> <p>If the current queue item has a workflow or metadata, it is automatically saved with the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The image to save, as a PIL image.</p> required <code>board_id</code> <code>Optional[str]</code> <p>The board ID to add the image to, if it should be added. It the invocation             inherits from <code>WithBoard</code>, that board will be used automatically. Use this only if             you want to override or provide a board manually!</p> <code>None</code> <code>image_category</code> <code>ImageCategory</code> <p>The category of the image. Only the GENERAL category is added             to the gallery.</p> <code>GENERAL</code> <code>metadata</code> <code>Optional[MetadataField]</code> <p>The metadata to save with the image, if it should have any. If the             invocation inherits from <code>WithMetadata</code>, that metadata will be used automatically.             Use this only if you want to override or provide metadata manually!</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageDTO</code> <p>The saved image DTO.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.TensorsInterface","title":"TensorsInterface","text":"<p>Methods:</p> Name Description <code>load</code> <p>Loads a tensor by name. This method returns a copy of the tensor.</p> <code>save</code> <p>Saves a tensor, returning its name.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.TensorsInterface.load","title":"load","text":"<pre><code>load(name: str) -&gt; Tensor\n</code></pre> <p>Loads a tensor by name. This method returns a copy of the tensor.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the tensor to load.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensor.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.TensorsInterface.save","title":"save","text":"<pre><code>save(tensor: Tensor) -&gt; str\n</code></pre> <p>Saves a tensor, returning its name.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to save.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The name of the saved tensor.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ConditioningInterface","title":"ConditioningInterface","text":"<p>Methods:</p> Name Description <code>load</code> <p>Loads conditioning data by name. This method returns a copy of the conditioning data.</p> <code>save</code> <p>Saves a conditioning data object, returning its name.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ConditioningInterface.load","title":"load","text":"<pre><code>load(name: str) -&gt; ConditioningFieldData\n</code></pre> <p>Loads conditioning data by name. This method returns a copy of the conditioning data.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the conditioning data to load.</p> required <p>Returns:</p> Type Description <code>ConditioningFieldData</code> <p>The conditioning data.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ConditioningInterface.save","title":"save","text":"<pre><code>save(conditioning_data: ConditioningFieldData) -&gt; str\n</code></pre> <p>Saves a conditioning data object, returning its name.</p> <p>Parameters:</p> Name Type Description Default <code>conditioning_data</code> <code>ConditioningFieldData</code> <p>The conditioning data to save.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The name of the saved conditioning data.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ModelsInterface","title":"ModelsInterface","text":"<p>Common API for loading, downloading and managing models.</p> <p>Methods:</p> Name Description <code>download_and_cache_model</code> <p>Download the model file located at source to the models cache and return its Path.</p> <code>exists</code> <p>Check if a model exists.</p> <code>get_absolute_path</code> <p>Gets the absolute path for a given model config or path.</p> <code>get_config</code> <p>Get a model's config.</p> <code>load</code> <p>Load a model.</p> <code>load_by_attrs</code> <p>Load a model by its attributes.</p> <code>load_local_model</code> <p>Load the model file located at the indicated path</p> <code>load_remote_model</code> <p>Download, cache, and load the model file located at the indicated URL or repo_id.</p> <code>search_by_attrs</code> <p>Search for models by attributes.</p> <code>search_by_path</code> <p>Search for models by path.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ModelsInterface.download_and_cache_model","title":"download_and_cache_model","text":"<pre><code>download_and_cache_model(source: str | AnyHttpUrl) -&gt; Path\n</code></pre> <p>Download the model file located at source to the models cache and return its Path.</p> <p>This can be used to single-file install models and other resources of arbitrary types which should not get registered with the database. If the model is already installed, the cached path will be returned. Otherwise it will be downloaded.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | AnyHttpUrl</code> <p>A URL that points to the model, or a huggingface repo_id.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to the downloaded model</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ModelsInterface.exists","title":"exists","text":"<pre><code>exists(identifier: Union[str, ModelIdentifierField]) -&gt; bool\n</code></pre> <p>Check if a model exists.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, ModelIdentifierField]</code> <p>The key or ModelField representing the model.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the model exists, False if not.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ModelsInterface.get_absolute_path","title":"get_absolute_path","text":"<pre><code>get_absolute_path(config_or_path: AnyModelConfig | Path | str) -&gt; Path\n</code></pre> <p>Gets the absolute path for a given model config or path.</p> <p>For example, if the model's path is <code>flux/main/FLUX Dev.safetensors</code>, and the models path is <code>/home/username/InvokeAI/models</code>, this method will return <code>/home/username/InvokeAI/models/flux/main/FLUX Dev.safetensors</code>.</p> <p>Parameters:</p> Name Type Description Default <code>config_or_path</code> <code>AnyModelConfig | Path | str</code> <p>The model config or path.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The absolute path to the model.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ModelsInterface.get_config","title":"get_config","text":"<pre><code>get_config(identifier: Union[str, ModelIdentifierField]) -&gt; AnyModelConfig\n</code></pre> <p>Get a model's config.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, ModelIdentifierField]</code> <p>The key or ModelField representing the model.</p> required <p>Returns:</p> Type Description <code>AnyModelConfig</code> <p>The model's config.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ModelsInterface.load","title":"load","text":"<pre><code>load(identifier: Union[str, ModelIdentifierField], submodel_type: Optional[SubModelType] = None) -&gt; LoadedModel\n</code></pre> <p>Load a model.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, ModelIdentifierField]</code> <p>The key or ModelField representing the model.</p> required <code>submodel_type</code> <code>Optional[SubModelType]</code> <p>The submodel of the model to get.</p> <code>None</code> <p>Returns:</p> Type Description <code>LoadedModel</code> <p>An object representing the loaded model.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ModelsInterface.load_by_attrs","title":"load_by_attrs","text":"<pre><code>load_by_attrs(name: str, base: BaseModelType, type: ModelType, submodel_type: Optional[SubModelType] = None) -&gt; LoadedModel\n</code></pre> <p>Load a model by its attributes.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>base</code> <code>BaseModelType</code> <p>The models' base type, e.g. <code>BaseModelType.StableDiffusion1</code>, <code>BaseModelType.StableDiffusionXL</code>, etc.</p> required <code>type</code> <code>ModelType</code> <p>Type of the model, e.g. <code>ModelType.Main</code>, <code>ModelType.Vae</code>, etc.</p> required <code>submodel_type</code> <code>Optional[SubModelType]</code> <p>The type of submodel to load, e.g. <code>SubModelType.UNet</code>, <code>SubModelType.TextEncoder</code>, etc. Only main</p> <code>None</code> <p>Returns:</p> Type Description <code>LoadedModel</code> <p>An object representing the loaded model.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ModelsInterface.load_local_model","title":"load_local_model","text":"<pre><code>load_local_model(model_path: Path, loader: Optional[Callable[[Path], AnyModel]] = None) -&gt; LoadedModelWithoutConfig\n</code></pre> <p>Load the model file located at the indicated path</p> <p>If a loader callable is provided, it will be invoked to load the model. Otherwise, <code>safetensors.torch.load_file()</code> or <code>torch.load()</code> will be called to load the model.</p> <p>Be aware that the LoadedModelWithoutConfig object has no <code>config</code> attribute</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>A model Path</p> required <code>loader</code> <code>Optional[Callable[[Path], AnyModel]]</code> <p>A Callable that expects a Path and returns a dict[str|int, Any]</p> <code>None</code> <p>Returns:</p> Type Description <code>LoadedModelWithoutConfig</code> <p>A LoadedModelWithoutConfig object.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ModelsInterface.load_remote_model","title":"load_remote_model","text":"<pre><code>load_remote_model(source: str | AnyHttpUrl, loader: Optional[Callable[[Path], AnyModel]] = None) -&gt; LoadedModelWithoutConfig\n</code></pre> <p>Download, cache, and load the model file located at the indicated URL or repo_id.</p> <p>If the model is already downloaded, it will be loaded from the cache.</p> <p>If the a loader callable is provided, it will be invoked to load the model. Otherwise, <code>safetensors.torch.load_file()</code> or <code>torch.load()</code> will be called to load the model.</p> <p>Be aware that the LoadedModelWithoutConfig object has no <code>config</code> attribute</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | AnyHttpUrl</code> <p>A URL or huggingface repoid.</p> required <code>loader</code> <code>Optional[Callable[[Path], AnyModel]]</code> <p>A Callable that expects a Path and returns a dict[str|int, Any]</p> <code>None</code> <p>Returns:</p> Type Description <code>LoadedModelWithoutConfig</code> <p>A LoadedModelWithoutConfig object.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ModelsInterface.search_by_attrs","title":"search_by_attrs","text":"<pre><code>search_by_attrs(name: Optional[str] = None, base: Optional[BaseModelType] = None, type: Optional[ModelType] = None, format: Optional[ModelFormat] = None) -&gt; list[AnyModelConfig]\n</code></pre> <p>Search for models by attributes.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name to search for (exact match).</p> <code>None</code> <code>base</code> <code>Optional[BaseModelType]</code> <p>The base to search for, e.g. <code>BaseModelType.StableDiffusion1</code>, <code>BaseModelType.StableDiffusionXL</code>, etc.</p> <code>None</code> <code>type</code> <code>Optional[ModelType]</code> <p>Type type of model to search for, e.g. <code>ModelType.Main</code>, <code>ModelType.Vae</code>, etc.</p> <code>None</code> <code>format</code> <code>Optional[ModelFormat]</code> <p>The format of model to search for, e.g. <code>ModelFormat.Checkpoint</code>, <code>ModelFormat.Diffusers</code>, etc.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[AnyModelConfig]</code> <p>A list of models that match the attributes.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ModelsInterface.search_by_path","title":"search_by_path","text":"<pre><code>search_by_path(path: Path) -&gt; list[AnyModelConfig]\n</code></pre> <p>Search for models by path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to search for.</p> required <p>Returns:</p> Type Description <code>list[AnyModelConfig]</code> <p>A list of models that match the path.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.LoggerInterface","title":"LoggerInterface","text":"<p>Methods:</p> Name Description <code>debug</code> <p>Logs a debug message.</p> <code>error</code> <p>Logs an error message.</p> <code>info</code> <p>Logs an info message.</p> <code>warning</code> <p>Logs a warning message.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.LoggerInterface.debug","title":"debug","text":"<pre><code>debug(message: str) -&gt; None\n</code></pre> <p>Logs a debug message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to log.</p> required"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.LoggerInterface.error","title":"error","text":"<pre><code>error(message: str) -&gt; None\n</code></pre> <p>Logs an error message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to log.</p> required"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.LoggerInterface.info","title":"info","text":"<pre><code>info(message: str) -&gt; None\n</code></pre> <p>Logs an info message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to log.</p> required"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.LoggerInterface.warning","title":"warning","text":"<pre><code>warning(message: str) -&gt; None\n</code></pre> <p>Logs a warning message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to log.</p> required"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ConfigInterface","title":"ConfigInterface","text":"<p>Methods:</p> Name Description <code>get</code> <p>Gets the app's config.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.ConfigInterface.get","title":"get","text":"<pre><code>get() -&gt; InvokeAIAppConfig\n</code></pre> <p>Gets the app's config.</p> <p>Returns:</p> Type Description <code>InvokeAIAppConfig</code> <p>The app's config.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.UtilInterface","title":"UtilInterface","text":"<p>Methods:</p> Name Description <code>flux2_step_callback</code> <p>The step callback for FLUX.2 Klein models (32-channel VAE).</p> <code>flux_step_callback</code> <p>The step callback emits a progress event with the current step, the total number of</p> <code>is_canceled</code> <p>Checks if the current session has been canceled.</p> <code>sd_step_callback</code> <p>The step callback emits a progress event with the current step, the total number of</p> <code>signal_progress</code> <p>Signals the progress of some long-running invocation. The progress is displayed in the UI.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.UtilInterface.flux2_step_callback","title":"flux2_step_callback","text":"<pre><code>flux2_step_callback(intermediate_state: PipelineIntermediateState) -&gt; None\n</code></pre> <p>The step callback for FLUX.2 Klein models (32-channel VAE).</p> <p>Parameters:</p> Name Type Description Default <code>intermediate_state</code> <code>PipelineIntermediateState</code> <p>The intermediate state of the diffusion pipeline.</p> required"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.UtilInterface.flux_step_callback","title":"flux_step_callback","text":"<pre><code>flux_step_callback(intermediate_state: PipelineIntermediateState) -&gt; None\n</code></pre> <p>The step callback emits a progress event with the current step, the total number of steps, a preview image, and some other internal metadata.</p> <p>This should be called after each denoising step.</p> <p>Parameters:</p> Name Type Description Default <code>intermediate_state</code> <code>PipelineIntermediateState</code> <p>The intermediate state of the diffusion pipeline.</p> required"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.UtilInterface.is_canceled","title":"is_canceled","text":"<pre><code>is_canceled() -&gt; bool\n</code></pre> <p>Checks if the current session has been canceled.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the current session has been canceled, False if not.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.UtilInterface.sd_step_callback","title":"sd_step_callback","text":"<pre><code>sd_step_callback(intermediate_state: PipelineIntermediateState, base_model: BaseModelType) -&gt; None\n</code></pre> <p>The step callback emits a progress event with the current step, the total number of steps, a preview image, and some other internal metadata.</p> <p>This should be called after each denoising step.</p> <p>Parameters:</p> Name Type Description Default <code>intermediate_state</code> <code>PipelineIntermediateState</code> <p>The intermediate state of the diffusion pipeline.</p> required <code>base_model</code> <code>BaseModelType</code> <p>The base model for the current denoising step.</p> required"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.UtilInterface.signal_progress","title":"signal_progress","text":"<pre><code>signal_progress(message: str, percentage: float | None = None, image: Image | None = None, image_size: tuple[int, int] | None = None) -&gt; None\n</code></pre> <p>Signals the progress of some long-running invocation. The progress is displayed in the UI.</p> <p>If a percentage is provided, the UI will display a progress bar and automatically append the percentage to the message. You should not include the percentage in the message.</p> Example <pre><code>total_steps = 10\nfor i in range(total_steps):\n    percentage = i / (total_steps - 1)\n    context.util.signal_progress(\"Doing something cool\", percentage)\n</code></pre> <p>If an image is provided, the UI will display it. If your image should be displayed at a different size, provide a tuple of <code>(width, height)</code> for the <code>image_size</code> parameter. The image will be displayed at the specified size in the UI.</p> <p>For example, SD denoising progress images are \u215b the size of the original image, so you'd do this to ensure the image is displayed at the correct size:     <pre><code># Calculate the output size of the image (8x the progress image's size)\nwidth = progress_image.width * 8\nheight = progress_image.height * 8\n# Signal the progress with the image and output size\nsignal_progress(\"Denoising\", percentage, progress_image, (width, height))\n</code></pre></p> <p>If your progress image is very large, consider downscaling it to reduce the payload size and provide the original size to the <code>image_size</code> parameter. The PIL <code>thumbnail</code> method is useful for this, as it maintains the aspect ratio of the image:     <pre><code># `thumbnail` modifies the image in-place, so we need to first make a copy\nthumbnail_image = progress_image.copy()\n# Resize the image to a maximum of 256x256 pixels, maintaining the aspect ratio\nthumbnail_image.thumbnail((256, 256))\n# Signal the progress with the thumbnail, passing the original size\nsignal_progress(\"Denoising\", percentage, thumbnail, progress_image.size)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>A message describing the current status. Do not include the percentage in this message.</p> required <code>percentage</code> <code>float | None</code> <p>The current percentage completion for the process. Omit for indeterminate progress.</p> <code>None</code> <code>image</code> <code>Image | None</code> <p>An optional image to display.</p> <code>None</code> <code>image_size</code> <code>tuple[int, int] | None</code> <p>The optional size of the image to display. If omitted, the image will be displayed at its original size.</p> <code>None</code>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.BoardsInterface","title":"BoardsInterface","text":"<p>Methods:</p> Name Description <code>add_image_to_board</code> <p>Adds an image to a board.</p> <code>create</code> <p>Creates a board.</p> <code>get_all</code> <p>Gets all boards.</p> <code>get_all_image_names_for_board</code> <p>Gets all image names for a board.</p> <code>get_dto</code> <p>Gets a board DTO.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.BoardsInterface.add_image_to_board","title":"add_image_to_board","text":"<pre><code>add_image_to_board(board_id: str, image_name: str) -&gt; None\n</code></pre> <p>Adds an image to a board.</p> <p>Parameters:</p> Name Type Description Default <code>board_id</code> <code>str</code> <p>The ID of the board to add the image to.</p> required <code>image_name</code> <code>str</code> <p>The name of the image to add to the board.</p> required"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.BoardsInterface.create","title":"create","text":"<pre><code>create(board_name: str) -&gt; BoardDTO\n</code></pre> <p>Creates a board.</p> <p>Parameters:</p> Name Type Description Default <code>board_name</code> <code>str</code> <p>The name of the board to create.</p> required <p>Returns:</p> Type Description <code>BoardDTO</code> <p>The created board DTO.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.BoardsInterface.get_all","title":"get_all","text":"<pre><code>get_all() -&gt; list[BoardDTO]\n</code></pre> <p>Gets all boards.</p> <p>Returns:</p> Type Description <code>list[BoardDTO]</code> <p>A list of all boards.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.BoardsInterface.get_all_image_names_for_board","title":"get_all_image_names_for_board","text":"<pre><code>get_all_image_names_for_board(board_id: str) -&gt; list[str]\n</code></pre> <p>Gets all image names for a board.</p> <p>Parameters:</p> Name Type Description Default <code>board_id</code> <code>str</code> <p>The ID of the board to get the image names for.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of all image names for the board.</p>"},{"location":"nodes/invocation-api/#invokeai.app.services.shared.invocation_context.BoardsInterface.get_dto","title":"get_dto","text":"<pre><code>get_dto(board_id: str) -&gt; BoardDTO\n</code></pre> <p>Gets a board DTO.</p> <p>Parameters:</p> Name Type Description Default <code>board_id</code> <code>str</code> <p>The ID of the board to get.</p> required <p>Returns:</p> Type Description <code>BoardDTO</code> <p>The board DTO.</p>"},{"location":"nodes/overview/","title":"Nodes","text":""},{"location":"nodes/overview/#what-are-nodes","title":"What are Nodes?","text":"<p>An Node is simply a single operation that takes in inputs and returns out outputs. Multiple nodes can be linked together to create more complex functionality. All InvokeAI features are added through nodes.</p>"},{"location":"nodes/overview/#anatomy-of-a-node","title":"Anatomy of a Node","text":"<p>Individual nodes are made up of the following:</p> <ul> <li>Inputs: Edge points on the left side of the node window where you connect outputs from other nodes.</li> <li>Outputs: Edge points on the right side of the node window where you connect to inputs on other nodes.</li> <li>Options: Various options which are either manually configured, or overridden by connecting an output from another node to the input.</li> </ul> <p>With nodes, you can can easily extend the image generation capabilities of InvokeAI, and allow you build workflows that suit your needs.</p> <p>You can read more about nodes and the node editor here.</p>"},{"location":"nodes/overview/#downloading-new-nodes","title":"Downloading New Nodes","text":"<p>To download a new node, visit our list of Community Nodes. These are nodes that have been created by the community, for the community.</p>"},{"location":"nodes/detailedNodes/faceTools/","title":"Face Nodes","text":""},{"location":"nodes/detailedNodes/faceTools/#faceoff","title":"FaceOff","text":"<p>FaceOff mimics a user finding a face in an image and resizing the bounding box around the head in Canvas.</p> <p>Enter a face ID (found with FaceIdentifier) to choose which face to mask.</p> <p>Just as you would add more context inside the bounding box by making it larger in Canvas, the node gives you a padding input (in pixels) which will simultaneously add more context, and increase the resolution of the bounding box so the face remains the same size inside it.</p> <p>The \"Minimum Confidence\" input defaults to 0.5 (50%), and represents a pass/fail threshold a detected face must reach for it to be processed. Lowering this value may help if detection is failing. If the detected masks are imperfect and stray too far outside/inside of faces, the node gives you X &amp; Y offsets to shrink/grow the masks by a multiplier.</p> <p>FaceOff will output the face in a bounded image, taking the face off of the original image for input into any node that accepts image inputs. The node also outputs a face mask with the dimensions of the bounded image. The X &amp; Y outputs are for connecting to the X &amp; Y inputs of the Paste Image node, which will place the bounded image back on the original image using these coordinates.</p>"},{"location":"nodes/detailedNodes/faceTools/#inputsoutputs","title":"Inputs/Outputs","text":"Input Description Image Image for face detection Face ID The face ID to process, numbered from 0. Multiple faces not supported. Find a face's ID with FaceIdentifier node. Minimum Confidence Minimum confidence for face detection (lower if detection is failing) X Offset X-axis offset of the mask Y Offset Y-axis offset of the mask Padding All-axis padding around the mask in pixels Chunk Chunk (or divide) the image into sections to greatly improve face detection success. Defaults to off, but will activate if no faces are detected normally. Activate to chunk by default. Output Description Bounded Image Original image bound, cropped, and resized Width The width of the bounded image in pixels Height The height of the bounded image in pixels Mask The output mask X The x coordinate of the bounding box's left side Y The y coordinate of the bounding box's top side"},{"location":"nodes/detailedNodes/faceTools/#facemask","title":"FaceMask","text":"<p>FaceMask mimics a user drawing masks on faces in an image in Canvas.</p> <p>The \"Face IDs\" input allows the user to select specific faces to be masked. Leave empty to detect and mask all faces, or a comma-separated list for a specific combination of faces (ex: <code>1,2,4</code>). A single integer will detect and mask that specific face. Find face IDs with the FaceIdentifier node.</p> <p>The \"Minimum Confidence\" input defaults to 0.5 (50%), and represents a pass/fail threshold a detected face must reach for it to be processed. Lowering this value may help if detection is failing.</p> <p>If the detected masks are imperfect and stray too far outside/inside of faces, the node gives you X &amp; Y offsets to shrink/grow the masks by a multiplier. All masks shrink/grow together by the X &amp; Y offset values.</p> <p>By default, masks are created to change faces. When masks are inverted, they change surrounding areas, protecting faces.</p>"},{"location":"nodes/detailedNodes/faceTools/#inputsoutputs_1","title":"Inputs/Outputs","text":"Input Description Image Image for face detection Face IDs Comma-separated list of face ids to mask eg '0,2,7'. Numbered from 0. Leave empty to mask all. Find face IDs with FaceIdentifier node. Minimum Confidence Minimum confidence for face detection (lower if detection is failing) X Offset X-axis offset of the mask Y Offset Y-axis offset of the mask Chunk Chunk (or divide) the image into sections to greatly improve face detection success. Defaults to off, but will activate if no faces are detected normally. Activate to chunk by default. Invert Mask Toggle to invert the face mask Output Description Image The original image Width The width of the image in pixels Height The height of the image in pixels Mask The output face mask"},{"location":"nodes/detailedNodes/faceTools/#faceidentifier","title":"FaceIdentifier","text":"<p>FaceIdentifier outputs an image with detected face IDs printed in white numbers onto each face.</p> <p>Face IDs can then be used in FaceMask and FaceOff to selectively mask all, a specific combination, or single faces.</p> <p>The FaceIdentifier output image is generated for user reference, and isn't meant to be passed on to other image-processing nodes.</p> <p>The \"Minimum Confidence\" input defaults to 0.5 (50%), and represents a pass/fail threshold a detected face must reach for it to be processed. Lowering this value may help if detection is failing. If an image is changed in the slightest, run it through FaceIdentifier again to get updated FaceIDs.</p>"},{"location":"nodes/detailedNodes/faceTools/#inputsoutputs_2","title":"Inputs/Outputs","text":"Input Description Image Image for face detection Minimum Confidence Minimum confidence for face detection (lower if detection is failing) Chunk Chunk (or divide) the image into sections to greatly improve face detection success. Defaults to off, but will activate if no faces are detected normally. Activate to chunk by default. Output Description Image The original image with small face ID numbers printed in white onto each face for user reference Width The width of the original image in pixels Height The height of the original image in pixels"},{"location":"nodes/detailedNodes/faceTools/#tips","title":"Tips","text":"<ul> <li>If not all target faces are being detected, activate Chunk to bypass full   image face detection and greatly improve detection success.</li> <li>Final results will vary between full-image detection and chunking for faces   that are detectable by both due to the nature of the process. Try either to   your taste.</li> <li>Be sure Minimum Confidence is set the same when using FaceIdentifier with   FaceOff/FaceMask.</li> <li>For FaceOff, use the color correction node before faceplace to correct edges   being noticeable in the final image (see example screenshot).</li> <li>Non-inpainting models may struggle to paint/generate correctly around faces.</li> <li>If your face won't change the way you want it to no matter what you change,   consider that the change you're trying to make is too much at that resolution.   For example, if an image is only 512x768 total, the face might only be 128x128   or 256x256, much smaller than the 512x512 your SD1.5 model was probably   trained on. Try increasing the resolution of the image by upscaling or   resizing, add padding to increase the bounding box's resolution, or use an   image where the face takes up more pixels.</li> <li>If the resulting face seems out of place pasted back on the original image   (ie. too large, not proportional), add more padding on the FaceOff node to   give inpainting more context. Context and good prompting are important to   keeping things proportional.</li> <li>If you find the mask is too big/small and going too far outside/inside the   area you want to affect, adjust the x &amp; y offsets to shrink/grow the mask area</li> <li>Use a higher denoise start value to resemble aspects of the original face or   surroundings. Denoise start = 0 &amp; denoise end = 1 will make something new,   while denoise start = 0.50 &amp; denoise end = 1 will be 50% old and 50% new.</li> <li>mediapipe isn't good at detecting faces with lots of face paint, hair covering   the face, etc. Anything that obstructs the face will likely result in no faces   being detected.</li> <li>If you find your face isn't being detected, try lowering the minimum   confidence value from 0.5. This could result in false positives, however   (random areas being detected as faces and masked).</li> <li>After altering an image and wanting to process a different face in the newly   altered image, run the altered image through FaceIdentifier again to see the   new Face IDs. MediaPipe will most likely detect faces in a different order   after an image has been changed in the slightest.</li> </ul>"}]}